[{"title":"NOSQL","date":"2019-08-07T14:59:05.594Z","path":"2019/08/07/NOSQL/","text":"NOSQL全称：NoSQL = Not Only SQL 泛指非关系型数据库 四大分类： 1）键值(Key-Value)存储数据库 Key/value模型对于IT系统来说的优势在于简单、易部署。但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。举例如：Redis. 2）列存储数据库 这部分数据库通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列。这些列是由列家族来安排的。如：Cassandra, HBase, Riak. 3）文档型 文档型数据库的灵感是来自于Lotus Notes办公软件的，而且它同第一种键值存储相类似。该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高。如：CouchDB, MongoDb.国内也有文档型数据库SequoiaDB，已经开源。 4）图形(Graph)数据库 图形结构的数据库同其他行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。 NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。 我们总结NoSQL数据库在以下的这几种情况下比较适用： 数据模型比较简单； 需要灵活性更强的IT系统； 对数据库性能要求较高； 不需要高度的数据一致性； 对于给定key，比较容易映射复杂值的环境。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"互联网金融行业数仓分层","date":"2019-08-07T14:37:26.011Z","path":"2019/08/07/互联网金融行业数仓分层/","text":"互联网金融行业数仓分层专业术语 ODL层 （Operational Data Layer）：操作数据层 外部数据什么样，该层数据就是什么样（关系型数据库、JSON格式等)部分关系型数据可以直接转IDL层 BDL层 （Base Data Layer）：基础数据层 ODL层经过简单格式化解析后存储到BDL层，常见于JSON日志格式的解析。 IDL层 （Interface Data Layer）：接口层，也称主题表，宽表 由BDL层经过去重、去噪、字典翻译、空值转化，日期格式化、关联JOIN、维度分析等清洗后的数据。如：用户、产品、绑卡、订单、用户行为等明细数据。 ADL层（Application Data Layer）：应用层 ，也称数据集市 通常与需求对接，由IDL层基于某些维度的深度加工统计汇总等操作转化而来，涉及到多个主题以及tmp数据之间的关联JOIN后的结果。 DIC层（Dictionary Data Layer）：字典层 存储一些诸如省、市、县区域表、渠道列表、商品类目等等表数据，可以从数据源直接sqoop生成dic_xxx表，也可以通过odl层转化层dic_表。 TMP层（Temporary Data Layer）：临时层 存储一些中间计算结果 简要说明: 层次间的转换没必要循规蹈矩，按部就班，适当做到灵活，避免重复清洗浪费资源 ODL层干净的关系型数据可以直接转换为IDL层数据，减少计算量 ODL层侧重与外部对接，BDL层/TMP层/IDL层侧重清洗，IDL层和ADL层侧重对外提供应用服务 层数太少不够灵活，太多则在数据推翻重洗耗时，时间成本（一个坑）数据源提供的数据越详细越好，避免后期大量重复的清洗工作。 “星型模型”和“雪花模型”简单解释： （1）星型模型：事实表+维度表（区域、类目、性别…)等多表通过预先JOIN冗余到一张宽表里去，常见IDL层。 （2）雪花模型：在计算的时候，才将事实表跟维度表做join。 现在一般都是采用（1）的模式，为什么呢？ 预先计算，挺高性能，避免后续重复计算。CPU和内存的资源永远比磁盘空间宝贵的多。至于（2)的方式，有点就是灵活，不需要太多的重复清洗，但是性能不如（1）. 建设思路 从需求出发，逆推应用层ADL结构，进而推导出它涉及的主题表IDL表结构，再推导可能涉及的基础表BDL表结构，最后分析所需的数据源取自何处。需求包含“明确”需求和“潜在”需求。 开发步骤 创建ODL、BDL、IDL、ADL层表结构(HQL) 确定数据抽取方案（增量或全量） 编写sqoop脚本将data同步到ODL层 编写ODL-&gt;BDL-&gt;IDL-&gt;ADL层ETL清洗脚本(HQL),注意：清洗的顺序，时间确保上一层的数据稳定，减少对下一层的影响 编写Hue workflow Ooize脚本 打通Kylin、FineBI、Hive关系，实现数据可视化、可导出目标,将稳定后所有脚本WIKI上保存一份 其他相关的请参照原博客 作者：水星有鱼链接：https://www.jianshu.com/p/f941967aeee8","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://wxzhongwang.github.io/tags/数据仓库/"}]},{"title":"SparkStreaming和Storm","date":"2019-08-07T14:37:26.004Z","path":"2019/08/07/SparkStreaming和Storm的区别/","text":"SparkStreaming和StormStorm和Spark Streaming都是分布式流处理的开源框架，但是它们之间还是有一些区别的，这里将进行比较并指出它们的重要的区别。 处理模型以及延迟虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此，Storm处理一个事件可以达到亚秒级的延迟，而Spark Streaming则有秒级的延迟。 容错和数据保证在容错数据保证方面的权衡方面，Spark Streaming提供了更好的支持容错状态计算。在Storm中，当每条单独的记录通过系统时必须被跟踪，所以Storm能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而Spark Streaming只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然Storm的 Trident library库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。 简而言之,如果你需要亚秒级的延迟，Storm是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming则更好。Spark Streaming编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。 实现和编程APIStorm主要是由Clojure语言实现，SparkStreaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和Twitter开发，而Spark Streaming是在UC Berkeley开发的。 Storm提供了Java API，同时也支持其他语言的API。SparkStreaming支持Scala和Java语言(其实也支持Python)。另外SparkStreaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写SparkStreaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。 生产支持Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。 Storm是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。 集群管理集成尽管两个系统都运行在它们自己的集群上，Storm也能运行在Mesos，而SparkStreaming能运行在YARN 和 Mesos上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Java基础面试题四","date":"2019-08-07T14:37:25.985Z","path":"2019/08/07/Java基础面试题四/","text":"Java基础面试题四 Q1: Java中垃圾回收有什么目的？什么时候进行垃圾回收？ 垃圾回收的目的是识别并且丢弃应用不再使用的对象来释放和重用资源。 Q2：如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？ 不会，在下一个垃圾回收周期中，这个对象将是可被回收的。 Q3: String是最基本的数据类型吗? 基本数据类型包括: byte int char long float double boolean short java.lang.String类是final类型的，因此不可以继承这个类、不能修改这个类。为了提高效率节省空间，我们应该用StringBuffer类。 Q4: int 和 Integer 有什么区别? Java 提供两种不同的类型：引用类型和原始类型（或内置类型）。Int是java的原始数据类型，Integer是java为int提供的封装类。Java为每个原始类型提供了封装类。 Q5: String 和 StringBuffer的区别? JAVA平台提供了两个类：String和StringBuffer，它们可以储存和操作字符串，即包含多个字符的字符数据。这个String类提供了数值不可改变的字符串。而这个StringBuffer类提供的字符串进行修改。 Q6: ArrayList,Vector,LinkedList的存储性能和特性? ArrayList和Vector都是使用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，它们都允许直接按序号索引元素，但是插入元素要涉及数组元素移动等内存操作，所以索引数据快而插入数据慢，Vector由于使用了synchronized方法（线程安全），通常性能上较ArrayList差，而LinkedList使用双向链表实现存储，按序号索引数据需要进行前向或后向遍历，但是插入数据时只需要记录本项的前后项即可，所以插入速度较快。12345678910List的子类特点 ArrayList: 底层数据结构是数组，查询快，增删慢 线程不安全，效率高 Vector: 底层数据结构是数组，查询快，增删慢 线程安全，效率低 LinkedList: 底层数据结构是链表，查询慢，增删快 线程不安全，效率高 Q7: Collection 和 Collections的区别 Collection是集合类的上级接口，继承与他的接口主要有Set和List. Collections是针对集合类的一个帮助类，他提供一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。集合的继承体系：","tags":[{"name":"面试题","slug":"面试题","permalink":"https://wxzhongwang.github.io/tags/面试题/"}]},{"title":"Java基础面试题五","date":"2019-08-07T14:37:25.985Z","path":"2019/08/07/Java基础面试题五/","text":"Java基础面试题五 Q1: &amp; 和 &amp;&amp; 的区别 &amp;是位运算符，表示按位与运算，&amp;&amp;是逻辑运算符，表示逻辑与（and）。 Q2: final, finally, finalize的区别 final 用于声明属性，方法和类，分别表示属性不可变，方法不可覆盖，类不可继承。 finally,异常处理语句结构的一部分，表示总是执行。 finalize是Object类的一个方法，垃圾收集器执行的时候会调用被回收对象的此方法，可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等。 Q3: sleep() 和 wait() 有什么区别? sleep是线程类（Thread）的方法，导致此线程暂停执行指定时间，给执行机会给其他线程，但是监控状态依然保持，到时后会自动恢复。调用sleep不会释放对象锁。 wait是Object类的方法，对此对象调用wait方法导致本线程放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象发出notify方法（或not ifyAll）后本线程才进入对象锁定池准备获得对象锁进入运行状态。 Q4: error和exception有什么区别? error:表示恢复不是不可能但很困难的情况下的一种严重问题。比如说内存溢出。不可能指望程序能处理这样的情况。 exception: 表示一种设计或实现问题。也就是说，它表示如果程序运行正常，从不会发生的情况。 Q5: GC是什么? 为什么要有GC? GC是垃圾收集的意思（Gabage Collection）,内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。123要请求垃圾收集，可以调用下面的方法之一： System.gc() Runtime.getRuntime().gc() 当然，如果需要，程序员可以在Java程序中显式地使用System.gc()来强制进行一次立即的内存清理。因为显式声明是做堆内存全扫描，也就是FullGC，是需要停止所有的活动的（Stop The World Collection），你的应用能承受这个吗？而其显示调用System.gc()只是给虚拟机一个建议，不一定会执行，因为System.gc()在一个优先级很低的线程中执行。","tags":[{"name":"面试题","slug":"面试题","permalink":"https://wxzhongwang.github.io/tags/面试题/"}]},{"title":"Java基础面试题三","date":"2019-08-07T14:37:25.984Z","path":"2019/08/07/Java基础面试题三/","text":"Java基础面试题三 Q1: 如何确保N个线程可以访问N个资源同时又不导致死锁？ 使用多线程的时候，一种非常简单的避免死锁的方式就是：指定获取锁的顺序，并强制线程按照指定的顺序获取锁。因此，如果所有的线程都是以同样的顺序加锁和释放锁，就不会出现死锁了。 Q2: Java集合类框架的基本接口有哪些？ Java集合类提供了一套设计良好的支持对一组对象进行操作的接口和类。Java集合类里面最基本的接口有： Collection：代表一组对象，每一个对象都是它的子元素。 Set：不包含重复元素的Collection。 List：有顺序的collection，并且可以包含重复元素。 Map：可以把键(key)映射到值(value)的对象，键不能重复。 Q3: 什么是迭代器(Iterator)？ Iterator接口提供了很多对集合元素进行迭代的方法。每一个集合类都包含了可以返回迭代器实例的迭代方法。迭代器可以在迭代的过程中删除底层集合的元素。 Q4: Iterator和ListIterator的区别是什么？ 下面列出了他们的区别： Iterator可用来遍历Set和List集合，但是ListIterator只能用来遍历List。 Iterator对集合只能是前向遍历，ListIterator既可以前向也可以后向。 ListIterator实现了Iterator接口，并包含其他的功能，比如：增加元素，替换元素，获取前一个和后一个元素的索引，等等。 Q5: Java中的HashMap的工作原理是什么？ Java中的HashMap是以键值对(key-value)的形式存储元素的。HashMap需要一个hash函数，它使用hashCode()和equals()方法来向集合/从集合添加和检索元素。当调用put()方法的时候，HashMap会计算key的hash值，然后把键值对存储在集合中合适的索引上。如果key已经存在了，value会被更新成新值。 HashMap的一些重要的特性是它的容量(capacity)，负载因子(load factor)和扩容极限(threshold resizing)。 Q6: HashMap和Hashtable有什么区别？ HashMap和Hashtable都实现了Map接口，因此很多特性非常相似。但是，他们有以下不同点： HashMap允许键和值是null，而Hashtable不允许键或者值是null。 Hashtable是同步的，而HashMap不是。因此，HashMap更适合于单线程环境，而Hashtable适合于多线程环境。 HashMap提供了可供应用迭代的键的集合，因此，HashMap是快速失败的。另一方面，Hashtable提供了对键的列举(Enumeration)。 Q7: 数组(Array)和列表(ArrayList)有什么区别？什么时候应该使用Array而不是ArrayList？ 下面列出了Array和ArrayList的不同点： Array可以包含基本类型和对象类型，ArrayList只能包含对象类型。 Array大小是固定的，ArrayList的大小是动态变化的。 ArrayList提供了更多的方法和特性，比如：addAll()，removeAll()，iterator()等等。 对于基本类型数据，集合使用自动装箱来减少编码工作量。但是，当处理固定大小的基本数据类型的时候，这种方式相对比较慢。 Q8: ArrayList和LinkedList有什么区别？ ArrayList和LinkedList都实现了List接口，他们有以下的不同点： ArrayList是基于索引的数据接口，它的底层是数组。它可以以O(1)时间复杂度对元素进行随机访问。与此对应，LinkedList是以元素链表的形式存储它的数据，每一个元素都和它的前一个和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是O(n)。 相对于ArrayList，LinkedList的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。 LinkedList比ArrayList更占内存，因为LinkedList为每一个节点存储了两个引用，一个指向前一个元素，一个指向下一个元素。也可以参考ArrayList vs. LinkedList。 Q9: 如何权衡是使用无序的数组还是有序的数组？ 有序数组最大的好处在于查找的时间复杂度是O(log n)，而无序数组是O(n)。有序数组的缺点是插入操作的时间复杂度是O(n)，因为值大的元素需要往后移动来给新元素腾位置。相反，无序数组的插入时间复杂度是常量O(1)。 Q10: HashSet和TreeSet有什么区别？ HashSet是由一个hash表来实现的，因此，它的元素是无序的。add()，remove()，contains()方法的时间复杂度是O(1)。 TreeSet是由一个树形的结构来实现的，它里面的元素是有序的。因此，add()，remove()，contains()方法的时间复杂度是O(logn)。","tags":[{"name":"面试题","slug":"面试题","permalink":"https://wxzhongwang.github.io/tags/面试题/"}]},{"title":"Java基础面试题二","date":"2019-08-07T14:37:25.984Z","path":"2019/08/07/Java基础面试题二/","text":"Java基础面试题二 Q1: 什么是死锁(deadlock)？ 两个进程都在等待对方执行完毕才能继续往下执行的时候就发生了死锁。结果就是两个进程都陷入了无限的等待中。代码表示：12345678public class DieLockDemo &#123; public static void main(String[] args) &#123; DieLock dl1 = new DieLock(true); DieLock dl2 = new DieLock(false); dl1.start(); dl2.start(); &#125;&#125; 理想状态下dl1线程为true从if执行先打出”if objA”然后再接着打出”if objB”之后释放A、B的锁对象，之后dl2线程执行else语句打出”else objB”，”else objA”。非理想状态下dl1先打出”if objA”，之后线程dl2执行打出”else objB”，然后1、2线程的锁对象A和B都处于被锁的状态，两个线程争夺锁对象发生死锁现象。 123456789101112131415161718192021222324public class DieLock extends Thread &#123; private boolean flag; public DieLock(boolean flag) &#123; this.flag = flag; &#125; @Override public void run() &#123; if (flag) &#123; synchronized (MyLock.objA) &#123; System.out.println(\"if objA\"); synchronized (MyLock.objB) &#123; System.out.println(\"if objB\"); &#125; &#125; &#125; else &#123; synchronized (MyLock.objB) &#123; System.out.println(\"else objB\"); synchronized (MyLock.objA) &#123; System.out.println(\"else objA\"); &#125; &#125; &#125; &#125;&#125;","tags":[{"name":"面试题","slug":"面试题","permalink":"https://wxzhongwang.github.io/tags/面试题/"}]},{"title":"Java基础面试题一","date":"2019-08-07T14:37:25.983Z","path":"2019/08/07/Java基础面试题/","text":"Java基础面试题 Q1: 什么是Java虚拟机? Java虚拟机是一个可以执行Java字节码的虚拟机进程。Java源文件被编译成能被Java虚拟机执行的字节码文件。 Q2: “static”关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法？ “static”关键字表明一个成员变量或者是成员方法可以在没有所属的类的实例变量的情况下被访问。简单说就是：不声明所属类的实例就可访问该变量或方法。Java中static方法不能被覆盖，因为方法覆盖是基于运行时动态绑定的，而static方法是编译时静态绑定的。 Q3: 是否可以在static环境中访问非static变量？ static变量在Java中是属于类的，它在所有的实例中的值是一样的。当类被Java虚拟机载入的时候，会对static变量进行初始化。如果你的代码尝试不用实例来访问非static的变量，编译器会报错，因为这些变量还没有被创建出来，还没有跟任何实例关联上。 Q4: Java支持的数据类型有哪些？什么是自动拆装箱？ 基本数据类型是： byte short int long float double boolean char 自动装箱是Java编译器在基本数据类型和对应的对象包装类型之间做的一个转化。比如：把int转化成Integer，float转化成double，等等。反之就是自动拆箱。 Q5: Overload和Override的区别。Overloaded的方法是否可以改变返回值的类型? 方法的重写Overriding和重载Overloading是Java多态性的不同表现。重写Overriding是父类与子类之间多态性的一种表现，重载Overloading是一个类中多态性的一种表现。如果在子类中定义某方法与其父类有相同的名称和参数，我们说该方法被重写(Overriding)。子类的对象使用这个方法时，将调用子类中的定义，对它而言，父类中的定义如同被”屏蔽”了。如果在一个类中定义了多个同名的方法，它们或有不同的参数个数或有不同的参数类型，则称为方法的重载(Overloading)。Overloaded的方法是可以改变返回值的类型。 Overload： 重写 子类重写父类方法Overload: 重载 相同方法名 Q6: Java支持多继承么？ 不支持，Java不支持多继承。每个类都只能继承一个类，但是可以实现多个接口。 Q7: 接口和抽象类的区别是什么？ Java提供和支持创建抽象类和接口。它们的实现有共同点，不同点在于： 接口中所有的方法隐含的都是抽象的。而抽象类则可以同时包含抽象和非抽象的方法。 类可以实现很多个接口，但是只能继承一个抽象类 类如果要实现一个接口，它必须要实现接口声明的所有方法。但是，类可以不实现抽象类声明的所有方法，当然，在这种情况下，类也必须得声明成是抽象的。 抽象类可以在不提供接口方法实现的情况下实现接口。 Java接口中声明的变量默认都是final的。抽象类可以包含非final的变量。 Java接口中的成员函数默认是public的。抽象类的成员函数可以是private，protected或者是public。 接口是绝对抽象的，不可以被实例化。抽象类也不可以被实例化，但是，如果它包含main方法的话是可以被调用的。 Q8: 什么是值传递和引用传递？ 值传递: 意味着传递了对象的一个副本。因此，就算是改变了对象副本，也不会影响源对象的值。 引用传递: 意味着传递的并不是实际的对象，而是对象的引用。因此，外部对引用对象所做的改变会反映到所有的对象上。 Q9: 创建线程有几种不同的方式？你喜欢哪一种？为什么？ 有三种方式可以用来创建线程： 继承Thread类 实现Runnable接口 应用程序可以使用Executor框架来创建线程池 实现Runnable接口这种方式更受欢迎，因为这不需要继承Thread类。在应用设计中已经继承了别的对象的情况下，这需要多继承（而Java不支持多继承），只能实现接口。同时，线程池也是非常高效的，很容易实现和使用。 Q10: 同步方法和同步代码块的区别是什么？ 在Java语言中，每一个对象有一把锁。线程可以使用synchronized关键字来获取对象上的锁。synchronized关键字可应用在方法级别(粗粒度锁：这里的锁对象可以是This)或者是代码块级别(细粒度锁：这里的锁对象就是任意对象)。 方法级别(粗粒度锁：这里的锁对象可以是This) 代码块级别(细粒度锁：这里的锁对象就是任意对象)","tags":[{"name":"面试题","slug":"面试题","permalink":"https://wxzhongwang.github.io/tags/面试题/"}]},{"title":"Git安装","date":"2019-08-07T14:37:25.977Z","path":"2019/08/07/Git安装/","text":"Linux 安装Git:12345678检查是否安装Gitgit --version 在 Ubuntu 这类 Debian 体系的系统上，可以用 apt-get 安装：sudo apt-get install gitCentOs:yum install -y git window安装比较简单不多说。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"NodeJs apiDoc","date":"2019-08-07T14:37:25.972Z","path":"2019/08/07/apiDoc接口文档/","text":"apiDoc全局安装，方便用命令创建文档1npm install apidoc -g 配置在你的项目根目录下新建apidoc.json文件，该文件描述了项目对外提供接口的概要信息如名称、版本、描述、文档打开时浏览器显示标题和接口缺省访问地址。apidoc.json 12345678910111213141516&#123; \"name\": \"xxx Api\", \"version\": \"1.0.0\", \"description\": \"xxx Api Documentation\", \"title\": \"xxx\", \"url\" : \"http://域名/api/v1\", \"sampleUrl\": \"http://域名/api/v1\", \"template\": &#123; \"withCompare\": true, \"withGenerator\": true, \"forceLanguage\":\"en\" &#125;, //顺序、若有需要可配置(books, student,xxx) Resources Name \"order\": [\"xxx\",\"xxx\"]&#125; Sample12345678910111213141516171819202122232425262728293031323334/** * Get Access Token * @api &#123;POST&#125; /token GetAccessToken * @apiDescription Generate a token that can be passed with each API request. * @apiName GetAccessToken * @apiPermission API User * @apiParam (Parameters) &#123;String&#125; username username * @apiParam (Parameters) &#123;String&#125; password password * @apiParamExample &#123;json&#125; Sample (Request body formats: text/plain, application/x-www-form-urlencoded, text/json, application/json) : * &#123; * \"username\": \"your accont\", * \"password\": \"your password\" * &#125; * @apiSampleRequest off * @apiSuccessExample &#123;json&#125; Response (Response body formats: application/json, text/json): * Success Response : * Status: 200 OK * &#123; * \"status\": \"success\", * \"message\": \"Authentication successful.\", * \"data\": &#123; * \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6Ikpxxxxx.xxxxx2VybmFtZSI6InNsIiwiaWF0IjoxNTQ1MTkyMzY4LCJleHAiOjE1NDUxOTI4NDh9.xxxxxxgS1Yw8rNRD31p97A7fdWvmkrXxo3llMJowX7U\" * &#125; * &#125; * Or * Bad Response: * Status: 400 * &#123; * \"status\": \"failure\", * \"message\": \"Authentication failed. Please provide a correct username or password.\" * &#125; * @apiGroup Token * @apiVersion 1.0.0 */ 生成文档初始化或发生改变时，在应用程序根目录执行相应命令以此项目为例： apidoc -i api/v1 -o public/apiDoc 1234-i input-o output指定文件夹路径 具体注释写法参照官网解释：http://apidocjs.com/#run public/apiDoc内文件不用上传到代码管理中可直接执行npm run doc命令自动生成","tags":[{"name":"NodeJs","slug":"NodeJs","permalink":"https://wxzhongwang.github.io/tags/NodeJs/"}]},{"title":"大数据","date":"2019-04-08T06:25:45.000Z","path":"2019/04/08/数据中台和数据仓库、数据平台/","text":"数据中台和数据仓库、数据平台的关键区别？概括地说，三者的关键区别有以下几方面： 数据中台是企业级的逻辑概念，体现企业 D2V（Data to Value）的能力，为业务提供服务的主要方式是数据 API； 数据仓库是一个相对具体的功能概念，是存储和管理一个或多个主题数据的集合，为业务提供服务的方式主要是分析报表； 数据平台是在大数据基础上出现的融合了结构化和非结构化数据的数据基础平台，为业务提供服务的方式主要是直接提供数据集； 数据中台距离业务更近，为业务提供速度更快的服务； 数据仓库是为了支持管理决策分析，而数据中台则是将数据服务化之后提供给业务系统，不仅限于分析型场景，也适用于交易型场景； 数据中台可以建立在数据仓库和数据平台之上，是加速企业从数据到业务价值的过程的中间层。 数据仓库具有历史性，其中存储的数据大多是结构化数据，这些数据并非企业全量数据，而是根据需求针对性抽取的， 因此数据仓库对于业务的价值是各种各样的报表，但这些报表又无法实时产生。数据仓库报表虽然能够提供部分业务价值，但不能直接影响业务。 数据平台的出现是为了解决数据仓库不能处理非结构化数据和报表开发周期长的问题，所以先撇开业务需求、把企业所有的数据都抽取出来放到一起，成为一个大的数据集，其中有结构化数据、非结构化数据等。 当业务方有需求的时候，再把他们需要的若干个小数据集单独提取出来，以数据集的形式提供给数据应用。 而数据中台是在数据仓库和数据平台的基础上，将数据生产为为一个个数据 API 服务，以更高效的方式提供给业务。","tags":[{"name":"大数据","slug":"大数据","permalink":"https://wxzhongwang.github.io/tags/大数据/"}]},{"title":"编码规范","date":"2019-03-31T03:18:02.000Z","path":"2019/03/31/编码规范/","text":"编码规范C# 编码风格指南一旦进入代码开发阶段，你必须安排好代码审查计划以确保每个人都遵守同样的规则。建议按以下3种方式进行代码审查： 互相审查 - 由其他团队成员进行代码审查以确保遵守了代码规范且达到要求。这种方式可以配合单元测试。项目中每个代码文件必须要通过这个程序。 架构审查 - 架构人员需对项目核心模块进行审查以确保符合设计，没有出现大的甚至有可能影响整个项目运转的纰漏。 团队审查 - 每周随机选择一个或多个文件进行一次团队审查。审查会议开始前30分钟，将文件打印并分发到每个成员手里，会议开始后用投影仪将文件内容展示出来。代码的每一块都要进行审查，让所有成员提出改进建议。（别忘了要感谢提供素材的开发人员，并确保他不会觉得受到了“群嘲”！） 代码文件组织C# 源代码文件每个源代码文件应该只包含一个类定义，也即类定义只出现在它自己的文件中。源代码文件名需与类声明里的类名保持一致。譬如一个名为User的类的源文件名应该是User.cs。 版权声明1234567891011//-------------------------------------------------------------------------------// Copyright (c) Dick. All Right Reserved. // This source is subject to the Microsoft Permissive License. // Please see the License.txt file for more information. // All other rights reserved. // // THIS CODE AND INFORMATION ARE PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY // KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE // IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A // PARTICULAR PURPOSE. //------------------------------------------------------------------------------- 目录设计每层命名空间都要建立目录（以MyProject.UI.Admin为例，请使用MyProject/UI/Admin这样的目录结构，而不是只建立一层名为MyProject.UI.Admin的目录）。 代码排列C#源代码文件整体顺序应为：123Using 声明Namespace声明Class 和 interface 声明 C#类内部分的顺序应为：12345成员变量属性构造函数方法` 以上每个部分都需要放到 #region里。 命名空间和USING声明 Using 和命名空间声明都要左边界齐平。 命名空间的每个组件名首字母要大写 如果组件名是个缩写，只让第一个字母大写，如 System.Data.Sql。 如果这个缩写只有2个字母，那可以2个字母都大写，如 System.IO。 注意: 移除不需要的或重复的命名空间，使用短的命名空间来代替。譬如：1234567891011121314// Preferredusing System.Data.SqlClient;public void ValidationCall()&#123; … SqlConnection conn = new SqlConnection(connstr);…&#125;// Avoidpublic void ValidationCall()&#123; … System.Data.SqlClient.SqlConnection conn = new SqlConnection(connstr);…&#125; 创建和修订记录文件创建和修订记录需要按以下格式填写到using代码段后面，namespace之前。12345/** 作者: 创建日期:* 修订者： 修订日期： 修订内容：* */ XML 文档Visual Studio提供了一种文档类型，在开发环境中可以用来检测并导出到结构化的XML中，以用来创建与源代码分离的代码级别的文档。XML 文档用于描述类、方法和属性。它应当在所有可用的情况下尽可能使用。 缩进空行适当的空行能增强代码可读性。它们能帮助区分逻辑不相干的代码片段。双行空行要放在：源代码中逻辑不相关的代码片段之间不同的类和接口定义（如果有时候不得不放在同个文件，尽量避免这种情况）单行空行要放在： 方法与方法之间 属性与属性之间 方法体内的局部变量和它的第一次使用之间 方法体内不同逻辑片段之间以增强可读性 换行如果一个表达式在一行内放不下，则根据以下常用原则来进行换行： 操作符号之后换行 逗号之后换行 择较高等的换行，其次才是较低等的换行 （譬如优先对括号外的部分换行）换行后需要缩进：1234567// 调用方法SomeMethod1(expression1, expression2, expression3, expression4, expression5);// 声明方法void SomeMethod1(long Expression1, long Expression2, long Expression3, long Expression4, long Expression5)&#123; //...&#125; 注意: 换行后第二行的缩进与第一个参数齐平以下是一个数学表达式换行的例子。第一种换行方式是较好的，因为是在括号以外的地方进行的换行，也即是在较高等进行的分行。 12345// Preferredvar1 = var2 * (var3 + var4 – var5) + var4 * var6;// Avoidvar1 = var2 * (var3 + var4 - var5) + var4 * var6; 对if表达式的分行需要使用缩进：1234if ((condition1 &amp;&amp; condition2) || (condition3 &amp;&amp; condition4) &amp;&amp; condition5 || !condition6)&#123; ...&#125; 三元表达式请用这两种格式：1234Alpha = aLongBooleanExpression ? beta : gamma;Alpha = aLongBooleanExpression ? beta : gamma; 代码间距空格间距在代码中间的逗号或分号后应该有单独的一个空格，此外一个关键字与后跟的括号之间也要有个空格，譬如：12345678910// CorrectTestMethod(a, b, c);while (condition)&#123; //... &#125;// AvoidTestMethod(a, b, c);TestMethod(a, b, c); 注意: 在方法名和它的前括号之间不需要有空格。这样便于区分是关键字还是方法名。操作符两边要加上空格（++或逻辑非这样的一元运算符除外），譬如：1234567a = b; // Avoid a=b;// Avoid for(int i=0; i&lt;10; ++i) or for(int i=0;i&lt;10;++i)for (int i = 0; i &lt; 10; ++i)&#123; ...&#125; TAB间距该使用几个空格作为代码缩进距离从来没有达成一致过。有人喜欢2个空格，有人喜欢4个而有人喜欢8个甚至更多。所以最好使用Tab缩进。它的好处有：可以自定义缩进距离它只需一个字符，所以只要按一次键而不是2,4,8… 如果你要增加缩进（或减少）一片代码，可以选中它们然后按Tab来增加或者Shift+Tab来减少缩进。几乎所有文本编辑器都支持这个操作。在这里Tab指的就是标准的缩进字符。注意： 不要使用空格来缩进代码-用Tab！在VS中把Tab配置成4个空格的距离。 一般注释通用注释前缀TODO : 表示以后别忘了在这里还需要进一步处理BUG: [bugid]：表示这里有个已知bug，解释一下bug情况，如果可以则给出bug id。KLUDGE: 表示这里的代码有点糟糕，解释一下有时间的话将如何改进。TRICKY: 表示以下代码比较奇技淫巧，如果没有仔细思考请不要改动。WARNING: 表示当心某事COMPILER: 表示临时注释掉某些影响编译通过的代码。这些问题最终会被解决的。ATTRIBUTE: value：嵌在注释中的属性的一般形式。你可以自定义属性，它们最终会被VS提取。 单行注释12//Console.WriteLine(\"哈哈\");//Console.ReadKey(); 多行注释1234/* * 注释内容 * 注释内容 */ 文档注释1234/// &lt;summary&gt;/// 文档注释/// 文档注释/// &lt;/summary&gt; 变量声明及命名规范变量：Camel：变量名首单词的首字母小写，其余每个单词首字母单词大写,多用于给变量或字段或方法参数命名。 123var str = \"123\";var highSchoolStudent = \"123\";int num = 5; 常量：1public static const string BaseIpAddress = \"\"; 方法名：Pascal：每个单词的首字母都要大些其余小写，多用于类或方法。尽量用一般名词或动名词。1234public string GetHighSchoolStudent()&#123; &#125;","tags":[{"name":"编码规范","slug":"编码规范","permalink":"https://wxzhongwang.github.io/tags/编码规范/"}]},{"title":"Java 并发进阶常见面试题总结","date":"2019-03-01T02:10:00.000Z","path":"2019/03/01/Java 并发进阶常见面试题总结/","text":"Java 并发进阶常见面试题总结synchronized 关键字synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。(后半句) 任意时刻只有一个线程在访问 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从JVM层面对synchronized较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 项目中怎么使用 synchronized 关键字synchronized关键字最主要的三种使用方式： 修饰实例方法:作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码块前要获得给定对象的锁。 比如以下的：双重校验锁实现单例1234567891011121314151617181920public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Java 并发基础常见面试题总结","date":"2019-03-01T02:10:00.000Z","path":"2019/03/01/Java 并发基础常见面试题总结/","text":"进程和线程何为进程?进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 何为线程?与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 一个 Java 程序的运行是 main 线程和多个其他线程同时运行。 一个进程中可以有多个线程，多个线程共享进程的堆和方法区，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。 进程间彼此相对独立，线程则不一定，因为同一进程中的线程极有可能会相互影响。 为什么程序计数器、虚拟机栈和本地方法栈是线程私有的呢？为什么堆和方法区是线程共享的呢？程序计数器为什么是私有的?程序计数器主要有下面两个作用： 1.字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 2.在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。 所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。 虚拟机栈和本地方法栈为什么是私有的?虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。 一句话简单了解堆和方法区堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象(所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 说说并发与并行的区别?并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)；并行： 单位时间内，多个任务同时执行。 为什么要使用多线程呢?先从总体上来说： 计算机底层角度： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。+ 当代互联网发展趋势角度： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 再深入到计算机底层来探讨： 单核时代： 在单核时代多线程主要是为了提高 CPU 和 IO 设备的综合利用率。举个例子：当只有一个线程的时候会导致 CPU 计算时，IO 设备空闲；进行 IO 操作时，CPU 空闲。我们可以简单地说这两者的利用率目前都是 50%左右。但是当有两个线程的时候就不一样了，当一个线程执行 CPU 计算时，另外一个线程可以进行 IO 操作，这样两个的利用率就可以在理想情况下达到 100%了。 多核时代: 多核时代多线程主要是为了提高 CPU 利用率。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，CPU 只会一个 CPU 核心被利用到，而创建多个线程就可以让多个 CPU 核心被利用到，这样就提高了 CPU 的利用率。 多线程带来的问题并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。 线程的生命周期和状态?线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 当线程执行 wait()方法之后，线程进入WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而TIME_WAITING(超时等待)状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 上下文切换多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。（时间片策略） 概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换会这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"字符型常量和字符串常量的区别","date":"2019-03-01T02:10:00.000Z","path":"2019/03/01/字符型常量和字符串常量的区别/","text":"字符型常量和字符串常量的区别形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小: 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志) (注意： char在Java中占两个字节) 数据类型 大小 包装器类型boolean - Booleanchar 16bits Characterbyte 8bits Byteshort 16bits Shortint 32bits Integerlong 64bits Longfloat 32bits Floatdouble 64bits Double","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Greenplum 从入门到放弃 四","date":"2019-02-04T02:10:00.000Z","path":"2019/02/04/Greenplum 从入门到放弃（四）/","text":"Greenplum 从入门到放弃（四）PostgreSQL与Greenplum的关系PostgreSQLPostgreSQL是一种非常先进的对象–关系型数据库管理系统（ORDBMS），是目前功能最强大，特性最丰富和技术最先进的自由软件数据库系统之一，其某些特性甚至连商业数据库都不具备。 PostgreSQL的特点可以说是数不胜数，称其为最先进的开源软件数据库当之无愧，支持绝大部分的主流数据库特性，主要体现在如下几方面： 函数/存储过程 PostgreSQL对非常丰富的过程类语言提供支持，可以编写自定义函数/存储过程 内置的plpgsql，一种类似Oracle的PLsql的语言 支持的脚本语言有：PL/Lua、PL/LOLCODE、PL/Perl、PL/HP、PL/Python、PL/Ruby、PL/sh、PL/Tcl和PL/Scheme。、 编译语言有C、C++和JAVA。 ·统计语言PL/R 索引 PostgreSQL支持用户定义的索引访问方法，并且内置了Btree、哈希和GiST索引。PostgreSQL中的索引有下面几个特点： 可以从后向前扫描 可以创建表达式索引 部分索引 触发器 触发器是由SQL查询的动作触发的事件。比如，一个INSERT查询可能激活一个检查输入值是否有效的触发器。大多数触发器都只对INSERT或者UPDATE查询有效。PostgreSQL完全支持触发器，可以附着在表上，但是不能在视图上。不过视图可以有规则。多个触发器是按照字母顺序触发的。我们还可以用其他过程语言书写触发器函数，不仅仅PL/PgSQL。 并发管理（MVCC） PostgreSQL的并发管理使用的是一种叫做“MVCC”（多版本并发机制）的机制，这种机制实际上就是现在在众多所谓的编程语言中极其火爆的“Lock Free”，其本质是通过类似科幻世界的时空穿梭的原理，给予每个用户一个自己的“时空”，然后通过原子的“时空”控制来控制时间基线，并以此控制并发更改的可见区域，从而实现近乎无锁的并发，而同时还能在很大程度上保证数据库的ACID特性。 规则（RULE） 规则允许我们对由一个查询生成的查询树进行改写。 数据类型 PostgreSQL支持非常广泛的数据类型，包括： 任意精度的数值类型； 无限长度的文本类型； 几何原语； IPv4和IPv6类型； CIDR块和MAC地址； 数组。 用户还可以创建自己的类型，并且可以利用GiST框架把这些类型做成完全可索引的，比如来自PostGIS的地理信息系统（GIS）的数据类型。 用户定义对象 因为PostgreSQL使用一种基于系统表的可扩展的结构设计，所以PostgreSQL内部的几乎所有对象都可以由用户定义，这些对象包括： 索引； 操作符（内部操作符可以被覆盖）； 聚集函数； 域； 类型转换； 编码转换。 继承 PostgreSQL的表是可以相互继承的。一个表可以有父表，父表的结构变化会导致子表的结构变化，而对子表的插入和数据更新等也会反映到父表中。 其他特性与扩展 二进制和文本大对象存储； 在线备份； TOAST（The Oversized-Attribute Storage Technique）用于透明地在独立的地方保存大的数据库属性，当数据超过一定大小的时候，会自动进行压缩以节省空间； 正则表达式。 此外PostgreSQL还有大量的附加模块和扩展版本，比如，多种不同的主从/主主复制方案: Slony-I； pgcluster； Mammoth replicator； Bucardo。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃 三","date":"2019-02-03T02:10:00.000Z","path":"2019/02/03/Greenplum 从入门到放弃（三）/","text":"Greenplum 从入门到放弃（三）master 和 segment关系Master和Segment其实都是一个单独的PostgreSQL数据库。每一个都有自己单独的一套元数据字典，在这里，Master节点一般也叫主节点，Segment也叫做数据节点。Segment节点与Master节点的通信，通过千兆（或万兆）网卡组成的内部连接（InterConnect），在同一台数据节点机器上可以放多个Segment，不同的Segment节点会被赋予不同的端口，同时，Segment之间也不断地进行着交互。为了实现高可用，每个Segment都有对应的备节点（Mirror Segment），分别存在于不同的机器上。 Client一般只能与Master节点进行交互，Client将SQL发给Master，然后Master对SQL进行分析后，再将其分配给所有的Segment进行操作，并且将汇总结果返回给客户端。 数据库存储对于数据库来说，在性能上磁盘IO很容易成为瓶颈，由于数据库的特性，每一个SQL基本都是对全表数据进行分析，每次处理的数据量非常大，数据基本上都是没有缓存的（数据字典除外），极度消耗IO资源（全表扫描主要都是顺序IO），所以Greenplum对存储的要求比较高。在文件系统的选择上，在Linux下建议使用XFS，在Solaris下建议使用ZFS，对于Raid根据需求选择硬Raid或软Raid，如果需要更大的空间，建议使用Raid5，如果对性能有更高的要求，可以选择Raid 1+0。 网络在确定机器配置的时候，要保证所有机器的网络都是通的，并且每台机器的防火墙都是关闭的，避免存在网络不通的问题。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃 二","date":"2019-02-02T02:10:00.000Z","path":"2019/02/02/Greenplum 从入门到放弃（二）/","text":"Greenplum 从入门到放弃（二）OLTP与OLAP数据库系统一般分为两种类型，一种是面向前台应用的，应用比较简单，但是重吞吐和高并发的OLTP类型；一种是重计算的，对大数据集进行统计分析的OLAP类型。Greenplum属于后者。 OLTP（On-Line TransactionProcessing，联机事务处理）系统也称为生产系统，它是事件驱动的、面向应用的，比如电子商务网站的交易系统就是一个典型的OLTP系统。OLTP的基本特点是： 数据在系统中产生 基于交易的处理系统（Transaction-Based） 每次交易牵涉的数据量很小 对响应时间要求非常高 用户数量非常庞大，主要是操作人员 数据库的各种操作主要基于索引进行 OLAP（On-Line Analytical Processing，联机分析处理）是基于数据仓库的信息分析处理过程，是数据仓库的用户接口部分。OLAP系统是跨部门的、面向主题的，其基本特点是： 本身不产生数据，其基础数据来源于生产系统中的操作数据（OperationalData） 基于查询的分析系统 复杂查询经常使用多表联结、全表扫描等，牵涉的数据量往往十分庞大 响应时间与具体查询有很大关系 用户数量相对较小，其用户主要是业务人员与管理人员 由于业务问题不固定，数据库的各种操作不能完全基于索引进行","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃 一","date":"2019-02-01T02:10:00.000Z","path":"2019/02/01/Greenplum 从入门到放弃（一）/","text":"Greenplum 从入门到放弃（一） Greenplum的性能在数据量为TB级别时表现非常优秀，单机性能相比Hadoop要快好几倍 Greenplum是基于PostgreSQL的一个完善的数据库，在功能和语法上都要比Hadoop上的SQL引擎Hive好用很多，对于普通用户来说更加容易上手。 Greenplum有着完善的工具，相比Hive，整个体系都比较完善，不需要像Hive一样花太多的时间和精力进行改造，非常适合作为一些大型的数据仓库解决方案。 Greenplum能够方便地与Hadoop进行结合，可直接把数据写在Hadoop上，还可以直接在数据库上写MapReduce任务，并且配置简单。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"世界以痛吻我 我却报之以歌","date":"2019-01-29T09:10:10.000Z","path":"2019/01/29/世界以痛吻我，我却报之以歌/","text":"今年发生不好的事，一切都显得不那么美好，就很多事情都不按大家的预想在发展。就一起都感觉不顺利，2018已经不够友好了，还记得刚刚过去的23岁生日，记得自己的愿望是希望，真挚希望所有的亲朋好友能够健康，健康就够了，钱多钱少不重要，就是健康就够了。。。 但是偏偏还是发生了。 世界以痛吻我 我却报之以歌亲人已仙游，未呈儿孙福，幽魂于千里，如何度思量。 2018年06月14日早9：00点，在杭州，还在床上的我猛的收到了家人群的消息，早已患糖尿病多年的舅舅病逝。尽管心里有多不愿意接受，但是心里也算是做好了准备。在五一回家时间便已经被告知舅舅身体日渐消瘦，骨瘦如柴。我还趁着工作之余，抽空去看望了。当看到那一瞬间，我感觉我整个人就不好了，眼泪一下就出来了，整个人像楞住了一般，久久说不出话，手也不知道往哪里放。当舅舅看到我，一把把我拽住，死死捏着，这一幕到现在我也忘不了… 又是在杭州，2019年01月29日早8: 00点，同样的戏码，姨夫病逝，癌症晚期，距离收到通知，也就短短三个多月时间，大概一百多天，原本一个看上去健健康康的人，就突然离开，就像上帝给你的人生突然上了锁，然后把钥匙给扔掉了的感觉… 醒来又是一天，开始干活，累…","tags":[{"name":"个人","slug":"个人","permalink":"https://wxzhongwang.github.io/tags/个人/"}]},{"title":"Hadoop Hive Hbase 简单区别及应用场景","date":"2019-01-11T02:10:00.000Z","path":"2019/01/11/Hadoop Hive Hbase 简单区别及应用场景/","text":"Hadoop Hive Hbase 简单区别及应用场景Hadoop它是一个分布式计算+分布式文件系统，前者其实就是MapReduce，后者是HDFS。后者可以独立运行，前者可以选择性使用，也可以不使用。 Hive通俗的说是一个数据仓库，仓库中的数据是被HDFS管理的数据文件，它支持类似sql语句的功能，你可以通过该语句完成分布式环境下的计算功能，Hive会把语句转换成MapReduce，然后交给Hadoop执行。这里的计算，仅限于查找和分析，而不是更新、增加和删除。它的优势是对历史数据进行处理，用时下流行的说法是离线计算，因为它的底层是MapReduce，MapReduce在实时计算上性能很差。它的做法是把数据文件加载进来作为一个Hive表（或者外部表），让你觉得你的sql操作的是传统的表。 HBase通俗的说，HBase的作用类似于数据库，传统数据库管理的是集中的本地数据文件，而HBase基于Hdfs实现对分布式数据文件的管理，比如增删改查。也就是说，HBase只是利用Hadoop的Hdfs帮助其管理数据的持久化文件（HFile），它跟MapReduce没任何关系。HBase的优势在于实时计算，所有实时数据都直接存入Hbase中，客户端通过API直接访问Hbase，实现实时计算。由于它使用的是nosql，或者说是列式结构，从而提高了查找性能，使其能运用于大数据场景，这是它跟MapReduce的区别。 总结： Hadoop是Hive和HBase的基础，Hive依赖Hadoop HBase仅依赖Hadoop的Hdfs模块。 Hive适用于离线数据的分析，操作的是通用格式的（如通用的日志文件）、被Hadoop管理的数据文件，它支持类sql，比编写MapReduce的java代码来的更加方便，它的定位是数据仓库，存储和分析历史数据 Hbase适用于实时计算，采用列式结构的nosql，操作的是自己生成的特殊格式的HFile、被hadoop管理的数据文件，它的定位是数据库，或者叫DBMS 最后补充一下：Hive可以直接操作Hdfs中的文件作为它的表的数据，也可以使HBase数据库作为它的表","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"大数据","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据/","text":"大数据生命周期 基础设施层，涵盖计算资源、内存与存储和网络互联，具体表现为计算节点、集群、机柜和数据中心。 数据存储和管理层，包括文件系统、数据库和类似YARN的资源管理系统。 计算处理层，如hadoop、MapReduce和Spark 在此之上的各种不同计算范式，如批处理、流处理和图计算等，包括衍生出编程模型的计算模型，如BSP、GAS等 数据分析和可视化基于计算处理层。分析包括简单的查询分析、流分析以及更复杂的分析(如机器学习、图计算等)。查询分析多基于表结构和关系函数，流分析基于数据、事件流以及简单的统计分析，而复杂分析则基于更复杂的数据结构与方法，如图、矩阵、迭代计算和线性代数。一般意义的可视化是对分析结果的展示。但是通过交互式可视化，还可以探索性地提问，使分析获得新的线索，形成迭代的分析和可视化。基于大规模数据的实时交互可视化分析以及在这个过程中引入自动化的因素是目前研究的热点。 大数据技术生态大数据的基本处理流程与传统数据处理流程并无太大差异，主要区别在于：由于大数据要处理大量、非结构化的数据，所以在各处理环节中都可以采用并行处理。目前，Hadoop、MapReduce和Spark等分布式处理方式已经成为大数据处理各环节的通用处理方法。 大数据采集与预处理 存储层 预处理层 采集层 在大数据的生命周期中，数据采集处于第一个环节。根据MapReduce产生数据的应用系统分类，大数据的采集主要有4种来源：管理信息系统、Web信息系统、物理信息系统、科学实验系统。对于不同的数据集，可能存在不同的结构和模式，如文件、XML树、关系表等，表现为数据的异构性。对多个异构的数据集，需要做进一步集成处理或整合处理，将来自不同数据集的数据收集、整理、清洗、转换后，生成到一个新的数据集，为后续查询和分析处理提供统一的数据视图。针对管理信息系统中异构数据库集成技术、Web信息系统中的实体识别技术和DeepWeb集成技术、传感器网络数据融合技术已经有很多研究工作，取得了较大的进展，已经推出了多种数据清洗和质量控制工具。 大数据的存储和管理按数据类型的不同，大数据的存储和管理采用不同的技术路线，大致可以分为3类。 第1类主要面对的是大规模的结构化数据。针对这类大数据，通常采用新型数据库集群。它们通过列存储或行列混合存储以及粗粒度索引等技术，结合MPP(MassiveParallelProcessing)架构高效的分布式计算模式，实现对PB量级数据的存储和管理。这类集群具有高性能和高扩展性特点，在企业分析类应用领域已获得广泛应用; 第2类主要面对的是半结构化和非结构化数据。应对这类应用场景，基于Hadoop开源体系的系统平台更为擅长。它们通过对Hadoop生态体系的技术扩展和封装，实现对半结构化和非结构化数据的存储和管理; 第3类面对的是结构化和非结构化混合的大数据，因此采用MPP并行数据库集群与Hadoop集群的混合来实现对百PB量级、EB量级数据的存储和管理。一方面，用MPP来管理计算高质量的结构化数据，提供强大的SQL和OLTP型服务;另一方面，用Hadoop实现对半结构化和非结构化数据的处理，以支持诸如内容检索、深度挖掘与综合分析等新型应用。这类混合模式将是大数据存储和管理未来发展的趋势。","tags":[{"name":"大数据","slug":"大数据","permalink":"https://wxzhongwang.github.io/tags/大数据/"}]},{"title":"Hadoop概念","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/Hadoop/","text":"Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 Hadoop概念 Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 ==不是为了大数据而大数据== Hadoop 是以一种可靠、高效、可伸缩的方式进行处理的。Hadoop 是可靠的，因为它假设计算元素和存储会失败，因此它维护多个工作数据副本，确保能够针对失败的节点重新分布处理。Hadoop 是高效的，因为它以并行的方式工作，通过并行处理加快处理速度。Hadoop 还是可伸缩的，能够处理 PB 级数据。核心Hadoop的核心就是==HDFS==和==MapReduce===，Hadoop旗下有很多经典子项目，比如HBase、Hive等，这些都是基于HDFS和MapReduce发展出来的。要想了解Hadoop，就必须知道HDFS和MapReduce是什么。HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 用处 搜索引擎 - 设计Hadoop的初衷，为了针对大规模的网页快速建立索引） 大数据存储 - 利用Hadoop的分布式存储能力，例如数据备份、数据仓库等。 大数据处理 - 利用Hadoop的分布式处理能力，例如数据挖掘、数据分析等。 科学研究 - Hadoop是一种分布式的开源框架，对于分布式计算有很大程度地参考价值。 优缺点优点==高可靠性==Hadoop按位存储和处理数据的能力值得信赖。 ==高扩展性==Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。 ==高效性==Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。 ==高容错性==Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。 ==低成本==与一体机、商用数据仓库以及QlikView、Yonghong Z-Suite等数据集市相比，hadoop是开源的，项目的软件成本因此会大大降低。 Hadoop设计对硬件需求比较低，只须运行在低廉的商用硬件集群上，而无需昂贵的高可用性机器上。廉价的商用机也就意味着大型集群中出现节点故障情况的概率非常高。这就要求设计HDFS时要充分考虑数据的可靠性，安全性及高可用性。 缺点==不适合低延迟数据访问== 如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。 改进策略：对于那些有低延时要求的应用程序，HBase是一个更好的选择。通过上层数据管理项目来尽可能地弥补这个不足。在性能上有了很大的提升，它的口号就是goes real time。使用缓存或多master设计可以降低client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延时了，HDFS不是万能的银弹。 ==无法高效存储大量小文件== 因为Namenode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由Namenode的内存大小来决定。一般来说，每一个文件、文件夹和Block需要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB内存。当前来说，数百万的文件还是可行的，当扩展到数十亿时，对于当前的硬件水平来说就没法实现了。还有一个问题就是，因为Maptask的数量是由splits来决定的，所以用MR处理大量的小文件时，就会产生过多的Maptask，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M，那就会有10000个Maptasks，会有很大的线程开销；若每个split为100M，则只有100个Maptasks，每个Maptask将会有更多的事情做，而线程的管理开销将减小很多。 ==不支持多用户写入及任意修改文件== 在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop技术体系","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据相关技术（Hadoop体系）/","text":"Hadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 Hadoop技术体系HadoopHadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。 MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 Map （映射） Reduce (简化)举个例子：假设你的手机通话信息保存在一个HDFS的文件callList.txt中，你想找到你与同事A的所有通话记录并排序。因为HDFS会把callLst.txt分成几块分别存，比如说5块，那么对应的Map过程就是找到这5块所在的5个节点，让它们分别找自己存的那块中关于同事A的通话记录，对应的Reduce过程就是把5个节点过滤后的通话记录合并在一块并按时间排序。MapReduce的计算模型通常把HDFS作为数据来源，很少会用到其它数据来源比如HBase。 Hbase这是Hadoop生态体系中的NOSQL数据库，他的数据是按照key和value的形式存储的并且key是唯一的，所以它能用来做数据的排重，它与MYSQL相比能存储的数据量大很多。所以他常被用于大数据处理完成之后的存储目的地。 HDFS和HBase是依靠外存（即硬盘）的分布式文件存储实现和分布式表存储实现。HDFS是一个分布式的“云存储”文件系统，它会把一个文件分块并分别保存，取用时分别再取出、合并。重要的是，这些分块通常会在3个节点（即集群内的服务器）上各有1个备份，因此即使出现少数节点的失效（如硬盘损坏、掉电等），文件也不会失效。如果说HDFS是文件级别的存储，那HBase则是表级别的存储。HBase是表模型，但比SQL数据库的表要简单的多，没有连接、聚集等功能。HBase的表是物理存储到HDFS的，比如把一个表分成4个HDFS文件并存储。由于HDFS级会做备份，所以HBase级不再备份。MapReduce则是一个计算模型，而不是存储模型；MapReduce通常与HDFS紧密配合。 HiveHive 是一种底层封装了Hadoop 的数据仓库处理工具，使用类SQL的HiveQL语言实现数据查询，所有Hive 的数据都存储在Hadoop 兼容的文件系统（如HDFS）中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下，++因此，Hive不支持对数据的改写和添加，所有的数据都是在加载的时候确定的++。对于会SQL语法的来说就是神器，它能让你处理大数据变的很简单，不会再费劲的编写MapReduce程序。 Spark它是用来弥补基于MapReduce处理数据速度上的缺点，它的特点是把数据装载到内存中计算而不是去读慢的要死进化还特别慢的硬盘。特别适合做迭代运算，所以算法流们特别稀饭它。它是用scala编写的。Java语言或者Scala都可以操作它，因为它们都是用JVM的。 其他相关技术Sqoop这个是用于把Mysql里的数据导入到Hadoop里的。当然你也可以不用这个，直接把Mysql数据表导出成文件再放到HDFS上也是一样的，当然生产环境中使用要注意Mysql的压力。 Flume apache Flume 是一个从可以收集例如日志，事件等数据资源，并将这些数量庞大的数据从各项数据资源中集中起来存储的工具/服务，或者数集中机制。flume具有高可用，分布式，配置工具，其设计的原理也是基于数据流，如日志数据从各种网站服务器上汇集起来存储到HDFS，HBase等集中存储器中。 一般实时系统，所选用组件如下 数据采集 ：负责从各节点上实时采集数据，选用Flume来实现 数据接入 ：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，选用apache的kafka 流式计算 ：对采集到的数据进行实时分析，选用apache的storm 数据输出 ：对分析后的结果持久化，暂定用mysql，另一方面是模块化之后，假如当Storm挂掉了之后，数据采集和数据接入还是继续在跑着，数据不会丢失，storm起来之后可以继续进行流式计算； KafkaKafka的整体架构非常简单，是显式分布式架构，producer、broker（kafka）和consumer都可以有多个。Producer，consumer实现Kafka注册的接口，数据从producer发送到broker，broker承担一个中间缓存和分发的作用。broker分发注册到系统中的consumer。broker的作用类似于缓存，即活跃的数据和离线处理系统之间的缓存。客户端和服务器端的通信，是基于简单，高性能，且与编程语言无关的TCP协议。 Kafka是一种分布式的、基于发布/订阅的消息系统。在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算（KAFKA+STORM+REDIS）。 特点： 消息持久化：通过O(1)的磁盘数据结构提供数据的持久化 高吞吐量：每秒百万级的消息读写 分布式：扩展能力强 多客户端支持：java、php、python、c++ …… 实时性：生产者生产的message立即被消费者可见 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实 现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"流处理、批处理、交互式查询","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/流处理、批处理、交互式查询/","text":"我们将大数据处理按处理时间的跨度要求分为以下几类 基于实时数据流的处理，通常的时间跨度在数百毫秒到数秒之间 基于历史数据的交互式查询，通常时间跨度在数十秒到数分钟之间 复杂的批量数据处理，通常的时间跨度在几分钟到数小时之间","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"消息队列","date":"2018-12-08T03:18:02.000Z","path":"2018/12/08/消息队列/","text":"一、消息队列的基本概念1.1 Broker==Broker== 的概念来自与Apache ActiveMQ，通俗的讲就是消息队列服务器。 1.2 消息生产者和消费者 消息生产者 ==Producer==：发送消息到消息队列。 消息消费者 ==Consumer==：从消息队列接收消息。 1.3 消息模型点对点消息队列模型消息生产者向一个特定的队列发送消息，消息消费者从该队列中接收消息。消息的生产者和消费者可以不同时处于运行状态。每一个成功处理的消息都由消息消费者签收确认（Acknowledge）。 发布订阅消息模型-Topic发布订阅消息模型中，支持向一个特定的主题Topic发布消息，0个或多个订阅者接收来自这个消息主题的消息。在这种模型下，发布者和订阅者彼此不知道对方。实际操作过程中，必须先订阅，再发送消息，而后接收订阅的消息，这个顺序必须保证。 1.4 消息顺序性保证基于Queue消息模型，利用FIFO先进先出的特性，可以保证消息的顺序性。 1.5 消息的ACK确认机制即消息的Ackownledge确认机制：为了保证消息不丢失，消息队列提供了消息Acknowledge机制，即ACK机制，当Consumer确认消息已经消费处理，发送一个ACK给消息队列，此时消息队列便可以删除这个消息了。如果Consumer宕机/关闭，没有发送ACK，消息队列将认为这个消息没有被处理，会将这个消息重新发送给其他的Consumer重新消费处理。 1.6 消息的持久化消息的持久化，对于一些关键的核心业务来说是非常重要的，启用消息持久化后，消息队列宕机重启后，消息可以从持久化存储恢复，消息不丢失，可以继续消费处理。 1.7 消息的同步和异步收发同步消息的收发支持同步收发的方式同步收发场景下，消息生产者和消费者双向应答模式，例如：张三写封信送到邮局中转站，然后李四从中转站获得信，然后在写一份回执信，放到中转站，然后张三去取，当然张三写信的时候就得写明回信地址。消息的接收如果以同步的方式(Pull)进行接收，如果队列中为空，此时接收将处于同步阻塞状态，会一直等待，直到消息的到达。 异步消息的收发同样支持异步方式：异步发送消息，不需要等待消息队列的接收确认。异步接收消息，以Push的方式触发消息消费者接收消息。 1.8 消息的事务支持消息的收发处理支持事务，例如：在任务中心场景中，一次处理可能涉及多个消息的接收、处理，这处于同一个事务范围内，如果一个消息处理失败，事务回滚，消息重新回到队列中。 二、JMS消费服务Java消息服务（Java Message Service，JMS）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 点对点与发布订阅最初是由JMS定义的。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费(多订阅) 。 JMS规范目前支持两种消息模型： 点对点（point to point， queue） 发布/订阅（publish/subscribe，topic） 2.1 点对点：Queue，不可重复消费消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 P2P模式包含三个角色： 消息队列（Queue） 发送者(Sender) 接收者(Receiver) 每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。 2.2 发布/订阅：Topic，可以重复消费消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 支持订阅组的发布订阅模式：发布订阅模式下，当发布者消息量很大时，显然单个订阅者的处理能力是不足的。实际上现实场景中是多个订阅者节点组成一个订阅组负载均衡消费topic消息即分组订阅，这样订阅者很容易实现消费能力线性扩展。可以看成是一个topic下有多个Queue，每个Queue是点对点的方式，Queue之间是发布订阅方式。 2.3 区别点对点模式生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 发布订阅模式发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 三、流行模型对比传统企业型消息队列ActiveMQ遵循了JMS规范，实现了点对点和发布订阅模型，但其他流行的消息队列RabbitMQ、Kafka并没有遵循JMS规范。 3.1 RabbitMQRabbitMQ实现了AQMP协议，AQMP协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同queue，消费端根据queue名称消费消息。RabbitMQ既支持内存队列也支持持久化队列，消费端为推模型，消费状态和订阅关系由服务端负责维护，消息消费完后立即删除，不保留历史消息。 点对点生产端发送一条消息通过路由投递到Queue，只有一个消费者能消费到。 多订阅当RabbitMQ需要支持多订阅时，发布者发送的消息通过路由同时写到多个Queue，不同订阅组消费不同的Queue。所以支持多订阅时，消息会多个拷贝。 3.2 KafkaKafka只支持消息持久化，消费端为拉模型，消费状态和订阅关系由客户端端负责维护，消息消费完后不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了。但是可能产生重复消费的情况。","tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://wxzhongwang.github.io/tags/消息队列/"}]},{"title":"你好，世界","date":"2018-08-31T09:54:54.000Z","path":"2018/08/31/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"},{"name":"博客系统","slug":"博客系统","permalink":"https://wxzhongwang.github.io/tags/博客系统/"}]},{"title":"计算机网络知识回顾","date":"2018-08-10T14:55:00.000Z","path":"2018/08/10/计算机网络知识回顾/","text":"#计算机网络知识回顾 OSI七层模型:被一些大公司甚至一些国家政府支持的OSI失败的原因： 1)OSI的专家缺乏实际经验，他们在完成OSI标准时缺乏商业驱动力 2)OSI的协议实现起来过分复杂，而且运行效率很低 3)OSI制定标准的周期太长，占领市场失败，使得按OSI标准生产的设备无法及时进入市场（20世纪90年代初期，虽然整套的OSI国际标准都已经制定出来，但基于TCP/IP的互联网已经抢先在全球相当大的范围成功运行了）OSI的层次划分不太合理，有些功能在多个层次中重复出现 ###OSI七层结构： 应用层 表示层 会话层 运输层 网络层 数据链路层 物理层 ###TCP\\IP四层协议： 应用层（各种应用层协议TELNET,SMTP,FTP） 运输层（TCP UDP） 网际层（IP） 网络接口层 ###五层协议体系结构： 应用层 运输层(TCP\\UDP) 网络层（IP） 数据链路层 物理层 应用层 应用程序间的网络通信交互规则 应用层的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的HTTP协议，支持电子邮件的SMTP协议等等。我们把应用层交互的数据单元称为报文。 DNS(Domain Name System)域名系统是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的Web网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。 HTTP协议超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。 传输层 传递应用层报文 运输层的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 -传输控制协议TCP（Transmisson Control Protocol）–提供面向连接的，可靠的数据传输服务。-用户数据协议UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 网络层 打包报文段或用户数据，并找寻合适的路由，进而找到目标主机 网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在TCP/IP体系结构中，由于网络层使用IP协议，因此分组也叫IP数据报，简称数据报。 数据链路层 两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层. 物理层 在物理层上所传送的数据单位是比特。实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。","tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://wxzhongwang.github.io/tags/计算机网络/"}]},{"title":"计算机网络相关问题","date":"2018-08-10T14:55:00.000Z","path":"2018/08/10/计算机网络相关问题/","text":"计算机网络相关问题①TCP三次握手和四次挥手三次握手（创建）1）客户端–发送带有SYN标志的数据包–一次握手–服务端2）服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端3）客户端–发送带有带有ACK标志的数据包–三次握手–服务端 第二次握手中，服务端为何要传回SYN（发起一个新链接）？ 接收端传回发送端所发送的SYN是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 双方通信无误必须是两者互相发送信息都无误。传了SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要ACK信号来进行验证。 四次挥手（断开）1）客户端-发送一个FIN，用来关闭客户端到服务器的数据传送2）服务器-收到这个FIN，发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号3）服务器-关闭与客户端的连接，发送一个FIN给客户端4）客户端-发回ACK报文确认，并将确认序号设置为收到序号加1 ②在浏览器中输入url地址-&gt;&gt;显示主页的过程(完整描述)域名解析 –&gt; 发起TCP的3次握手 –&gt; 建立TCP连接后发起http请求 –&gt; 服务器响应http请求，浏览器得到html代码 –&gt; 浏览器解析html代码，并请求html代码中的资源（如js、css、图片等） –&gt; 浏览器对页面进行渲染呈现给用户 ③HTTP和HTTPS的区别④TCP、UDP协议的区别UDP: UDP在传送数据之前不需要先建立连接，远地主机在收到UDP报文后，不需要给出任何确认。虽然UDP不提供可靠交付，但在某些情况下UDP确是一种最有效的工作方式（一般用于即时通信），比如： QQ语音 QQ视频 、直播等等 1）无连接2）尽力交付，不保证可靠3）面向报文4）没有拥塞控制（网络出现拥塞不会使源主机的发送速率降低，对实时应用很有用）5）支持一对一，一对多，多对多交互通信6）UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 TCP: TCP提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或多播服务。由于TCP要提供可靠的，面向连接的运输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接，四次挥手，用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP一般用于文件传输、发送和接收邮件、远程登录等场景。 1)面向连接（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）2)TCP连接只能是点对点的（一对一）3)可靠(通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达)4)TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据5)面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 ⑤常见的状态码1XX informational（信息性状态码） 请求正在处理 2XX Success(成功状态码) 请求正常处理完毕 3XX Redirection(重定向状态码) 需要进行附加操作才能完成请求 4XX Client Error(客户端错误状态码) 服务无法处理请求 5XX Server Error（服务端错误状态码） 服务端处理请求出错","tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://wxzhongwang.github.io/tags/计算机网络/"}]},{"title":"sass sass-loader","date":"2018-06-12T08:45:02.000Z","path":"2018/06/12/windows下安装node-sass和sass-loader失败/","text":"window下无法安装sass sass-loader node-sass安装失败的原因是网络限制导致无法下载.node文件 推荐方法：使用淘宝镜像 1npm set sass_binary_site=https://npm.taobao.org/mirrors/node-sass/ or 12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install node-sass sass-loader -S 其他翻墙、手动导入文件的方式不推荐。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"JVM 相关问题","date":"2018-06-08T04:22:30.000Z","path":"2018/06/08/JVM相关/","text":"JVM相关问题 1.JVM内存模型，GC机制和原理。 2.GC分两种，Minor GC和 Full GC有什么区别？什么时候会触发Full GC?分别采用什么算法？ 3.JVM里有几种classloader,为什么会有多种？ 4.什么是双亲委派机制？介绍一些运作过程，双亲委派模式的好处？ 5.什么情况下我们需要破坏双亲委派模型？ 6.常见的JVM调优有哪些？可以具体到那个参数，调成什么值？ 7.JVM虚拟机内存划分、类加载器、垃圾收集算法、垃圾收集器、class文件结构是如何解析的？","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Zookeeper通知机制","date":"2018-06-06T02:10:00.000Z","path":"2018/06/06/Zookeeper通知机制/","text":"Zookeeper通知机制客户端注册监听(watch)它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。 Zookeeper做了什么？1.命名服务 2.配置管理 3.集群管理 4.分布式锁 5.队列管理 命名服务在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现。 命名服务是分布式系统中比较常见的一类场景。在分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等——这些我们都可以统称它们为名字（Name），其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等。 Java中的JNDI便是一种典型的命名服务。JNDI是Java命名与目录接口（Java Naming and Directory Interface）的缩写，是J2EE体系中重要的规范之一，标准的J2EE容器都提供了对JNDI规范的实现。因此，在实际开发中，开发人员常常使用应用服务窗口自带的JNDI实现来完成数据源的配置与管理。使用JNDI方式后，开发人员可以完全不需要关心数据库相关的任何信息，包括数据库类型、JDBC驱动类型及数据库账户等。 ZooKeeper的命名服务有两个应用方向： ZooKeeper提供类似JNDI服务，都能够帮助应用系统通过一个资源引用的方式来实现对资源的定位与实用。 利用ZooKeeper顺序节点的特性，制作分布式的ID生成器。 配置管理程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。 集群管理所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了。 对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。 Zookeeper分布式锁有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。 队列管理两种类型的队列： 1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 2、队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://wxzhongwang.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper分布式协调服务","date":"2018-06-05T02:10:00.000Z","path":"2018/06/05/Zookeeper/","text":"Zookeeper分布式协调服务分布式特点： 凡是分布式就有路由 凡是路由就有负载 凡是有负载就一定会有宕机 凡是分布式就有链接 凡是链接就有安全 分布式、开源、分布式应用程序协调服务。包括配置维护、域名服务、分布式同步、组服务等。 单机模式：Mode: standaloneC/S架构Znode 称为zk节点数据 对节点基本操作create /monkey xxx 创建节点set /monkey xxxxa 更新节点get /zk_test 获取节点delete /zk_test 一般删除（若里面不为空，无法删除）rmr /zk_test 递归删除 节点类型： 持久节点 持久有序 临时节点 临时有序 watcher(监听作用)get /zk_test watch 监听节点数据，一旦发生改变，可以通知 源码分析jps 查看进程号 ===》 查看方法名12345C:\\Users\\Administrator&gt;jps1748 Bootstrap4788 QuorumPeerMain2424 ZooKeeperMain3116 Jps ZooKeeperMain客户端入口QuorumPeerMain服务端源码 通过源码发现：help 帮助history 历史 create -s /monkey xxxxx 持久: 当客户端断开时，znode节点不会被删除 临时: 当客户端断开时，znode节点自动删除 利用Watch机制实现 》》》 分布式配置中心 利用临时有序节点（失效删除，自增等特点） 》》》分布式任务调度 》》》分布式服务注册与订阅 znode节点属性：cZxid = 0x2ctime = Tue Jun 04 23:21:32 CST 2019mZxid = 0x2mtime = Tue Jun 04 23:21:32 CST 2019pZxid = 0xacversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 集群模式 （选举）zk一般部署奇数个 zoo.cfg文件修改，2888端口，添加集群机器server.1=196.128.3.1:2888:3888server.2=196.128.3.2:2888:3888server.3=196.128.3.3:2888:3888 选举模式 ==&gt; 过半选举（过半原则）Mode: leaderMode: follower 查看状态：sh zkServer.sh status","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://wxzhongwang.github.io/tags/Zookeeper/"}]},{"title":"Spring源码","date":"2018-06-03T04:00:00.000Z","path":"2018/06/03/Spring源码/","text":"#Spring源码 spring-jcl日志源码分析1.spring的基本应用和spring源码的编译 2.java混乱的日志系统，JUL,JCL,log4j,slf4j spring aop源码分析1.AspectJ和spring AOP, aspectj的静态织入 2.JDK动态代理的源码分析，JDK如何操作字节码 3.spring通过cglib完成AOP,cglib如何完成方法拦截 4.AnnotationAwareAspectJAutoProxyCreator是如何实现代理织入的 spring IOC、AOP、MVC源码分析1.BeanDefinition作用，如何改变bean的行为 2.BeanDefinitionRegistry的作用，源码分析 3.BeanNameGenerator如何改变beanName的生成策略 4.BeanPostProcessor在bean实例化过程中可以做什么？经典应用场景有哪些？","tags":[{"name":"Spring","slug":"Spring","permalink":"https://wxzhongwang.github.io/tags/Spring/"}]},{"title":"Spring Boot 介绍","date":"2018-06-03T04:00:00.000Z","path":"2018/06/03/Spring Boot/","text":"Spring Boot 介绍Spring Boot 提供了一组工具只需要极少的配置就可以快速的构建并启动基于 Spring 的应用程序。解决了传统 Spring 开发需要配置大量配置文件的痛点，同时 Spring Boot 对于第三方库设置了合理的默认值，可以快速的构建起应用程序。当然 Spring Boot 也可以轻松的自定义各种配置，无论是在开发的初始阶段还是投入生成的后期阶段。 Spring Boot 优点1.快速的创建可以独立运行的 Spring 项目以及与主流框架的集成。 2.使用嵌入式的 Servlet 容器，用于不需要打成war包。 3.使用很多的启动器（Starters）自动依赖与版本控制。 4.大量的自动化配置，简化了开发，当然，我们也可以修改默认值。 5.不需要配置 XML 文件，无代码生成，开箱即用。 6.准生产环境的运行时应用监控。 7.与云计算的天然集成。","tags":[{"name":"Spring","slug":"Spring","permalink":"https://wxzhongwang.github.io/tags/Spring/"}]},{"title":"JVM相关问题","date":"2018-05-08T04:22:30.000Z","path":"2018/05/08/JVM/","text":"JVM,什么是JVM? Java语言虚拟机后，Java语言在不同平台上运行时不需要重新编译。Java语言使用Java虚拟机屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。 Java虚拟机在软件层层面屏蔽了底层硬件、底层指令的细节。跨平台：程序可以运行基于不同平台版本的jvm就可以了。 数据类型Java虚拟机中，数据类型可以分为两类：基本类型和引用类型。基本类型的变量保存原始值，即：他代表的值就是数值本身；而引用类型的变量保存引用值。“引用值”代表了某个对象的引用，而不是对象本身，对象本身存放在这个引用值所表示的地址的位置。 基本类型包括：byte,short,int,long,char,float,double,Boolean 引用类型包括：类类型，接口类型和数组。 App.java 源码 编译时环境（jdk） App.class 字节码 JVMJava虚拟机：在软件层层面屏蔽了底层硬件、底层指令的细节。 运行时环境 (jre) 操作系统可以运行的文件 机器码 类加载器 ClassLoader JVM将class文件加载至内存模块。运行时数据区 Runtime Data Area 线程共享区： Heap堆， Method Area方法区 线程独占区： 程序计数器，虚拟机栈，本地方法栈 程序最小单元：线程","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"nginx.conf中文详解","date":"2018-05-06T14:22:22.000Z","path":"2018/05/06/Nginx配置文件nginx.conf中文详解/","text":"Nginx配置文件nginx.conf中文详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329#定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /usr/local/nginx/logs/error.log info;#进程pid文件pid /usr/local/nginx/logs/nginx.pid;#指定进程可以打开的最大描述符：数目#工作模式与连接数上限#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。worker_rlimit_nofile 65535;events&#123; #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. open_file_cache_errors on;&#125; #设定http服务器，利用它的反向代理功能提供负载均衡支持http&#123; #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream jh.w3cschool.cn &#123; #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend &#123; # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #&#125; #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend &#123; # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #&#125; #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend &#123; # server server1; # server server2; # fair; #&#125; #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend &#123; # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #&#125; #tips: #upstream bakend&#123;#定义负载均衡设备的Ip及设备状态&#125;&#123; # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #&#125; #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 &#125; #虚拟主机的配置 server &#123; #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.w3cschool.cn w3cschool.cn; index index.html index.htm index.php; root /data/www/w3cschool; #对******进行负载均衡 location ~ .*.(php|php5)?$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ &#123; expires 10d; &#125; #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ &#123; expires 1h; &#125; #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for'; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 \"/\" 启用反向代理 location / &#123; proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic \"NginxStatus\"; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 &#125; #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ &#123; expires 15d; &#125; location ~ .*.(js|css)?$ &#123; expires 1h; &#125; &#125;&#125;","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]},{"title":"Nginx的负载均衡策略","date":"2018-05-06T14:22:22.000Z","path":"2018/05/06/Nginx的负载均衡策略(粗略)/","text":"Nginx的负载均衡策略轮询（默认）:每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。1234upstream backserver &#123; server 192.168.0.14; server 192.168.0.15; &#125; 指定权重指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。1234upstream backserver &#123; server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; &#125; IP绑定 ip_hash每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。12345upstream backserver &#123; ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; &#125; fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。12345upstream backserver &#123; server 192.168.0.14:88; server 192.168.0.15:80; fair; &#125; url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。1234567upstream backserver &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; &#125;","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]},{"title":"Nginx工作原理","date":"2018-05-06T14:22:22.000Z","path":"2018/05/06/Nginx的accept_mutex配置/","text":"Nginx的accept_mutex配置通常多数人不会注意Nginx的accept_mutex配置，不过实际上它对系统的吞吐量有一定的影响。 123events &#123; accept_mutex off; &#125; 当一个新连接到达时:如果激活了accept_mutex，那么多个Worker将以串行方式来处理，其中有一个Worker会被唤醒，其他的Worker继续保持休眠状态；如果没有激活accept_mutex，那么所有的Worker都会被唤醒，不过只有一个Worker能获取新连接，其它的Worker会重新进入休眠状态，这就是「惊群问题」。 Nginx缺省激活了accept_mutex，也就是说不会有惊群问题，但真的有那么严重么？ 简单点说：Apache动辄就会启动成百上千的进程，如果发生惊群问题的话，影响相对较大；但是对Nginx而言，一般来说，worker_processes会设置成CPU个数，所以最多也就几十个，即便发生惊群问题的话，影响相对也较小。 另：高版本的Linux中，accept不存在惊群问题，不过epoll_wait等操作还有。 很形象： 假设你养了一百只小鸡，现在你有一粒粮食，那么有两种喂食方法： 你把这粒粮食直接扔到小鸡中间，一百只小鸡一起上来抢，最终只有一只小鸡能得手，其它九十九只小鸡只能铩羽而归。这就相当于关闭了accept_mutex。你主动抓一只小鸡过来，把这粒粮食塞到它嘴里，其它九十九只小鸡对此浑然不知，该睡觉睡觉。这就相当于激活了accept_mutex。可以看到此场景下，激活accept_mutex相对更好一些，让我们修改一下问题的场景，我不再只有一粒粮食，而是一盆粮食，怎么办？ 此时如果仍然采用主动抓小鸡过来塞粮食的做法就太低效了，一盆粮食不知何年何月才能喂完，大家可以设想一下几十只小鸡排队等着喂食时那种翘首以盼的情景。此时更好的方法是把这盆粮食直接撒到小鸡中间，让它们自己去抢，虽然这可能会造成一定程度的混乱，但是整体的效率无疑大大增强了。 accept_mutex会轮流来选择worker进程。Nginx默认开启了accept_mutex。Nginx缺省激活了accept_mutex，是一种保守的选择。如果关闭了它，可能会引起一定程度的惊群问题，表现为上下文切换增多（sar -w）或者负载上升，但是如果你的网站访问量比较大，为了系统的吞吐量，我还是建议大家关闭它。","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]},{"title":"Nginx工作原理","date":"2018-05-01T08:22:22.000Z","path":"2018/05/01/nginx工作原理/","text":"Nginx工作原理当我们在操作Nginx的时候，Nginx内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供80端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个worker进程都是从master进程fork过来，在master进程里面，先建立好需要listen的socket（listenfd）之后，然后再fork出多个worker进程。所有worker进程的listenfd会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有worker进程在注册listenfd读事件前抢accept_mutex，抢到互斥锁的那个进程注册listenfd读事件，在读事件里调用accept接受该连接。当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由worker进程来处理，而且只在一个worker进程中处理。 一个请求在一个worker进程中处理。同时一个worker只有一个主线程。（worker进程通过监听共享套接字接受新请求） 进程模型进程模型有什么好处呢？ 首先，对于每个worker进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master进程则很快启动新的worker进程。 高并发那是如何解决高并发的呢？每个 worker 里面只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？ Nginx 采用了异步非阻塞的方式来处理请求 Nginx供了一种机制，同时监控多个事件(具体到系统调用就是像select、poll、epoll、kqueue这样的系统调用，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿epoll为例，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回EAGAIN时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在epoll里面等着。我们就可以并发处理大量的并发了。 线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]},{"title":"tomcat源码解析","date":"2018-04-20T04:00:00.000Z","path":"2018/04/20/tomcat源码解析/","text":"tomcat源码解析1.tomcat的总体概述和tomcat的启动流程源码分析 2.tomcat当中的web请求源码分析？一个http请求是如何请求到tomcat的？tomcat如何处理的？ 3.tomcat的协议分析，从源码分析tomcat当中各种协议详细配置的意义。 4.tomcat和apache、nginx等主流静态资源服务器的搭配使用 5.tomcat的性能调优？生产环境下如何让tomcat容器的性能达到最高","tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://wxzhongwang.github.io/tags/Tomcat/"}]},{"title":"Thread实现多线程三","date":"2018-04-11T02:10:00.000Z","path":"2018/04/11/Thread实现多线程三/","text":"Thread实现多线程三接上文，关系到线程运行状态的几个方法： 6）interrupt方法 interrupt，顾名思义，即中断的意思。单独调用interrupt方法可以使得处于阻塞状态的线程抛出一个异常，也就说，它可以用来中断一个正处于阻塞状态的线程；另外，通过interrupt方法和isInterrupted()方法来停止正在运行的线程。 下面看一个例子：12345678910111213141516171819202122232425262728293031import java.io.IOException;public class ThreadInterupt &#123; public static void main(String[] args) throws IOException &#123; ThreadInterupt test = new ThreadInterupt(); MyThread thread = test.new MyThread(); //线程开始 System.out.println(\"进入线程\" + Thread.currentThread().getName()); thread.start(); try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; thread.interrupt(); &#125; class MyThread extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"进入睡眠状态\"); Thread.currentThread().sleep(10000); System.out.println(\"线程\" + Thread.currentThread().getName() + \"睡眠完毕\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"得到中断异常\"); &#125; System.out.println(\"线程\" + Thread.currentThread().getName() + \"run方法执行完毕\"); &#125; &#125;&#125; 执行结果 ：1234进入线程main线程Thread-0进入睡眠状态线程Thread-0得到中断异常线程Thread-0run方法执行完毕 从这里可以看出，在主线程中,通过interrupt方法可以中断处于阻塞状态的线程。 那么能不能中断处于非阻塞状态的线程呢？看下面这个例子： 123456789101112131415161718192021222324252627282930import java.io.IOException;public class InteruptRunningThread &#123; public static void main(String[] args) throws IOException &#123; InteruptRunningThread test = new InteruptRunningThread(); //线程开始 System.out.println(\"进入线程\" + Thread.currentThread().getName()); MyThread thread = test.new MyThread(); thread.start(); try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; thread.interrupt(); &#125; class MyThread extends Thread&#123; @Override public void run() &#123; int i = 0; while(i &lt; Integer.MAX_VALUE)&#123; System.out.println(\"while循环, i = \" + i + \"\\r\\n\"); i++; &#125; &#125; &#125;&#125; 执行结果：123456while循环, i = 394572while循环, i = 394573while循环, i = 394574...//还在继续 运行该程序会发现，while循环会一直运行直到变量i的值超出Integer.MAX_VALUE。所以说直接调用interrupt方法不能中断正在运行中的线程。 但是如果配合isInterrupted()能够中断正在运行的线程，因为调用interrupt方法相当于将中断标志位置为true，那么可以通过调用isInterrupted()判断中断标志是否被置位来中断线程的执行。比如下面这段代码： 1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class InteruptRunningThread &#123; public static void main(String[] args) throws IOException &#123; InteruptRunningThread test = new InteruptRunningThread(); //线程开始 System.out.println(\"进入线程\" + Thread.currentThread().getName()); MyThread thread = test.new MyThread(); thread.start(); try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; thread.interrupt(); System.out.println(\"结束线程\" + Thread.currentThread().getName()); &#125; class MyThread extends Thread&#123; @Override public void run() &#123; int i = 0; /*while(i &lt; Integer.MAX_VALUE)&#123; System.out.println(\"while循环, i = \" + i + \"\\r\\n\"); i++; &#125;*/ while(!isInterrupted() &amp;&amp; i&lt;Integer.MAX_VALUE)&#123; System.out.println(i+\" while循环\"); i++; &#125; &#125; &#125;&#125; 执行结果：1234138537 while循环138538 while循环138539 while循环结束线程main 但是一般情况下不建议通过这种方式来中断线程，一般会在MyThread类中增加一个属性 isStop来标志是否结束while循环，然后再在while循环中判断isStop的值。那么就可以在外面通过调用setStop方法来终止while循环。12345678910111213141516class MyThread extends Thread&#123; private volatile boolean isStop = false; @Override public void run() &#123; int i = 0; while(!isStop &amp;&amp; i &lt; Integer.MAX_VALUE)&#123; System.out.println(\"while循环, i = \" + i + \"\\r\\n\"); i++; &#125; &#125; public void setStop(boolean stop)&#123; this.isStop = stop; &#125;&#125; 7）stop方法 stop方法已经是一个废弃的方法，它是一个不安全的方法。因为调用stop方法会直接终止run方法的调用，并且会抛出一个ThreadDeath错误，如果线程持有某个对象锁的话，会完全释放锁，导致对象状态不一致。所以stop方法基本是不会被用到的。 8）destroy方法 destroy方法也是废弃的方法。基本不会被使用到。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Mock.js","date":"2018-04-08T05:22:02.000Z","path":"2018/04/08/Mockjs/","text":"目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. Mock.jsMockjs是什么?目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. 安装Mockjs123npm install mockjs -S or npm install mockjs -D 引用MockjsMock.js暴露了一个全局的Mock对象, 我们只需要将Mock对象引入到文件中, 调用Mock对象的方法即可 CommonJS的引入方式 12345678910//CommonJS引入let Mock = require('mockjs) //调用Mock.mock()方法模拟数据let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); ES6的引入方式 123456789//ES6的引入方式import Mock from 'mockjs' let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); 简单用法Mock对象提供了4个方法, 分别是 Mock.mock() Mock.setup() Mock.valid Mock.toJSONSchema() 以及一个工具库 Mock.Random. 其中我们经常使用到的就是Mock.mock()和Mock.Random.","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Java线程池工作原理","date":"2018-04-08T04:18:45.000Z","path":"2018/04/08/Java线程池工作原理/","text":"Java线程池工作原理Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池，所以我们就要认识并弄懂线程池，以便于更好为业务场景服务。(异步与并发) 一、线程池的好处在开发过程中，合理地使用线程池大致有3个好处： 第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 但是，要做到合理利用线程池，必须对其实现原理了如指掌。 二、线程池工作流程1）当提交一个新任务到线程池时，线程池判断corePoolSize线程池是否都在执行任务，如果有空闲线程，则创建一个新的工作线程来执行任务，直到当前线程数等于corePoolSize； 2）如果当前线程数为corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行； 3）如果阻塞队列满了，那就创建新的线程执行当前任务，直到线程池中的线程数达到maxPoolSize，这时再有任务来，由饱和策略来处理提交的任务。 三、线程池参数下面是ThreadPoolExecutor类的构造方法传参数1234567891011121314public ThreadPoolExecutor(int corePoolSize, #核心线程数int maximumPoolSize, #最大线程数long keepAliveTime, #达到最大线程数数时候，线程池的工作线程空闲后，保持存活的时间TimeUnit unit, #keepAliveTime单位BlockingQueue&lt;Runnable&gt; workQueue #阻塞队列RejectedExecutionHandler handler #饱和策略) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125;new ThreadPoolExecutor(6 ,12, 5L, TimeUnit.SECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(10),new ThreadPoolExecutor.CallerRunsPolicy()); 比如corePoolSize为6，maximumPoolSize为12，keepAliveTime为5秒，队列长度为10；提交任务数达到核心线程数6时候，新来的任务就会被放入LinkedBlockingQueue阻塞队列。当队列任务数达到10个时候，就会创建新线程执行任务，直到达到maximumPoolSize数量12。如果还有新来的任务，由策略来处理提交的任务；如果没有，线程池空闲时候，超过5秒，创建的maximumPoolSize，就会被销毁。 四、阻塞队列阻塞队列BlockingQueue接口，从jdk1.5开始，有四个实现类，jdk8亦是如此 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue，静态工厂方法Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个列。 PriorityBlockingQueue：一个具有优先级的无限阻塞队列。 五、饱和策略当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。在JDK 1.5中Java线程池框架提供了以下4种策略。 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 当然，也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化存储不能处理的任务。 六、向线程池提交任务可以使用两个方法向线程池提交任务，分别为execute()和submit()方法 1、execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功 2、submit()方法用于提交需要返回值的任务。 线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get(long timeout，TimeUnit unit)，在指定的时间内会等待任务执行，超时则抛出超时异常，等待时候会阻塞当前线程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.mine.test;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ThreadPoolTest &#123; public static void main(String[] args) &#123; // 新建线程池 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(6, // 核心线程数 12, // 最大线程数 5L, // KeepAlive Time long TimeUnit.SECONDS, // TimeOut new LinkedBlockingQueue&lt;Runnable&gt;(10), // 阻塞队列 new ThreadPoolExecutor.CallerRunsPolicy()// 饱和策略 ); // 向线程池提交任务 // 1、execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功 threadPoolExecutor.execute(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub System.out.println( \"执行当前线程体,线程名： \" + Thread.currentThread().getName() + \"当前:\" + System.currentTimeMillis()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;); Future&lt;?&gt; future = threadPoolExecutor.submit(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub System.out.println(\"执行当前线程体,线程名： \" + Thread.currentThread().getName()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;); /** * 线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值， * get()方法会阻塞当前线程直到任务完成， 而使用get(long timeout，TimeUnit * unit)，在指定的时间内会等待任务执行，超时则抛出超时异常，等待时候会阻塞当前线程 */ try &#123; // 阻塞当前线程，直到任务完成 Object obj = future.get(); // 当前线程等待执行结果的返回值，延迟2s Object obj2 = future.get(10, TimeUnit.MINUTES); &#125; catch (InterruptedException e) &#123; // 处理中断异常 // TODO: handle exception &#125; catch (ExecutionException e) &#123; // 处理执行异常 // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (TimeoutException e) &#123; // 处理超时异常 e.printStackTrace(); &#125; finally &#123;// 关闭线程池 threadPoolExecutor.shutdown(); &#125; threadPoolExecutor.shutdown(); &#125;&#125; 七、关闭线程池ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程 shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务。 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务。 只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。 因此，判断线程池所有线程是否执行完成，可以这样写：1234567while(true)&#123;//死循环 if(threadPool.isTerminated()) &#123; //执行自己的操作 break;//true停止 &#125; Thread.sleep(500);//休眠500继续循环&#125; shutdown，只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程，等待执行任务的线程完成。 shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表。 八、线程池状态线程池有五种运行状态：123456789101112131415161718192021222324252627282930311、RUNNING(1) 状态说明：线程池处在RUNNING状态时，能够接收新任务，以及对已添加的任务进行处理。(2) 状态切换：线程池的初始化状态是RUNNING。线程池被一旦被创建，就处于RUNNING状态，且线程池中的任务数为02、 SHUTDOWN(1) 状态说明：线程池处在SHUTDOWN状态时，不接收新任务，但能处理已添加的任务。(2) 状态切换：调用线程池的shutdown()接口时，线程池由RUNNING -&gt; SHUTDOWN。3、STOP(1) 状态说明：线程池处在STOP状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。(2) 状态切换：调用线程池的shutdownNow()接口时，线程池由(RUNNING or SHUTDOWN ) -&gt; STOP。4、TIDYING(1) 状态说明：当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现。(2) 状态切换：当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN-&gt;TIDYING。当线程池在STOP状态下，线程池中执行的任务为空时，就会由STOP -&gt; TIDYING。5、 TERMINATED(1) 状态说明：线程池彻底终止，就变成TERMINATED状态。(2) 状态切换：线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -&gt; TERMINATED。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Java对象的大小","date":"2018-04-08T04:18:45.000Z","path":"2018/04/08/Java对象的大小/","text":"Java对象的大小基本数据的类型的大小是固定的，这里就不多说了。对于非基本类型的Java对象，其大小就值得商榷。 在Java中，一个空Object对象的大小是8byte，这个大小只是保存堆中一个没有任何属性的对象的大小。看下面语句： 基本类型包括：byte, short, int, long, char, float, double, Boolean 引用类型包括：类类型，接口类型和数组。 1Object ob = new Object(); 这样在程序中完成了一个Java对象的生命，但是它所占的空间为：4byte+8byte。4byte是因为在程序中，创建一个对象，Java栈中保存引用的所需要的空间。而那8byte则是Java堆中对象的信息。因为所有的Java非基本类型的对象都需要默认继承Object对象，因此不论什么样的Java对象，其大小都必须是大于8byte。 1234567891011Class NewObject &#123; int count; boolean flag; Object ob;&#125;其大小为：空对象大小(8byte)+int大小(4byte)+Boolean大小(1byte)+空Object引用的大小(4byte)=17byte。但是因为Java在对对象内存分配时都是以8的整数倍来分，因此大于17byte的最接近8的整数倍的是24，因此此对象的大小为24byte。 这里需要注意一下基本类型的包装类型的大小。因为这种包装类型已经成为对象了，因此需要把他们作为对象来看待。包装类型的大小至少是12byte（声明一个空Object至少需要的空间），而且12byte没有包含任何有效信息，同时，因为Java对象大小是8的整数倍，因此一个基本类型包装类的大小至少是16byte。这个内存占用是很恐怖的，它是使用基本类型的N倍（N&gt;2），有些类型的内存占用更是夸张（随便想下就知道了）。因此，可能的话应尽量少使用包装类。在JDK5.0以后，因为加入了自动类型装换，因此，Java虚拟机会在存储方面进行相应的优化。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Dubbo核心角色","date":"2018-04-07T02:10:00.000Z","path":"2018/04/07/Dubbo深入/","text":"Dubbo核心角色核心角色： 服务提供者者， 服务消费者， 注册中心， 监控中心 服务提供者： 提供服务的接口 （API） 实现服务（实现类） 注册服务(远程服务，本地注册) 暴露服务（比如启动tomcat） 注册中心： 保存服务名与服务器地址映射的关系， 服务地址变更主动通知服务消费者 服务消费者： 启动时从服务中心获取服务地址并缓存 根据负载均衡策略选出一个服务地址进行服务调用。 监控中心：统计服务的调用次数和调用时间的监控中心","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://wxzhongwang.github.io/tags/Dubbo/"}]},{"title":"Dubbo","date":"2018-04-07T02:10:00.000Z","path":"2018/04/07/Dubbo/","text":"Dubbo高性能Java RPC框架.Apache Dubbo |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。 HTTP/TCP 都属于RPC 特性一览面向接口代理的高性能RPC调用提供高性能的基于代理的远程调用能力，服务以接口为粒度，为开发者屏蔽远程调用底层细节。 智能负载均衡内置多种负载均衡策略，智能感知下游节点健康状况，显著减少调用延迟，提高系统吞吐量。 服务自动注册与发现支持多种注册中心服务，服务实例上下线实时感知。 高度可扩展能力遵循微内核+插件的设计原则，所有核心能力如Protocol、Transport、Serialization被设计为扩展点，平等对待内置实现和第三方实现。 运行期流量调度内置条件、脚本等路由策略，通过配置不同的路由规则，轻松实现灰度发布，同机房优先等功能。 可视化的服务治理与运维提供丰富服务治理、运维工具：随时查询服务元数据、服务健康状态及调用统计，实时下发路由策略、调整配置参数。","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://wxzhongwang.github.io/tags/Dubbo/"}]},{"title":"Nginx","date":"2018-04-06T08:22:22.000Z","path":"2018/04/06/nginx/","text":"Nginxnginx是一个轻量级的Web服务器/反向代理服务器以及电子邮件（IMAP/POP3/SMTP）代理服务器，特点：占内存少，并发能力强。是俄罗斯人编写的十分轻量级的 HTTP 服务器。 基本特点：1.处理静态文件，索引文件以及自动索引；打开文件描述符缓冲。2.无缓存的反向代理加速，简单的负载均衡和容错。3.FastCGI,简单的负载均衡和容错。4.模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个SSI，则这项处理可以并行运行，而不需要相互等待。5.支持 SSL 和 TLSSNI。 优势：Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 Nginx 具有很高的稳定性。其它 HTTP 服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前 apache 一旦上到 200 个以上进程，web响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。Nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 Nginx 来说基本上是毫无用处的。就稳定性而言,Nginx 比 lighthttpd 更胜一筹。 Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。 Nginx 采用 master-slave 模型,能够充分利用 SMP 的优势，且能够减少工作进程在磁盘 I/O 的阻塞延迟。当采用 select()/poll() 调用时，还可以限制每个进程的连接数。 Nginx 代码质量非常高，代码很规范，手法成熟，模块扩展也很容易。特别值得一提的是强大的 Upstream 与 Filter 链。Upstream 为诸如 reverse proxy,与其他服务器通信模块的编写奠定了很好的基础。而 Filter 链最酷的部分就是各个 filter 不必等待前一个 filter 执行完毕。它可以把前一个 filter 的输出做为当前 filter 的输入，这有点像 Unix 的管线。这意味着，一个模块可以开始压缩从后端服务器发送过来的请求，且可以在模块接收完后端服务器的整个请求之前把压缩流转向客户端。 Nginx 采用了一些 os 提供的最新特性如对 sendfile (Linux2.2+)，accept-filter (FreeBSD4.1+)，TCP_DEFER_ACCEPT (Linux 2.4+)的支持，从而大大提高了性能。","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]},{"title":"并发编程","date":"2018-04-05T04:10:22.000Z","path":"2018/04/05/并发编程/","text":"并发编程JVM内存模型（JMM）1.Java当中的线程通讯和消息传递2.什么是重排序和顺序一致性？Happens-Before?As-If-Serial? Synchronized的概念和分析1.同步，重量级锁以及Sychronized的原理分析2.自旋锁，偏向锁，轻量级锁，重量级锁的概念、使用以及如何优化 Volatile和DCL的知识1.Volatile的使用场景和Volatile实现机制、内存语义、内存模型2.DCL的单例模式，什么是DCL?如何来解决DCL的问题 并发基础之AQS的深度分析1.AnstractAueuedSynchronizer同步器的概念、CLH同步队列是什么？2.同步状态的获取和释放、线程阻塞和唤醒 Lock和并发常用工具类1.Java当中的Lock、ReenrantLock、ReentrantReadWriteLock、Condition2.Java当中的并发工具类CyclicBarrier、CountdownLatch、Semphore3.Java当中的并发集合类ConcurrentHashMap、ConcurrentLinkedQueue… 原子操作常用知识讲解1.基本类型的原子操作比如经典的AtomicBoolean、AtomicIngter、AtomicLong2.数组类型的原子操作代表的几个类AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray3.引用类型的原子操作的典型AtomicReference、AtomicReferenceFieldUpdater…4.CAS概念知识、COmpare And Swap以及缺陷 线程池和并发并行1.Excutor、ThreadPoolExcutor、Callable &amp; Future、ScheduledExcutorService2.ThreadLocal、Fork &amp; Join？什么是并行？线程池如何保证核心线程不被销毁？","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"引用类型","date":"2018-04-05T04:10:22.000Z","path":"2018/04/05/引用类型/","text":"引用类型对象引用类型分为强引用、软引用、弱引用和虚引用。 强引用: 就是我们一般声明对象是时虚拟机生成的引用，强引用环境下，垃圾回收时需要严格判断当前对象是否被强引用，如果被强引用，则不会被垃圾回收 软引用: 软引用一般被做为缓存来使用。与强引用的区别是，软引用在垃圾回收时，虚拟机会根据当前系统的剩余内存来决定是否对软引用进行回收。如果剩余内存比较紧张，则虚拟机会回收软引用所引用的空间；如果剩余内存相对富裕，则不会进行回收。换句话说，虚拟机在发生OutOfMemory时，肯定是没有软引用存在的。 弱引用: 弱引用与软引用类似，都是作为缓存来使用。但与软引用不同，弱引用在进行垃圾回收时，是一定会被回收掉的，因此其生命周期只存在于一个垃圾回收周期内。 强引用不用说，我们系统一般在使用时都是用的强引用。而“软引用”和“弱引用”比较少见。他们一般被作为缓存使用，而且一般是在内存大小比较受限的情况下做为缓存。因为如果内存足够大的话，可以直接使用强引用作为缓存即可，同时可控性更高。因而，他们常见的是被使用在桌面应用系统的缓存。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"对象实例与对象引用有何不同","date":"2018-04-04T01:30:00.000Z","path":"2018/04/04/对象实例与对象引用有何不同/","text":"对象实例与对象引用有何不同new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）。 1）分配地址2）创建对象实例（堆内存）3）对象引用指向对象实例 （栈内存） 栈代表了处理逻辑，而堆代表了数据。 栈是运行时的单位，而堆是存储式的单位。 栈解决程序运行的问题，解决程序如何运行的问题，如何处理数据。 堆解决数据存储问题，数据存哪，怎么存。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"定时任务","date":"2018-04-03T02:10:22.000Z","path":"2018/04/03/定时任务 /","text":"定时任务Java实现定时任务的三种方法: 普通thread实现 TimerTask实现 ScheduledExecutorService实现 普通thread这是最常见的，创建一个thread，然后让它在while循环里一直运行着，通过sleep方法来达到定时任务的效果。这样可以快速简单的实现，代码如下：12345678910111213141516171819202122public class ScheduleTask &#123; public static void main(String[] args) &#123; // run in a second 定时任务 final long timeInterval = 1000; Runnable runnable = new Runnable() &#123; public void run() &#123; while (true) &#123; // ------- code for task to run System.out.println(\"Hello !!\"); // ------- ends here try &#123; Thread.sleep(timeInterval); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; Thread thread = new Thread(runnable); thread.start(); &#125;&#125; Timer和TimerTask上面的实现是非常快速简便的，但它也缺少一些功能。用Timer和TimerTask的话与上述方法相比有如下好处： 当启动和去取消任务时可以控制 第一次执行任务时可以指定你想要的delay时间 在实现时，Timer类可以调度任务，TimerTask则是通过在run()方法里实现具体任务。 Timer实例可以调度多任务，它是线程安全的。 当Timer的构造器被调用时，它创建了一个线程，这个线程可以用来调度任务：12345678910111213141516171819202122232425import java.util.Timer;import java.util.TimerTask;public class TimerTaskTest &#123; public static void main(String[] args) &#123; TimerTask task = new TimerTask() &#123; @Override public void run() &#123; // TODO Auto-generated method stub System.out.println(\"Hello World!\"); &#125; &#125;; Timer timer = new Timer(); long delay = 0; long inteval = 1 * 1000; timer.scheduleAtFixedRate(task, delay, inteval); &#125;&#125; ScheduledExecutorService是从Java SE 5的java.util.concurrent里，做为并发工具类被引进的，这是最理想的定时任务实现方式。相比于上两个方法，它有以下好处： 相比于Timer的单线程，它是通过线程池的方式来执行任务的 可以很灵活的去设定第一次执行任务delay时间 提供了良好的约定，以便设定执行的时间间隔 123456789101112131415161718import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ScheduledExecutorServiceTest &#123; public static void main(String[] args) &#123; Runnable runnable = new Runnable() &#123; public void run() &#123; // task to run goes here System.out.println(\"Hello !!\"); &#125; &#125;; ScheduledExecutorService service = Executors.newSingleThreadScheduledExecutor(); service.scheduleAtFixedRate(runnable, 0, 1, TimeUnit.SECONDS); &#125;&#125;","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"接口和抽象类的区别","date":"2018-04-01T15:00:00.000Z","path":"2018/04/01/接口和抽象类的区别/","text":"接口和抽象类的区别接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Thread sleep() 和 wait()","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Thread sleep() 和  wait()/","text":"Thread sleep() 和 wait()sleep()方法是Thread类里面的，主要的意义就是让当前线程停止执行，让出CPU给其他的线程，但是不会释放对象锁资源以及监控的状态，当指定的时间到了之后又会自动恢复运行状态。 wait()方法是Object类里面的，主要的意义就是让线程放弃当前的对象的锁，进入等待此对象的等待锁定池，只有针对此对象调动notify方法后本线程才能够进入对象锁定池准备获取对象锁进入运行状态。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ThreadSleepWait &#123; /** java中的sleep()和wait()的区别 */ public static void main(String[] args) &#123; new Thread(new Thread1()).start(); try &#123; System.out.println(\"主线程Sleep\"); Thread.sleep(5000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; new Thread(new Thread2()).start(); &#125; private static class Thread1 implements Runnable&#123; @Override public void run()&#123; synchronized (ThreadSleepWait.class) &#123; System.out.println(\"启动线程\" + Thread.currentThread().getName()); System.out.println(\"线程等待中...\"); try &#123; //调用wait()方法，线程会放弃对象锁，进入等待此对象的等待锁定池 ThreadSleepWait.class.wait(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程1继续\"); System.out.println(\"线程1结束\"); &#125; &#125; &#125; private static class Thread2 implements Runnable&#123; @Override public void run()&#123; synchronized (ThreadSleepWait.class) &#123; System.out.println(\"启动线程\" + Thread.currentThread().getName()); System.out.println(\"线程等待中...\"); //只有针对此锁对象调用notify后 //本线程才进入对象锁定池准备获取对象锁进入运行状态。 ThreadSleepWait.class.notify(); //区别 //如果我们把代码：ThreadSleepWait.class.notify给注释掉， //ThreadSleepWait.class调用了wait()方法但是没有调用notify()方法, //则线程1永远处于挂起状态。 try &#123; //Sleep方法导致了程序暂停执行指定的时间,让出cpu该其他线程， //但是他的监控状态依然保持者,当指定的时间过又会自动恢复运行状态。 //在调用sleep方法的过程中，线程不会释放对象锁。 Thread.sleep(5000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程2继续\"); System.out.println(\"线程2结束\"); &#125; &#125; &#125;&#125; 运行效果：123456789主线程Sleep启动线程Thread-0线程等待中...启动线程Thread-1线程等待中...线程2继续线程2结束线程1继续线程1结束 如果注释掉代码，之后调用1ThreadSleepWait.class.notify(); 运行效果：1234567主线程Sleep启动线程Thread-0线程等待中...启动线程Thread-1线程等待中...线程2继续线程2结束","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"死锁问题","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/死锁问题/","text":"死锁问题123456789101112131415161718192021222324252627282930313233343536public class DeadLockDemo &#123; private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) &#123; new Thread(() -&gt; &#123; synchronized (resource1) &#123; System.out.println(Thread.currentThread() + \"get resource1\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) &#123; System.out.println(Thread.currentThread() + \"get resource2\"); &#125; &#125; &#125;, \"线程 1\").start(); new Thread(() -&gt; &#123; synchronized (resource2) &#123; System.out.println(Thread.currentThread() + \"get resource2\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) &#123; System.out.println(Thread.currentThread() + \"get resource1\"); &#125; &#125; &#125;, \"线程 2\").start(); &#125;&#125; 产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系 如何避免线程死锁?我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 使用多线程的时候，一种非常简单的避免死锁的方式就是：指定获取锁的顺序，并强制线程按照指定的顺序获取锁。因此，如果所有的线程都是以同样的顺序加锁和释放锁，就不会出现死锁了。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Java 线程属性","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Thread线程属性/","text":"#线程属性 以下是关系到线程属性的几个方法： 1）getId 用来得到线程ID 2）getName和setName 用来得到或者设置线程名称。 3）getPriority和setPriority 用来获取和设置线程优先级。 4）setDaemon和isDaemon 用来设置线程是否成为守护线程和判断线程是否是守护线程。 守护线程和用户线程的区别在于：守护线程依赖于创建它的线程，而用户线程则不依赖。举个简单的例子：如果在main线程中创建了一个守护线程，当main方法运行完毕之后，守护线程也会随着消亡。而用户线程则不会，用户线程会一直运行直到其运行完毕。在JVM中，像垃圾收集器线程就是守护线程。 Thread类有一个比较常用的静态方法currentThread()用来获取当前线程。 在上面已经说到了Thread类中的大部分方法，那么Thread类中的方法调用到底会引起线程状态发生怎样的变化呢？","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Thread实现多线程二","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Thread实现多线程二/","text":"Thread实现多线程二接上文，关系到线程运行状态的几个方法： 4）yield方法 调用yield方法会让当前线程交出CPU权限，让CPU去执行其他的线程。它跟sleep方法类似，同样不会释放锁。但是yield不能控制具体的交出CPU的时间，另外，yield方法只能让拥有相同优先级的线程有获取CPU执行时间的机会。 注意，调用yield方法并不会让线程进入阻塞状态，而是让线程重回就绪状态，它只需要等待重新获取CPU执行时间，这一点是和sleep方法不一样的。 5）join方法 join方法有三个重载版本：123join()join(long millis) //参数为毫秒join(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 假如在main线程中，调用thread.join方法，则main方法会等待thread线程执行完毕或者等待一定的时间。如果调用的是无参join方法，则等待thread执行完毕，如果调用的是指定了时间参数的join方法，则等待一定的事件。123456789101112131415161718192021222324252627282930313233import java.io.IOException;public class ThreadJoin &#123; public static void main(String[] args) throws IOException &#123; System.out.println(\"进入线程\" + Thread.currentThread().getName()); ThreadJoin test = new ThreadJoin(); MyThread thread1 = test.new MyThread(); //线程开始 thread1.start(); try &#123; System.out.println(\"线程 \" + Thread.currentThread().getName() + \"等待\"); thread1.join();//使主线程进入等待，其他线程执行完毕 System.out.println(\"线程\" + Thread.currentThread().getName() + \"继续执行\"); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; class MyThread extends Thread&#123; @Override public void run() &#123; System.out.println(\"进入线程\" + Thread.currentThread().getName()); try &#123; Thread.currentThread().sleep(5000); &#125; catch (InterruptedException e) &#123; // TODO: handle exception &#125; System.out.println(\"线程\" + Thread.currentThread().getName()+\"执行完毕\"); &#125; &#125;&#125; 执行结果 ：12345进入线程main线程 main等待进入线程Thread-0线程Thread-0执行完毕线程main继续执行 可以看出，当调用thread1.join()方法后，main线程会进入等待，然后等待thread1执行完之后再继续执行。 实际上调用join方法是调用了Object的wait方法 。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Thread实现多线程一","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Thread实现多线程一/","text":"Thread实现多线程一Thread类实现了Runnable接口。 以下是关系到线程运行状态的几个方法： 1）start方法 start()用来启动一个线程，当调用start方法后，系统才会开启一个新的线程来执行用户定义的子任务，在这个过程中，会为相应的线程分配需要的资源。 2）run方法 run()方法是不需要用户来调用的，当通过start方法启动一个线程之后，当线程获得了CPU执行时间，便进入run方法体去执行具体的任务。注意，继承Thread类必须重写run方法，在run方法中定义具体要执行的任务。 3）sleep方法 sleep方法有两个重载版本：12sleep(long millis) //参数为毫秒sleep(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 sleep相当于让线程睡眠，交出CPU，让CPU去执行其他的任务。 但是有一点要非常注意，sleep方法不会释放锁，也就是说如果当前线程持有对某个对象的锁，则即使调用sleep方法，其他线程也无法访问这个对象。 1234567891011121314151617181920212223242526272829303132333435363738import java.util.Date;public class MutipleThread &#123; private int i = 10; private Object object = new Object(); //锁 public class ProcessThread extends Thread&#123; @Override public void run() &#123; System.out.println(\"开始线程\" + new Date()); //会上锁 synchronized (object) &#123; i++; System.out.println(\"i:\" + i); try &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"进入睡眠状态\"); Thread.currentThread().sleep(10000); &#125; catch (InterruptedException e) &#123; // TODO: handle exception &#125; System.out.println(\"线程\"+Thread.currentThread().getName()+\"睡眠结束\"); i++; System.out.println(\"i:\" + i); &#125; &#125; &#125; public static void main(String[] args) &#123; // TODO Auto-generated method stub MutipleThread mThread = new MutipleThread(); ProcessThread pThread = mThread.new ProcessThread(); ProcessThread pThread2 = mThread.new ProcessThread(); pThread.start(); pThread2.start(); &#125;&#125; 执行结果 ：12345678910开始线程Thu Jun 27 09:43:40 CST 2019i:11线程Thread-0进入睡眠状态开始线程Thu Jun 27 09:43:40 CST 2019线程Thread-0睡眠结束i:12i:13线程Thread-1进入睡眠状态线程Thread-1睡眠结束i:14 从上面输出结果可以看出，当Thread-0进入睡眠状态之后，Thread-1并没有去执行具体的任务。只有当Thread-0执行完之后，此时Thread-0释放了对象锁，Thread-1才开始执行。 注意，如果调用了sleep方法，必须捕获InterruptedException异常或者将该异常向上层抛出。当线程睡眠时间满后，不一定会立即得到执行，因为此时可能CPU正在执行其他的任务。所以说调用sleep方法相当于让线程进入阻塞状态。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Java 多线程上下文切换","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/多线程上下文切换/","text":"多线程上下文切换对于单核CPU来说（对于多核CPU，此处就理解为一个核），CPU在一个时刻只能运行一个线程，当在运行一个线程的过程中转去运行另外一个线程，这个叫做线程上下文切换（对于进程也是类似）。 由于可能当前线程的任务并没有执行完毕，所以在切换时需要保存线程的运行状态，以便下次重新切换回来时能够继续切换之前的状态运行。举个简单的例子：比如一个线程A正在读取一个文件的内容，正读到文件的一半，此时需要暂停线程A，转去执行线程B，当再次切换回来执行线程A的时候，我们不希望线程A又从文件的开头来读取。 因此需要记录线程A的运行状态，那么会记录哪些数据呢？因为下次恢复时需要知道在这之前当前线程已经执行到哪条指令了，所以需要记录程序计数器的值，另外比如说线程正在进行某个计算的时候被挂起了，那么下次继续执行的时候需要知道之前挂起时变量的值时多少，因此需要记录CPU寄存器的状态。所以一般来说，线程上下文切换过程中会记录程序计数器、CPU寄存器状态等数据。 说简单点的：对于线程的上下文切换实际上就是 存储和恢复CPU状态的过程，它使得线程执行能够从中断点恢复执行。 虽然多线程可以使得任务执行的效率得到提升，但是由于在线程切换时同样会带来一定的开销代价，并且多个线程会导致系统资源占用的增加，所以在进行多线程编程时要注意这些因素。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Thread start() 和 run()","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Thread start() 和  run().md/","text":"Thread start() 和 run() start 和 run 方法解释： 1） start：用start方法来启动线程，真正实现了多线程运行，这时无需等待run方法体代码执行完毕而直接继续执行下面的代码。通过调用Thread类的start()方法来启动一个线程，这时此线程处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行run()方法，这里方法 run()称为线程体，它包含了要执行的这个线程的内容，Run方法运行结束，此线程随即终止。 2） run：run()方法只是类的一个普通方法而已，如果直接调用Run方法，程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到写线程的目的。总结：调用start方法方可启动线程，而run方法只是thread的一个普通方法调用，还是在主线程里执行。这两个方法应该都比较熟悉，把需要并行处理的代码放在run()方法中，start()方法启动线程将自动调用 run()方法，这是由jvm的内存机制规定的。并且run()方法必须是public访问权限，返回值类型为void。 Java 的线程是通过java.lang.Thread类来实现的。VM启动时会有一个由主方法所定义的线程。可以通过创建Thread的实例来创建新的线程。每个线程都是通过某个特定Thread对象所对应的方法run（）来完成其操作的，方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。 在Java 当中，线程通常都有五种状态，创建、就绪、运行、阻塞和死亡。第一是创建状态。在生成线程对象，并没有调用该对象的start方法，这是线程处于创建状态。第二是就绪状态。当调用了线程对象的start方法之后，该线程就进入了就绪状态，但是此时线程调度程序还没有把该线程设置为当前线程，此时处于就绪状态。在线程运行之后，从等待或者睡眠中回来之后，也会处于就绪状态。第三是运行状态。线程调度程序将处于就绪状态的线程设置为当前线程，此时线程就进入了运行状态，开始运行run函数当中的代码。第四是阻塞状态。线程正在运行的时候，被暂停，通常是为了等待某个时间的发生(比如说某项资源就绪)之后再继续运行。sleep,suspend，wait等方法都可以导致线程阻塞。第五是死亡状态。如果一个线程的run方法执行结束或者调用stop方法后，该线程就会死亡。对于已经死亡的线程，无法再使用start方法令其进入就绪。 调用start（）后，线程会被放到等待队列，等待CPU调度，并不一定要马上开始执行，只是将这个线程置于可动行状态。然后通过JVM，线程Thread会调用run（）方法，执行本线程的线程体。先调用start后调用run，这么麻烦，为了不直接调用run？就是为了实现多线程的优点，没这个start不行。 1.start（）方法来启动线程，真正实现了多线程运行。这时无需等待run方法体代码执行完毕，可以直接继续执行下面的代码；通过调用Thread类的start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。 然后通过此Thread类调用方法run()来完成其运行操作的， 这里方法run()称为线程体，它包含了要执行的这个线程的内容， Run方法运行结束， 此线程终止。然后CPU再调度其它线程。2.run（）方法当作普通方法的方式调用。程序还是要顺序执行，要等待run方法体执行完毕后，才可继续执行下面的代码；程序中只有主线程——这一个线程，其程序执行路径还是只有一条， 这样就没有达到写线程的目的。记住：多线程就是分时利用CPU，宏观上让所有线程一起执行 ，也叫并发。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"多线程","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/多线程/","text":"多线程一般来说，线程包括以下这几个状态：创建(new)、就绪(runnable)、运行(running)、阻塞(blocked)、time waiting、waiting、消亡（dead）。 当需要新起一个线程来执行某个子任务时，就创建了一个线程。但是线程创建之后，不会立即进入就绪状态，因为线程的运行需要一些条件，只有线程运行需要的所有条件满足了，才进入就绪状态。 当线程进入就绪状态后，不代表立刻就能获取CPU执行时间，也许此时CPU正在执行其他的事情，因此它要等待。当得到CPU执行时间之后，线程便真正进入运行状态。 线程在运行状态过程中，可能有多个原因导致当前线程不继续运行下去，比如用户主动让线程睡眠（睡眠一定的时间之后再重新执行）、用户主动让线程等待，或者被同步块给阻塞，此时就对应着多个状态：time waiting（睡眠或等待一定的事件）、waiting（等待被唤醒）、blocked（阻塞）。 当由于突然中断或者子任务执行完毕，线程就会被消亡。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Runnable接口程实现多线程","date":"2018-04-01T02:10:00.000Z","path":"2018/04/01/Runnable实现多线程/","text":"Runnable接口程实现多线程写一个类实现Runnable接口，实现run方法。用new Thread(Runnableclass).start()方法来启动 12345678910111213141516171819202122232425262728293031323334public class RunnableTask &#123; public static void main(String[] args) &#123; RunnableTask rt = new RunnableTask(); Runner1 runner1 = rt.new Runner1(); Runner2 runner2 = rt.new Runner2(); Thread thread1 = new Thread(runner1); Thread thread2 = new Thread(runner2); thread1.start(); thread2.start(); //thread1.run(); //thread2.run(); &#125; class Runner1 implements Runnable &#123; // 实现了Runnable接口，jdk就知道这个类是一个线程 public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.println(\"进入Runner1运行状态——————————\" + i); &#125; &#125; &#125; class Runner2 implements Runnable &#123; // 实现了Runnable接口，jdk就知道这个类是一个线程 public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.println(\"进入Runner2运行状态==========\" + i); &#125; &#125; &#125;&#125;","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"Linux文件内容查看","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux 文件内容查看/","text":"Linux文件内容查看Linux文件内容查看Linux 文件内容查看Linux系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒著写！ nl 显示的时候，顺道输出行号！ more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页！ head 只看头几行 tail 只看尾巴几行 cat (由第一行开始显示文件内容)语法：1cat [-AbEnTv] 参数： -A ：相当於 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号 -E ：将结尾的断行字节 $ 显示出来； -n ：列印出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符检看 /etc/issue 这个文件的内容：123[root@www ~]# cat /etc/issueCentOS release 6.4 (Final)Kernel \\r on an \\m","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux 文件属性","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux文件属性/","text":"Linux 文件属性在Linux中我们可以使用命令来显示一个文件的属性以及文件所属的用户和组，如：12345[root@www /]# ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot…… note: “ll” 也可以。 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等。”d”在Linux中代表该文件是一个目录文件。 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 接下来的字符中，以三个为一组，且均为『rwx』 的三个参数的组合。 [ r ]代表可读(read) [ w ]代表可写(write) [ x ]代表可执行(execute) 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号[ - ]而已。 对于文件来说，它都有一个特定的所有者，也就是对该文件具有所有权的用户。同时，在Linux系统中，用户是按组分类的，一个用户属于一个或多个组。文件所有者以外的用户又可以分为文件所有者的同组用户和其他用户。因此，Linux系统按文件所有者、文件所有者同组用户和其他用户来规定了不同的文件访问权限。 对于 root 用户来说，一般情况下，文件的权限对其不起作用。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux常用命令","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux 常用命令/","text":"Linux常用命令 #Linux 常用命令 cd ==cd /== 进入主目录 ==cd ~== 进入Home目录 ==cd -== 进入上一次工作中路径 ls ==ls -a== 列出所有 ==ls -r== 反序排列 ==ls -t== 以文件修改时间排列 ==ls -l== 将文件名,文件所有者，文件大小信息详细信息列出来 pwd ==pwd== 展示当前工作目录路径 mkdir创建文件夹可用选项： ==mkdir -m==: 对新建目录设置存取权限,也可以用chmod命令设置; ==mkdir -p==: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立那 些尚不在的目录,即一次可以建立多个目录; rm删除文件，删除一个目录中的一个或多个文件或目录，如果没有使用- r选项，则rm不会删除目录 rmdir从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。 mv移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 cp将源文件复制至目标文件，或将多个源文件复制至目标目录。 free显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存 -b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s &lt;间隔秒数&gt; 持续显示内存 -t 显示内存使用总合 cat cat主要有三大功能： 一次显示整个文件:cat filename 从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 将几个文件合并为一个文件:cat file1 file2 &gt; file -b对非空输出行号 -n输出所有行号","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux用户和用户组管理","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux用户和用户组管理/","text":"Linux用户和用户组管理与用户和用户组相关的信息都存放在一些系统文件中，这些文件包括/etc/passwd, /etc/shadow, /etc/group等。完成用户管理的工作有许多种方法，但是每一种方法实际上都是对有关的系统文件进行修改。 用户管理每个用户账号都拥有一个惟一的用户名和各自的口令。 用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改。 用户口令的管理。 用户组的管理。 用户账号的管理添加账号： useradd1useradd 选项 用户名 参数说明： 选项: -c comment 指定一段注释性描述。 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。 -g 用户组 指定用户所属的用户组。 -G 用户组，用户组 指定用户所属的附加组。 -s Shell文件 指定用户的登录Shell。 -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。 用户名:指定新账号的登录名。 1# useradd –d /usr/sam -m sam 此命令创建了一个用户sam，其中-d和-m选项用来为登录名sam产生一个主目录/usr/sam（/usr为默认的用户主目录所在的父目录）。 1# useradd -s /bin/sh -g group –G adm,root gem 此命令新建了一个用户gem，该用户的登录Shell是 /bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。 这里可能新建组：#groupadd group及groupadd adm 增加用户账号就是在/etc/passwd文件中为新用户增加一条记录，同时更新其他系统文件如/etc/shadow, /etc/group等。 删除账号 userdel如果一个用户的账号不再使用，可以从系统中删除。删除用户账号就是要将/etc/passwd等系统文件中的该用户记录删除，必要时还删除用户的主目录。删除一个已有的用户账号使用userdel命令，其格式如下：1userdel 选项 用户名 常用的选项是 -r，它的作用是把用户的主目录一起删除。1# userdel -r sam 修改账号 usermod修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 修改已有用户的信息使用usermod命令，其格式如下：1usermod 选项 用户名 常用的选项包括-c, -d, -m, -g, -G, -s, -u以及-o等，这些选项的意义与useradd命令中的选项一样，可以为用户指定新的资源值。 1# usermod -s /bin/ksh -d /home/z –g developer sam 此命令将用户sam的登录Shell修改为ksh，主目录改为/home/z，用户组改为developer。 密码管理（口令）用户管理的一项重要内容是用户口令的管理。用户账号刚创建时没有口令，但是被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。 指定和修改用户口令的Shell命令是passwd。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。命令的格式为：1passwd 选项 用户名 可使用的选项： -l 锁定口令，即禁用账号。 -u 口令解锁。 -d 使账号无口令。 -f 强迫用户下次登录时修改口令。如果默认用户名，则修改当前用户的口令。 例如，假设当前用户是sam，则下面的命令修改该用户自己的口令：1234$ passwd Old password:****** New password:******* Re-enter new password:******* 如果是超级用户，可以用下列形式指定任何用户的口令：123# passwd sam New password:******* Re-enter new password:******* 用户组管理添加组 groupadd1groupadd 选项 用户组 可以使用的选项有：(选项不常用) -g GID 指定新用户组的组标识号（GID）。-o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。1# groupadd group1 此命令向系统中增加了一个新组group1，新组的组标识号是在当前已有的最大组标识号的基础上加1。 1# groupadd -g 101 group2 此命令向系统中增加了一个新组group2，同时指定新组的组标识号是101。 删除组 groupdel123groupdel 用户组# groupdel group1 修改组 groupmod1groupmod 选项 用户组 常用的选项有： -g GID 为用户组指定新的组标识号。 -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。 -n 新用户组 将用户组的名字改为新名字 切换如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限 1$ newgrp root 这条命令将当前用户切换到root用户组，前提条件是root用户组确实是该用户的主组或附加组。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"log4j","date":"2018-03-03T06:22:30.000Z","path":"2018/03/03/Log配置文件/","text":"123456789101112131415161718192021222324### 设置###log4j.rootLogger = debug,stdout,D,E### 输出信息到控制台 ###log4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n### 输出DEBUG 级别以上的日志文件设置 ###log4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = logs/debug.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = DEBUG log4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [ %t:%r ] - [ %p ] %m%n### 输出ERROR 级别以上的日志文件设置 ###log4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File = logs/error.loglog4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [ %t:%r ] - [ %p ] %m%n 测试 ：12345678910111213141516package com.test.util;import org.apache.log4j.*;public class LogUtil &#123; private static Logger logger = Logger.getLogger(LogUtil.class); public static void main(String[] args) throws Exception &#123; // 记录debug级别的信息 logger.debug(&quot;This is debug message.&quot;); // 记录info级别的信息 logger.info(&quot;This is info message.&quot;); // 记录error级别的信息 logger.error(&quot;This is error message.&quot;); &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Maven 添加阿里云镜像","date":"2018-03-03T06:22:30.000Z","path":"2018/03/03/Maven 添加阿里云镜像/","text":"Maven 添加阿里云镜像setting.xml 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wxzhongwang.github.io/tags/Maven/"}]},{"title":"Linux Java 安装环境变量","date":"2018-03-03T06:22:30.000Z","path":"2018/03/03/Linux Java 安装环境变量 /","text":"Linux Java 安装环境变量如下安装目录为 /usr/java12345tar -zxf jdk-8u211-linux-x64.tag.gzvim etc/profileexport JAVA_HOME=/usr/java/jdk-8u221export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 修改.bash_profile文件 这种方法更为安全，它可以把使用这些环境变量的权限控制到用户级别，如果你需要给某个用户权限使用这些环境变量，你只需要修改其个人用户主目录下的.bash_profile文件就可以了。·用文本编辑器打开用户目录下的.bash_profile文件·在.bash_profile文件末尾加入：123export JAVA_HOME=/usr/java/jdk-8u221export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"String StringBuffer 和 StringBuilder","date":"2018-03-01T02:10:00.000Z","path":"2018/03/01/String StringBuffer 和 StringBuilder/","text":"String、StringBuffer 和 StringBuilder可变性String: 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。 StringBuilder 与 StringBuffer: 而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 线程安全性String 中的对象是不可变的，也就可以理解为常量，线程安全。 AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。 StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。 StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer 装箱与拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型；","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"JVM 3.0","date":"2018-02-24T02:10:00.000Z","path":"2018/02/24/3.0 Java虚拟机的内部体系结构/","text":"Java虚拟机的内部体系结构在 Java虚拟机规范中，一个虚拟机实例的行为是分别按照子系统、内存区、数据类型和指令来描述的，这些组成部分一起展示了抽象的虚拟机内部体系结构。 运行时数据区Java虚拟机在执行Java程序的时候会把它管理的内存划分为若干个不同的数据区域，这些区域有各自的用途以及创建和销毁的时机，有的区域随着虚拟机进程的启动而存在（线程共享），有的区域则随着用户线程的启动和结束而建立和销毁（线程私有）。Java虚拟机所管理的内存包括以下几个运行时数据区域。 程序计数器对于一个运行中的Java程序而言，每一个线程都有它的程序计数器，也叫PC寄存器，可以看做当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支，循环，跳转，异常处理，线程恢复等都需要依赖这个计数器。 程序计数器既能持有一个本地指针，也能持有一个returnAddress。当线程执行某个Java方法时，程序计数器的值总是下一条被执行指令的地址。这里的地址可以是一个本地指针，也可以是方法字节码中相对该方法起始指令的偏移量。如果该线程正在执行一个本地方法，那么此时程序计数器的值是“undefined”。 程序计数器属于线程私有的内存，也就是说，每当创建一个线程，都将得到该线程自己的一个程序计数器。Java虚拟机的多线程是通过线程的轮换并且分配处理器的执行时间来实现的，在一个确定的时刻，一个处理器都只会执行一条线程中的指令，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器。 Java虚拟机栈（Java栈）Java虚拟机栈也是线程私有的，它的生命周期和线程相同。Java虚拟机栈描述的是Java方法执行的内存模型，每个方法在执行时会创建一个栈帧，用于存放局部变量表，操作数栈，动态链接，方法出口等信息。一个Java方法从调用到执行结束的过程，相当于一个栈帧在Java虚拟机栈中从入栈到出栈的过程。（1）局部变量表局部变量表用于存放编译时期可知的各种基本数据类型，对象的引用（不等同于对象，可能是指向对象的起始地址的指针，也可能是指向一个代表对象的句柄）以及returnAddress类型（指向了一条字节码地址）。局部变量在方法执行时被创建，在方法执行结束时销毁。 字节码指令通过从0开始的索引使用其中的数据。类型为int, float, reference和returnAddress的值在数组中占据一项，而类型为byte, short和char的值在存入数组前都被转换为int值，也占据一项。但类型为long和double的值在数组中却占据连续的两项。 操作数栈和局部变量区一样，操作数栈也是被组织成一个以字长为单位的数组。它通过标准的栈操作访问–压栈和出栈。由于程序计数器无法被程序指令直接访问，Java虚拟机的指令是从操作数栈中取得操作数，所以它的运行方式是基于栈而不是基于寄存器。虚拟机把操作数栈作为它的工作区，因为大多数指令都要从这里弹出数据，执行运算，然后把结果压回操作数栈。 帧数据区除了局部变量区和操作数栈，Java栈帧还需要帧数据区来支持常量池解析、正常方法返回以及异常派发机制。每当虚拟机要执行某个需要用到常量池数据的指令时，它会通过帧数据区中指向常量池的指针来访问它。除了常量池的解析外，帧数据区还要帮助虚拟机处理Java方法的正常结束或异常中止。如果通过return正常结束，虚拟机必须恢复发起调用的方法的栈帧，包括设置程序计数器指向发起调用方法的下一个指令；如果方法有返回值，虚拟机需要将它压入到发起调用的方法的操作数栈。为了处理Java方法执行期间的异常退出情况，帧数据区还保存一个对此方法异常表的引用。 本地方法栈任何本地方法接口都会使用某种本地方法栈，本地方法栈与Java虚拟机栈发挥的作用类似。当线程调用Java方法时，虚拟机会创建一个新的栈帧并压入Java栈。当它调用的是本地方法时，虚拟机会保持Java栈不变，不再在线程的Java栈中压入新的栈，虚拟机只是简单地动态连接并直接调用指定的本地方法。 堆Java程序在运行时创建的所有类实例或数组（数组在Java虚拟机中是一个真正的对象）都放在同一个堆中，堆是虚拟机管理的内存最大的一块，被所有线程所共享，在虚拟机启动的时候创建。堆内存的唯一目的就是存放对象实例，几乎所有的对象实例都在堆中分配内存。 方法区方法区同Java堆一样，是各个线程共享的内存区域，用于存储已经被虚拟机加载的类信息，常量，静态变量以及及时编译器编译后的代码等信息。当虚拟机装载某个类型时，它使用类装载器定位相应的class文件，然后读入这个class文件并将它传输到虚拟机中，接着虚拟机提取其中的类型信息，并将这些信息存储到方法区。方法区也可以被垃圾回收器收集，因为虚拟机允许通过用户定义的类装载器来动态扩展Java程序。 方法区中存放了以下信息： • 这个类型的全限定名（如全限定名java.lang.Object） • 这个类型的直接超类的全限定名 • 这个类型是类类型还是接口类型 • 这个类型的访问修饰符（public, abstract, final的某个子集） • 任何直接超接口的全限定名的有序列表 • 该类型的常量池，Class文件中除了有类的版本，字段，方法，接口等描述信息外，还有一项是常量池，用于存放编译时生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放 • 字段信息（字段名、类型、修饰符） • 方法信息（方法名、返回类型、参数数量和类型、修饰符） • 除了常量以外的所有类（静态）变量 • 指向ClassLoader类的引用（每个类型被装载时，虚拟机必须跟踪它是由启动类装载器还是由用户自定义类装载器装载的） • 指向Class类的引用（对于每一个被装载的类型，虚拟机相应地为它创建一个java.lang.Class类的实例存于堆中。比如你有一个到java.lang.Integer类的对象的引用，那么只需要调用Integer对象引用的getClass()方法，就可以得到表示java.lang.Integer类的Class对象） 类加载子系统虚拟机把Java描述类的信息加载到内存，并对数据进行校验，转换解析和初始化，形成可以被Java虚拟机直接使用的Java类型，这就是Java虚拟机的类加载机制。类从加载到虚拟机开始，到卸载出内存为止，它的整个生命周期包括： 加载，验证，准备，解析，初始化，使用和卸载7个阶段。 加载加载是类加载的一个阶段，在该阶段，Java虚拟机主要完成以下三件事情： • 根据此类的全限定名来确定该类的二进制字节流； • 将这个字节流所代表的静态数据存储结构转换为方法区的运行时数据结构； • 在堆内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 验证验证是连接的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息是否符合该虚拟机的要求。 准备准备阶段正式为类变量在方法区中分配内存并且赋初始值，初始值一般为该类变量类型的零值。 解析解析阶段虚拟机将常量池内的符号引用替换为直接引用。符号引用：以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义的定位到目标即可。直接引用：是指能直接指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。 初始化初始化是类加载过程的最后一个阶段，在前面的类加载过程中，除了在加载阶段用户可以自定义类加载器参与之外，其余动作完全由虚拟机主导和控制，到了初始化阶段，才真正开始执行类中定义的Java代码（字节码）。准备阶段为类变量分配内存并且赋初始值，赋值操作在初始化阶段执行。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"JVM 4.0","date":"2018-02-24T02:10:00.000Z","path":"2018/02/24/4.0 Class文件/","text":"Class文件Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在Class文件中，中间没有添加任何分隔符号。Class文件采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只包含两种数据类型，无符号数和表。无符号数属于基本的数据类型，以u1,u2,u4,u8分别代表1个字节，2个字节，4个字节和8个字节的无符号数，可以用来描述数字、索引引用、数量值或者按照utf-8编码构成字符串值。表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以“_info”结尾，用来描述有层次关系的复合结构数据。 Class文件的内容包括： 1234567891011121314151617181920ClassFile &#123; u4 magic; //魔数：0xCAFEBABE，用来判断是否是Java class文件 u2 minor_version; //次版本号 u2 major_version; //主版本号 u2 constant_pool_count; //常量池大小 cp_info constant_pool[constant_pool_count-1]; //常量池 u2 access_flags; //类和接口层次的访问标志（通过|运算得到） u2 this_class; //类索引（指向常量池中的类常量） u2 super_class; //父类索引（指向常量池中的类常量） u2 interfaces_count; //接口索引计数器 u2 interfaces[interfaces_count]; //接口索引集合 u2 fields_count; //字段数量计数器 field_info fields[fields_count]; //字段表集合 u2 methods_count; //方法数量计数器 method_info methods[methods_count]; //方法表集合 u2 attributes_count; //属性个数 attribute_info attributes[attributes_count]; //属性表&#125; 访问标志：类还是接口；是否定义为public类型，abstract类型；若为类，是否声明为final。 字段：用来描述接口或类中的变量，但不包括在方法内部的变量。 字面量：文本字符串，声明为final的常量值等。 符号引用：类和接口的全限定名；字段的名称和描述符；方法的名称和描述符。 类索引，父类索引，接口索引：用来确定类的继承关系。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Git 远程仓库","date":"2018-02-13T02:10:00.000Z","path":"2018/02/13/Git 远程仓库/","text":"Git 远程仓库为了能在任意 Git 项目上协作，你需要知道如何管理自己的远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库。你可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。 与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。管理远程仓库包括了解如何添加远程仓库、移除无效的远程仓库、管理不同的远程分支并定义它们是否被跟踪等等。 查看远程仓库如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。 如果你已经克隆了自己的仓库，那么至少应该能看到 origin - 这是 Git 给你克隆的仓库服务器的默认名字：12345678910$ git clone git@172.16.5.77:shengwangzhong/storelcoator.gitCloning into 'storelcoator'...remote: Reusing existing pack: 1857, done.remote: Total 1857 (delta 0), reused 0 (delta 0)Receiving objects: 100% (1857/1857), 374.35 KiB | 268.00 KiB/s, done.Resolving deltas: 100% (772/772), done.Checking connectivity... done.$ cd storelcoator$ git remoteorigin 你也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。123$ git remote -vorigin git@172.16.5.77:shengwangzhong/vue-myblog.git (fetch)origin git@172.16.5.77:shengwangzhong/vue-myblog.git (push) 如果你的远程仓库不止一个，该命令会将它们全部列出。 例如，与几个协作者合作的，拥有多个远程仓库的仓库看起来像下面这样：123456789101112$ cd grit$ git remote -vbakkdoor https://github.com/bakkdoor/grit (fetch)bakkdoor https://github.com/bakkdoor/grit (push)cho45 https://github.com/cho45/grit (fetch)cho45 https://github.com/cho45/grit (push)defunkt https://github.com/defunkt/grit (fetch)defunkt https://github.com/defunkt/grit (push)koke git://github.com/koke/grit.git (fetch)koke git://github.com/koke/grit.git (push)origin git@github.com:mojombo/grit.git (fetch)origin git@github.com:mojombo/grit.git (push) 添加远程仓库运行 git remote add 添加一个新的远程 Git 仓库，同时指定一个你可以轻松引用的简写：12345678$ git remoteorigin$ git remote add pb https://github.com/paulboone/ticgit$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)pb https://github.com/paulboone/ticgit (fetch)pb https://github.com/paulboone/ticgit (push) 现在你可以在命令行中使用字符串 pb 来代替整个 URL。 例如，如果你想拉取 Paul 的仓库中有但你没有的信息，可以运行 git fetch pb：12345678$ git fetch pbremote: Counting objects: 43, done.remote: Compressing objects: 100% (36/36), done.remote: Total 43 (delta 10), reused 31 (delta 5)Unpacking objects: 100% (43/43), done.From https://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在 Paul 的 master 分支可以在本地通过 pb/master 访问到 - 你可以将它合并到自己的某个分支中，或者如果你想要查看它的话，可以检出一个指向该点的本地分支。 从远程仓库中抓取与拉取就如刚才所见，从远程仓库中获得数据，可以执行：1$ git fetch [remote-name] 这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。 所以，git fetch origin 会抓取克隆（或上一次抓取）后新推送的所有工作。 必须注意 git fetch 命令会将数据拉取到你的本地仓库 - 它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。这对你来说可能是一个更简单或更舒服的工作流程；==默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master 分支（或不管是什么名字的默认分支）。== 运行 git pull 通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。 推送到远程仓库当你想分享你的项目时，必须将其推送到上游。 这个命令很简单：git push [remote-name] [branch-name]。 当你想要将 master 分支推送到 origin 服务器时（再次说明，克隆时通常会自动帮你设置好那两个名字），那么运行这个命令就可以将你所做的备份到服务器：1$ git push origin master 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。 查看远程仓库(详细)1234567891011$ git remote show origin* remote origin Fetch URL: git@172.16.5.77:shengwangzhong/vue-myblog.git Push URL: git@172.16.5.77:shengwangzhong/vue-myblog.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (up to date) 这个命令列出了当你在特定的分支上执行 git push 会自动地推送到哪一个远程分支。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 git pull 时哪些分支会自动合并。 远程仓库的移除与重命名如果想要重命名引用的名字可以运行 git remote rename 去修改一个远程仓库的简写名。 例如，想要将 pb 重命名为 paul，可以用 git remote rename 这样做：1234$ git remote rename pb paul$ git remoteoriginpaul","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"JVM 2.0","date":"2018-02-12T02:10:00.000Z","path":"2018/02/12/2.0 Java虚拟机/","text":"#Java虚拟机 作用： Java虚拟机的主要任务是装载class文件并且执行其中的字节码。Java虚拟机包含一个类装载器（class loader），它可以从程序和API中装载class文件，Java API中只有程序执行时需要的类才会被装载，字节码由执行引擎来执行。 当Java虚拟机由主机操作系统上的软件实现时，Java程序通过调用本地方法和主机进行交互。Java方法由Java语言编写，编译成字节码，存储在class文件中。本地方法由C/C++/汇编语言编写，编译成和处理器相关的机器代码，存储在动态链接库中，格式是各个平台专有。所以本地方法是联系Java程序和底层主机操作系统的连接方式。（跨平台）","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Git 撤销操作","date":"2018-02-12T02:10:00.000Z","path":"2018/02/12/Git 撤销操作/","text":"Git 撤销操作在任何一个阶段，你都有可能想要撤消某些操作。 注意，有些撤消操作是不可逆的。 这是在使用 Git 的过程中，会因为操作失误而导致之前的工作丢失的少有的几个地方之一。 撤销操作有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 –amend 选项的提交命令尝试重新提交：1$ git commit --amend 这个命令会将暂存区中的文件提交。如果自上次提交以来你还未做任何修改（例如，在上次提交后马上执行了此命令），那么快照会保持不变，而你所修改的只是提交信息。 例如，你提交后发现忘记了暂存某些需要的修改，可以像下面这样操作：123$ git commit -m 'initial commit'$ git add forgotten_file$ git commit --amend 最终你只会有一个提交 - 第二次提交将代替第一次提交的结果。 取消暂存的文件接下来的两个小节演示如何操作暂存区域与工作目录中已修改的文件。 这些命令在修改文件状态的同时，也会提示如何撤消操作。 例如，你已经修改了两个文件并且想要将它们作为两次独立的修改提交，但是却意外地输入了 git add * 暂存了它们两个。 如何只取消暂存两个中的一个呢？ git status 命令提示了你：12345678$ git add *$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README modified: CONTRIBUTING.md 在 “Changes to be committed” 文字正下方，提示使用 git reset HEAD … 来取消暂存。 所以，我们可以这样来取消暂存 CONTRIBUTING.md 文件：123456789101112131415$ git reset HEAD CONTRIBUTING.mdUnstaged changes after reset:M CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; READMEChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 这个命令有点儿奇怪，但是起作用了。 CONTRIBUTING.md 文件已经是修改未暂存的状态了。 Note：虽然在调用时加上 –hard 选项可以令 git reset 成为一个危险的命令（译注：可能导致工作目录中所有当前进度丢失！）不加选项地调用 git reset 并不危险 — 它只会修改暂存区域。 撤消对文件的修改如果你并不想保留对 CONTRIBUTING.md 文件的修改怎么办？ 你该如何方便地撤消修改 - 将它还原成上次提交时的样子（或者刚克隆完的样子，或者刚把它放入工作目录时的样子）？ 幸运的是，git status 也告诉了你应该如何做。 在最后一个例子中，未暂存区域是这样：12345Changes not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 它非常清楚地告诉了你如何撤消之前所做的修改。 让我们来按照提示执行：1234567$ git checkout -- CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README 可以看到那些修改已经被撤消了。 Important: 你需要知道 git checkout – [file] 是一个危险的命令，这很重要。 你对那个文件做的任何修改都会消失 - 你只是拷贝了另一个文件来覆盖它。 除非你确实清楚不想要那个文件了，否则不要使用这个命令。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git移动文件","date":"2018-02-09T02:10:00.000Z","path":"2018/02/09/Git 移动文件/","text":"移动文件Git 并不显式跟踪文件移动操作。如果在Git中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。 不过 Git 非常聪明，它会推断出究竟发生了什么。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。 要在 Git 中对文件改名，可以这么做：1$ git mv file_from file_to 它会恰如预期般正常工作。实际上，即便此时查看状态信息，也会明白无误地看到关于重命名操作的说明：1234567$ git mv README.md README$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README 其实，运行 git mv 就相当于运行了下面三条命令：123$ mv README.md README$ git rm README.md$ git add README 如此分开操作，Git 也会意识到这是一次改名，所以不管何种方式结果都一样。 两者唯一的区别是，mv 是一条命令而另一种方式需要三条命令，直接用 git mv 轻便得多。 不过有时候用其他工具批处理改名的话，要记得在提交前删除老的文件名，再添加新的文件名。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git移除文件","date":"2018-02-09T02:10:00.000Z","path":"2018/02/09/Git 移除文件/","text":"git rm要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。 可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 “Changes not staged for commit” 部分（也就是 未暂存清单）看到：1234567891011$ rm PROJECTS.md$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use \"git add/rm &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) deleted: PROJECTS.mdno changes added to commit (use \"git add\" and/or \"git commit -a\") 然后再运行 git rm 记录此次移除文件的操作：12345678$ git rm PROJECTS.mdrm 'PROJECTS.md'$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) deleted: PROJECTS.md 下一次提交时，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即force的首字母）。这是一种安全特性，用于防止误删还没有添加到快照的数据，这样的数据不能被 Git 恢复。 另外一种情况是，我们想把文件从Git仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 换句话说，你想让文件保留在磁盘，但是并不想让 Git 继续跟踪。 当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆.a这样的编译生成文件添加到暂存区时，这一做法尤其有用。 为达到这一目的，使用 –cached 选项：1$ git rm --cached README git rm 命令后面可以列出文件或者目录的名字，也可以使用 glob 模式。 比方说：1$ git rm log/\\*.log 注意到星号 * 之前的反斜杠 \\， 因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开。 此命令删除 log/ 目录下扩展名为 .log 的所有文件。 类似的比如：1$ git rm \\*~ 该命令为删除以 ~ 结尾的所有文件。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"清除eclipse launch configuration","date":"2018-02-08T06:18:02.000Z","path":"2018/02/08/清除eclipse launch configuration/","text":"清除eclipse launch configurationeclipse里面export runnable jar之前要先run一下才行, 但是之后launch configuration里面的记录一直存在, 非常难看, 清除的方法是:清空workspace文件夹下的里的内容。 注意是删掉文件夹里面的内容, 而不是把文件夹删了 1234.metadata/.plugins/org.eclipse.debug.core/.launcheseg:E:\\JavaWorkSpace\\.metadata\\.plugins\\org.eclipse.debug.core\\.launches","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Java多线程相关问题","date":"2018-02-08T04:18:45.000Z","path":"2018/02/08/Java多线程/","text":"Java多线程相关问题 1.线程池的原理，为什么要创建线程池？创建线程池的方式？ 2.线程的生命周期？什么时候会出现僵死线程？ 说说线程安全问题？什么是线程安全？如何实现线程安全？ 创建线程池有哪几个核心参数？如何合理的配置线程池的大小？ 5.volatile、ThreadLocal的使用场景和原理； 6.ThreadLocal什么时候会出现OOM情况？为什么？ 7.sychronized、volatile区别？synchronized锁粒度、模拟死锁场景、原子性和可见性","tags":[{"name":"多线程","slug":"多线程","permalink":"https://wxzhongwang.github.io/tags/多线程/"}]},{"title":"IO复用技术","date":"2018-02-08T04:18:45.000Z","path":"2018/02/08/IO复用技术/","text":"IO复用技术多进程方式实现的服务器端，一次创建多个工作子进程来给客户端提供服务。其实这种方式是存在问题的。 可以打个比方：如果我们先前创建的几个进程承载不了目前快速发展的业务的话，是不是还得增加进程数？我们都知道系统创建进程是需要消耗大量资源的，所以这样就会导致系统资源不足的情况。 那么有没有一种方式可以让一个进程同时为多个客户端端提供服务？ 接下来要讲的IO复用技术就是对于上述问题的最好解答。 对于IO复用，我们可以通过一个例子来很好的理解它。（例子来自于《TCP/IP网络编程》） 某教室有10名学生和1名老师，这些学生上课会不停的提问，所以一个老师处理不了这么多的问题。那么学校为每个学生都配一名老师， 也就是这个教室目前有10名老师。此后，只要有新的转校生，那么就会为这个学生专门分配一个老师。 如果把以上例子中的学生比作客户端，那么老师就是负责进行数据交换的服务端。则该例子可以比作是多进程的方式。 后来有一天，来了一位具有超能力的老师，这位老师回答问题非常迅速，并且可以应对所有的问题。而这位老师采用的方式是学生提问前必须先举手，确认举手学生后在回答问题。则现在的情况就是IO复用。 目前的常用的IO复用模型有三种：select，poll，epoll。 select模型说的通俗一点就是各个客户端(连接的文件描述符)套接字，都被放到了一个集合中，调用select函数之后会一直监视这些文件描述符中有哪些可读，如果有可读的描述符那么我们的工作进程就去读取资源。 poll模型poll 和 select 的实现非常类似，本质上的区别就是存放 fd 集合的数据结构不一样。select 在一个进程内可以维持最多 1024 个连接，poll 在此基础上做了加强，可以维持任意数量的连接。 但 select 和 poll 方式有一个很大的问题就是，我们不难看出来select是通过轮训的方式来查找是否可读或者可写，打个比方，如果同时有100万个连接都没有断开，而只有一个客户端发送了数据，所以这里它还是需要循环这么多次，造成资源浪费。所以后来出现了 epoll系统调用。 epoll模型epoll是 select 和 poll 的增强版，epoll 同 poll 一样，文件描述符数量无限制。 epoll是基于内核的反射机制，在有活跃的 socket 时，系统会调用我们提前设置的回调函数。而 poll 和 select 都是遍历。 但是也并不是所有情况下 epoll 都比 select/poll 好，比如在如下场景： 在大多数客户端都很活跃的情况下，系统会把所有的回调函数都唤醒，所以会导致负载较高。既然要处理这么多的连接，那倒不如 select 遍历简单有效。 内核（kernel）利用文件描述符（file descriptor）来访问文件。文件描述符是非负整数。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。","tags":[{"name":"IO","slug":"IO","permalink":"https://wxzhongwang.github.io/tags/IO/"}]},{"title":"HttpStatusCode","date":"2018-02-08T04:18:45.000Z","path":"2018/02/08/HttpStatusCode/","text":"HttpStatusCode12345678 /* 1xx：相关信息 2xx：操作成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误*/ 字段 状态码 说明 Continue 100 指示客户端可能继续其请求。 SwitchingProtocols 100 指示正在更改协议版本或协议。 OK 200 指示请求成功，且请求的信息包含在响应中。 Created 201 指示请求导致在响应被发送前创建新资源。 Accepted 202 指示请求已被接受做进一步处理。 NonAuthoritativeInformation 202 指示返回的元信息来自缓存副本而不是原始服务器，因此可能不正确。 NoContent 204 指示请求成功，指示已成功处理请求并且响应已被设定为无内容。 ResetContent 205 指示客户端应重置（或重新加载）当前资源。 PartialContent 206 指示响应是包括字节范围的 GET 请求所请求的部分响应。 MultipleChoices 300 指示请求的信息有多种表示形式，默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 Ambiguous 300 指示请求的信息有多种表示形式。默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 MovedPermanently 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。 Moved 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Found 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Redirect 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 SeeOther 303 作为 POST 的结果，SeeOther 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 RedirectMethod 303 作为 POST 的结果，RedirectMethod 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 NotModified 304 指示客户端的缓存副本是最新的。未传输此资源的内容。 UseProxy 305 指示请求应使用位于 Location 头中指定的 URI 的代理服务器。 Unused 306 是未完全指定的 HTTP/1.1 规范的建议扩展。 TemporaryRedirect 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 RedirectKeepVerb 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 BadRequest 400 指示服务器未能识别请求。如果没有其他适用的错误，或者如果不知道准确的错误或错误没有自己的错误代码，则发送 BadRequest。 Unauthorized 401 指示请求的资源要求身份验证。WWW-Authenticate头包含如何执行身份验证的详细信息。 PaymentRequired 402 保留 PaymentRequired 以供将来使用。 Forbidden 403 指示服务器拒绝满足请求。 NotFound 404 指示请求的资源不在服务器上。 MethodNotAllowed 405 指示请求的资源上不允许请求方法（POST 或 GET）。 NotAcceptable 406 指示客户端已用 Accept 头指示将不接受资源的任何可用表示形式。 ProxyAuthenticationRequired 407 指示请求的代理要求身份验证。Proxy-authenticate 头包含如何执行身份验证的详细信息。 RequestTimeout 408 指示客户端没有在服务器期望请求的时间内发送请求。 Conflict 409 指示由于服务器上的冲突而未能执行请求。 Gone 410 指示请求的资源不再可用。 LengthRequired 411 指示缺少必需的 Content-length 头。 PreconditionFailed 412 指示为此请求设置的条件失败，且无法执行此请求。条件是用条件请求标头（如 If-Match、If-None-Match 或 If-Unmodified-Since）设置的。 RequestEntityTooLarge 413 指示请求太大，服务器无法处理。 RequestUriTooLong 414 指示 URI 太长。 UnsupportedMediaType 415 指示请求是不支持的类型。 RequestedRangeNotSatisfiable 416 RequestedRangeNotSatisfiable指示无法返回从资源请求的数据范围，因为范围的开头在资源的开头之前，或因为范围的结尾在资源的结尾之后。 ExpectationFailed 417 指示服务器未能符合 Expect 头中给定的预期值。 UpgradeRequired 426 客户端应当切换到TLS/1.0 InternalServerError 500 指示服务器上发生了一般错误。 NotImplemented 501 指示服务器不支持请求的函数。 BadGateway 502 指示中间代理服务器从另一代理或原始服务器接收到错误响应。 ServiceUnavailable 503 指示服务器暂时不可用，通常是由于过多加载或维护。 GatewayTimeout 504 指示中间代理服务器在等待来自另一个代理或原始服务器的响应时已超时。 HttpVersionNotSupported 505 指示服务器不支持请求的 HTTP 版本。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"},{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"}]},{"title":"Git提交","date":"2018-02-07T02:10:00.000Z","path":"2018/02/07/Git Commit/","text":"Git Commit现在的暂存区域已经准备妥当可以提交了。 在此之前，请一定要确认还有什么修改过的或新建的文件还没有 git add 过，否则提交的时候不会记录这些还没暂存起来的变化。 这些修改过的文件只保留在本地磁盘。 所以，每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit：1$ git commit 你也可以在 commit 命令后添加 -m 选项，将提交信息与命令放在同一行。 请在每次提交时添加comment1234$ git commit -m \"Story 182: Fix benchmarks for speed\"[master 463dc4f] Story 182: Fix benchmarks for speed 2 files changed, 2 insertions(+) create mode 100644 README 好，现在你已经创建了第一个提交！ 可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添加和删改过。 请记住，提交时记录的是放在暂存区域的快照。 任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。 每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。 Git 提供了一个跳过使用暂存区域的方式， 只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤：123456789101112$ git statusOn branch masterChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.mdno changes added to commit (use \"git add\" and/or \"git commit -a\")$ git commit -a -m 'added new benchmarks'[master 83e38c7] added new benchmarks 1 file changed, 5 insertions(+), 0 deletions(-) 此时，提交之前不再需要 git add 文件“CONTRIBUTING.md”了。个人觉得不推荐。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Docker初探 1.0","date":"2018-02-07T02:10:00.000Z","path":"2018/02/07/Docker基本命令/","text":"Docker命令12从公网拉取一个镜像docker pull images_name 12查看已有的docker镜像docker images 12查看帮助docker command --help 12看容器的端口映射情况docker port con_id 12查看正在运行的容器docker ps 12查看所有的容器docker ps -a","tags":[{"name":"Docker","slug":"Docker","permalink":"https://wxzhongwang.github.io/tags/Docker/"}]},{"title":"Git查看历史提交","date":"2018-02-07T02:10:00.000Z","path":"2018/02/07/Git 查看提交历史/","text":"在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。 正如你所看到的，这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 Git 查看提交历史在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。 正如你所看到的，这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 git log 有许多选项可以帮助你搜寻你所要找的提交， 接下来我们介绍些最常用的。 定制输出格式-p一个常用的选项是 -p，用来显示每次提交的内容差异。 你也可以加上 -2 来仅显示最近两次提交： 该选项除了显示基本信息之外，还附带了每次commit的变化。当进行代码审查，或者快速浏览某个搭档提交的commit所带来的变化的时候，这个参数就非常有用了。1234567891011121314151617181920212223242526272829303132333435363738394041$ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numberdiff --git a/Rakefile b/Rakefileindex a874b73..8f94139 100644--- a/Rakefile+++ b/Rakefile@@ -5,7 +5,7 @@ require 'rake/gempackagetask' spec = Gem::Specification.new do |s| s.platform = Gem::Platform::RUBY s.name = \"simplegit\"- s.version = \"0.1.0\"+ s.version = \"0.1.1\" s.author = \"Scott Chacon\" s.email = \"schacon@gee-mail.com\" s.summary = \"A simple gem for using Git in Ruby code.\"commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary testdiff --git a/lib/simplegit.rb b/lib/simplegit.rbindex a0a60ae..47c6340 100644--- a/lib/simplegit.rb+++ b/lib/simplegit.rb@@ -18,8 +18,3 @@ class SimpleGit end end--if $0 == __FILE__- git = SimpleGit.new- puts git.show-end\\ No newline at end of file –stat你也可以为 git log 附带一系列的总结性选项。比如说，如果你想看到每次提交的简略的统计信息，你可以使用 –stat 选项：1234567891011121314151617181920212223242526272829$ git log --statcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number Rakefile | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-)commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test lib/simplegit.rb | 5 ----- 1 file changed, 5 deletions(-)commit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit README | 6 ++++++ Rakefile | 23 +++++++++++++++++++++++ lib/simplegit.rb | 25 +++++++++++++++++++++++++ 3 files changed, 54 insertions(+) 正如你所看到的，–stat 选项在每次提交的下面列出所有被修改过的文件、有多少文件被修改了以及被修改过的文件的哪些行被移除或是添加了。 在每次提交的最后还有一个总结。 –pretty 这个选项可以指定使用不同于默认格式的方式展示提交历史。 这个选项有一些内建的子选项供你使用。 比如用 oneline 将每个提交放在一行显示，查看的提交数很大时非常有用。 另外还有 short，full 和 fuller 可以用，展示的信息或多或少有些不同。1234$ git log --pretty=onelineca82a6dff817ec66f44342007202690a93763949 changed the version number085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary testa11bef06a3f659402fe7563abf99ad00de2209e6 first commit 但最有意思的是 format，可以定制要显示的记录格式。 这样的输出对后期提取分析格外有用。因为你知道输出的格式不会随着 Git 的更新而发生改变：1234$ git log --pretty=format:\"%h - %an, %ar : %s\"ca82a6d - Scott Chacon, 6 years ago : changed the version number085bb3b - Scott Chacon, 6 years ago : removed unnecessary testa11bef0 - Scott Chacon, 6 years ago : first commit git log –pretty=format 常用的选项 列出了常用的格式占位符写法及其代表的意义。 选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 –date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者（committer）的名字 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 你一定奇怪 作者 和 提交者之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。 –graph当 oneline 或 format 与另一个 log 选项 –graph 结合使用时尤其有用。 这个选项添加了一些ASCII字符串来形象地展示你的分支、合并历史：1234567891011$ git log --pretty=format:\"%h %s\" --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch 'master' of git://github.com/dustin/grit|\\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge branch 'defunkt' into local 常用汇总 选项 说明 -p 按补丁格式显示每个更新之间的差异。 –stat 显示每次更新的文件修改统计信息。 –shortstat 只显示 –stat 中最后的行数修改添加移除统计。 –name-only 仅在提交信息后显示已修改的文件清单。 –name-status 显示新增、修改、删除的文件清单。 –abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 –relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 –graph 显示 ASCII 图形表示的分支合并历史。 –pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 限制输出长度it log 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。 之前你已经看到过 -2 了，它只显示最近的两条提交， 实际上，这是 - 选项的写法，其中的 n 可以是任何整数，表示仅显示最近的若干条提交。 不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序，所以你一次只会看到一页的内容。 常用汇总 选项 说明 -(n) 仅显示最近的 n 条提交。 –since, –after 仅显示指定时间之后的提交。 –until, –before 仅显示指定时间之前的提交。 –author 仅显示指定作者相关的提交。 –committer 仅显示指定提交者相关的提交。 –grep 仅显示含指定关键字的提交。 -S 仅显示添加或移除了某个关键字的提交。 另外还有按照时间作限制的选项，比如 –since 和 –until 也很有用。 例如，下面的命令列出所有最近两周内的提交：1$ git log --since=2.weeks 这个命令可以在多种格式下工作，比如说具体的某一天 “2008-01-15”，或者是相对地多久以前 “2 years 1 day 3 minutes ago”。 来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试文件，可以用下面的查询命令：12345678$ git log --pretty=\"%h - %s\" --author=gitster --since=\"2008-10-01\" \\ --before=\"2008-11-01\" --no-merges -- t/5610e3b - Fix testcase failure when extended attributes are in useacd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;() APIf563754 - demonstrate breakage of detached checkout with symbolic link HEADd1a43f2 - reset --hard/read-tree --reset -u: remove unmerged new paths51a94af - Fix \"checkout --track -b newbranch\" on detached HEADb0ad11e - pull: allow \"git pull origin $something:$current_branch\" into an unborn branch","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git Diff","date":"2018-02-03T02:10:00.000Z","path":"2018/02/03/Git Diff/","text":"#Git Diff 如果 git status 命令的输出对于你来说过于模糊，你想知道具体修改了什么地方，可以用 git diff 命令。 尽管 git status 已经通过在相应栏下列出文件名的方式回答了这个问题，git diff 将通过文件补丁的格式显示具体哪些行发生了改变。 作用此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变。化内容。 若要查看已暂存的将要添加到下次提交里的内容，可以用 git diff –cached 命令。（Git 1.6.1 及更高版本还允许使用 git diff –staged，效果是相同的，但更好记些。） 请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。 所以有时候你一下子暂存了所有更新过的文件后，运行 git diff 后却什么也没有，就是这个原因。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Java 字节码","date":"2018-02-02T14:10:00.000Z","path":"2018/02/02/字节码/","text":"在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java程序从源代码到运行一般一下三步： (1).java文件 JDK中javac编译 (2).class文件（JVM能理解的Java字节） JVM (3)机器可以执行的二进制文件 需要格外注意的是 .class-&gt;机器码 这一步：在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 总结： Java虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Git .gitignore","date":"2018-02-02T02:10:00.000Z","path":"2018/02/02/Git gitignore/","text":".gitignore 忽略文件一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。 来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。 第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。 此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。 要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（) 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。 我们再看一个 .gitignore 文件的例子：1234567891011121314151617# no .a files*.a# but do track lib.a, even though you're ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"堆和栈","date":"2018-02-01T08:20:10.000Z","path":"2018/02/01/堆和栈/","text":"堆和栈栈是运行时的单位，而堆是存储式的单位。 栈解决程序运行的问题，解决程序如何运行的问题，如何处理数据。 堆解决数据存储问题，数据存哪，怎么存。 Java中，一个线程，就会有一个线程栈，不同的线程执行逻辑有所不同，因此需要一个独立的线程栈。而堆，是所有线程共有的。栈，是运行单元，里面存的信息都是与当前程序（线程）相关的。包括局部变量，程序运行状态，方法返回值等。堆只负责存储对象信息。 为什么要把堆和栈区分出来呢？栈中不是也可以存储数据吗？第一，从软件设计的角度看，栈代表了处理逻辑，而堆代表了数据。这样分开，使得处理逻辑更为清晰。分而治之的思想。这种隔离、模块化的思想在软件设计的方方面面都有体现。 第二，堆与栈的分离，使得堆中的内容可以被多个栈共享（也可以理解为多个线程访问同一个对象）。这种共享的收益是很多的。一方面这种共享提供了一种有效的数据交互方式(如：共享内存)，另一方面，堆中的共享常量和缓存可以被所有栈访问，节省了空间。 第三，栈因为运行时的需要，比如保存系统运行的上下文，需要进行地址段的划分。由于栈只能向上增长，因此就会限制住栈存储内容的能力。而堆不同，堆中的对象是可以根据需要动态增长的，因此栈和堆的拆分，使得动态增长成为可能，相应栈中只需记录堆中的一个地址即可。 第四，面向对象就是堆和栈的完美结合。其实，面向对象方式的程序与以前结构化的程序在执行上没有任何区别。但是，面向对象的引入，使得对待问题的思考方式发生了改变，而更接近于自然方式的思考。当我们把对象拆开，你会发现，对象的属性其实就是数据，存放在堆中；而对象的行为（方法），就是运行逻辑，放在栈中。我们在编写对象的时候，其实即编写了数据结构，也编写的处理数据的逻辑。 堆和栈中，栈是程序运行最根本的东西。程序运行可以没有堆，但是不能没有栈。而堆是为栈进行数据存储服务，说白了堆就是一块共享的内存。不过，正是因为堆和栈的分离的思想，才使得Java的垃圾回收成为可能。 栈溢出Java中，栈的大小通过-Xss来设置，当栈中存储数据比较多时，需要适当调大这个值，否则会出现java.lang.StackOverflowError异常。常见的出现这个异常的是无法返回的递归，因为此时栈中保存的信息都是方法返回的记录点。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Java 字节码","date":"2018-02-01T04:10:00.000Z","path":"2018/02/01/JDK&JRE/","text":"JDK 和 JREJDK: JDK是Java Development Kit，它是功能齐全的Java SDK。它拥有JRE所拥有的一切，还有编译器（javac）和工具（如javadoc和jdb）。它能够创建和编译程序。 JRE:Java运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java虚拟机（JVM），Java类库，java命令和其他的一些基础构件。但是，它不能用于创建新程序。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"JVM 1.0","date":"2018-02-01T02:10:00.000Z","path":"2018/02/01/1.0 Java程序执行流程/","text":"1.Java程序执行流程 Java技术的核心就是Java虚拟机，因为所有的Java程序都在虚拟机上运行。Java程序的运行需要Java虚拟机、Java API和Java Class文件的配合。Java虚拟机实例负责运行一个Java程序。当启动一个Java程序时，一个虚拟机实例就诞生了。当程序结束，这个虚拟机实例也就消亡。 分为编译时环境和运行时环境，很基础很重要。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://wxzhongwang.github.io/tags/JVM/"}]},{"title":"Git记录更新到仓库","date":"2018-02-01T02:10:00.000Z","path":"2018/02/01/Git 基础 - 记录每次更新到仓库/","text":"记录每次更新到仓库现在我们手上有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。 接下来，对这些文件做些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 请记住，你工作目录下的每一个文件都不外乎这两种状态：已跟踪或未跟踪。 已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。 工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。 初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态。 编辑过某些文件之后，由于自上次提交后你对它们做了修改，Git 将它们标记为已修改文件。 我们逐步将这些修改过的文件放入暂存区，然后提交所有暂存了的修改，如此反复。所以使用 Git 时文件的生命周期如下： 1234检查当前文件状态:$ git statusOn branch masternothing to commit, working directory clean 这说明你现在的工作目录相当干净。换句话说，所有已跟踪文件在上次提交后都未被更改过。 此外，上面的信息还表明，当前目录下没有出现任何处于未跟踪状态的新文件，否则 Git 会在这里列出来。 最后，该命令还显示了当前所在分支，并告诉你这个分支同远程服务器上对应的分支没有偏离。 现在，分支名是 “master”,这是默认的分支名。 现在，让我们在项目下创建一个新的 README 文件。 如果之前并不存在这个文件，使用 git status 命令，你将看到一个新的未跟踪文件：123456789$ echo 'My Project' &gt; README$ git statusOn branch masterUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) READMEnothing added to commit but untracked files present (use \"git add\" to track) 在状态报告中可以看到新建的 README 文件出现在 Untracked files 下面。 未跟踪的文件意味着 Git 在之前的快照（提交）中没有这些文件；Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”， 这样的处理让你不必担心将生成的二进制文件或其它不想被跟踪的文件包含进来。 不过现在的例子中，我们确实想要跟踪管理 README 这个文件。12使用命令 git add 开始跟踪一个文件。 所以，要跟踪 README 文件，运行：$ git add README 这时候我们在运行git status查看状态：123456$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README 文件已经处于被追踪的状态了，并处于暂存状态。 现在我们来修改一个已被跟踪的文件。 如果你修改了一个名为 CONTRIBUTING.md 的已被跟踪的文件，然后运行 git status 命令，会看到下面内容：123456789101112$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: READMEChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 文件 CONTRIBUTING.md 出现在 Changes not staged for commit 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。 要暂存这次更新，需要运行 git add命令。 git add: 这是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。 将这个命令理解为“添加内容到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 现在让我们运行 git add 将”CONTRIBUTING.md”放到暂存区，然后再看看 git status 的输出：12345678$ git add CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.md 现在两个文件都已暂存，下次提交时就会一并记录到仓库。 假设此时，你想要在 CONTRIBUTING.md 里再加条注释， 重新编辑存盘后，准备好提交。 不过且慢，再运行 git status 看看：1234567891011121314$ vim CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.mdChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 怎么回事？ 现在 CONTRIBUTING.md 文件同时出现在暂存区和非暂存区。 这怎么可能呢？ 好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本， 如果你现在提交，CONTRIBUTING.md 的版本是你最后一次运行 git add 命令时的那个版本，而不是你运行 git commit 时，在工作目录中的当前版本。 所以，运行了 git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来：12345678$ git add CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.md git statusgit status 命令的输出十分详细，但其用语有些繁琐。 如果你使用 git status -s 命令或 git status –short 命令，你将得到一种更为紧凑的格式输出。 运行 git status -s ，状态报告输出如下：123456$ git status -s M READMEMM RakefileA lib/git.rbM lib/simplegit.rb?? LICENSE.txt 新添加的未跟踪文件前面有 ?? 标记，新添加到暂存区中的文件前面有 A 标记，修改过的文件前面有 M 标记。 你可能注意到了 M 有两个可以出现的位置，出现在右边的 M 表示该文件被修改了但是还没放入暂存区，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。 例如，上面的状态报告显示： README 文件在工作区被修改了但是还没有将修改后的文件放入暂存区,lib/simplegit.rb 文件被修改了并将修改后的文件放入了暂存区。 而 Rakefile 在工作区被修改并提交到暂存区后又在工作区中被修改了，所以在暂存区和工作区都有该文件被修改了的记录。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git Clone","date":"2018-01-14T02:10:00.000Z","path":"2018/01/14/Git 获取Git仓库/","text":"获取 Git 仓库有两种取得 Git 项目仓库的方法。 第一种是在现有项目或目录下导入所有文件到 Git 中 第二种是从一个服务器克隆一个现有的 Git 仓库 在现有目录初始化 1$ git init 该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件，这些文件是 Git 仓库的骨干。 但是，在这个时候，我们仅仅是做了一个初始化的操作，你的项目里的文件还没有被跟踪。 克隆现有的仓库 如果你想获得一份已经存在了的Git仓库的拷贝，比如说，你想为某个开源项目贡献自己的一份力，这时就要用到 git clone 命令。 克隆仓库的命令格式是 git clone [url] 。 1$ git clone git@172.16.5.77:shengwangzhong/hexo-blog.git 这会在当前目录下创建一个名为 “hexo-blog” 的目录，并在这个目录下初始化一个 .git 文件夹，从远程仓库拉取下所有数据放入.git文件夹，然后从中读取最新版本的文件的拷贝。 如果你进入到这个新建的hexo-blog文件夹，你会发现所有的项目文件已经在里面了，准备就绪等待后续的开发和使用。 如果你想在克隆远程仓库的时候，自定义本地仓库的名字，你可以使用如下命令： 1$ git clone git@172.16.5.77:shengwangzhong/hexo-blog.git your-folder-name 这将执行与上一个命令相同的操作，不过在本地创建的仓库名字变为 your-folder-name。 Git 支持多种数据传输协议。 ssh\\https\\git","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git帮助","date":"2018-01-12T02:10:00.000Z","path":"2018/01/12/Git帮助/","text":"Git帮助123若你使用 Git 时需要获取帮助，有三种方法可以找到 Git 命令的使用手册：$ git help &lt;verb&gt;$ git &lt;verb&gt; --help 例如，要想获得 config 命令的手册，执行1$ git help config","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git起步","date":"2018-01-11T02:10:00.000Z","path":"2018/01/11/Git起步/","text":"Git起步 安装好第一步一定是配置用户名和邮箱. 配置用户名邮箱12$ git config --global user.name \"dick\"$ git config --global user.email 1528683621@qq.com 再次强调，如果使用了 –global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 –global 选项的命令来配置。 检查配置如果想要检查你的配置，可以使用 git config –list 命令来列出所有 Git 当时能找到的配置。 123456789101112131415161718192021$ git config --listcore.symlinks=falsecore.autocrlf=truecore.fscache=truecolor.diff=autocolor.status=autocolor.branch=autocolor.interactive=truehelp.format=htmlrebase.autosquash=truehttp.sslcainfo=D:/Git/mingw64/ssl/certs/ca-bundle.crthttp.sslbackend=openssldiff.astextplain.textconv=astextplainfilter.lfs.clean=git-lfs clean -- %ffilter.lfs.smudge=git-lfs smudge -- %ffilter.lfs.required=truefilter.lfs.process=git-lfs filter-processcredential.helper=manageruser.name=dickuser.email=1528683621@qq.comcredential.helper=store","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git使用中的问题","date":"2018-01-08T04:20:02.000Z","path":"2018/01/08/Git使用中的问题/","text":"git push失败 fatal: Could not read from remote repository 阐述问题git push失败 fatal: Could not read from remote repository. 因为仓库地址不对。更改地址就可以push了。 问题原因12$ git remote -v$ git remote set-url origin XXX 服务器上的 Git - 生成 SSH 公钥生成 SSH 公钥大多数 Git 服务器都会选择使用 SSH 公钥来进行授权。系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。生成公钥的过程在所有操作系统上都差不多。首先先确认一下是否已经有一个公钥了。SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。若想在github中使用的话需要将公钥复制到github&gt;setting&gt;SSH and GPG keys中添加ssh keys。 1234生成钥匙$ ssh-keygen查看公钥$cat ~/.ssh/id_rsa.pub","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"架构演变背景","date":"2018-01-01T04:10:10.000Z","path":"2018/01/01/架构演变背景/","text":"架构演变背景单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。","tags":[{"name":"架构","slug":"架构","permalink":"https://wxzhongwang.github.io/tags/架构/"}]},{"title":"优秀的开源项目","date":"2017-12-08T02:18:02.000Z","path":"2017/12/08/优秀的开源项目及优秀文章地址/","text":"此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 优秀的开源项目此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 前端相关NET相关NodeJS相关NodeJS 中文社区开源官网地址: https://cnodejs.org/ 项目开源地址： https://github.com/cnodejs/nodeclub/ JAVA相关数据库相关其他架构相关优秀文章地址此部分总结平时工作中积累的优秀文章或者博客 前端、JAVA、Python https://www.jqhtml.com/ Nginx:详细文档: http://tengine.taobao.org/book/index.html Jekins:教程http://blog.51cto.com/12832314/2140304","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"修改jar包中的配置文件","date":"2017-12-01T09:10:10.000Z","path":"2017/12/01/修改jar包中的配置文件/","text":"修改jar包中的配置文件一 通过vim命令直接修改保存jar。超方便。1.通过vim命令直接编辑jarvim xxx.jar 该命令首先会列出全部文件，可以通过输入/abc来搜索，定位到对应的abc文件后回车进入配置文件内进行编辑，:wq保存。 二 通过jar命令替换jar包中的文件(也可新增)1.列出jar包中的文件清单jar tf client.jar 2.提取出内部jar包的指定文件jar xf client.jar BOOT-INF/classes/realtime/t_ivr_data_bj.json 3.然后可以修改文件vim BOOT-INF/classes/realtime/t_ivr_data_bj.json 4.更新配置文件到内部jar包.(存在覆盖，不存在就新增)jar uf client.jar BOOT-INF/classes/realtime/t_ivr_data_bj.json 4.1更新内部jar包到jar文件jar uf client.jar 内部jar包.jar 5.可以查看验证是否已经更改vim client.jar","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Redis相关问题二","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis相关问题二/","text":"Redis 相关问题二 Redis采用多线程会有哪些问题？ 公司某个项目使用了redis进行数据的快速写入与查询,但在测试中发现它的查询速度完全不敌MySQL.经过代码分析发现,我们的架构师来了个奇葩操作.Redis与MySQL的关联,也就是先从Redis中查出相关数据在循环查询MySQL来获取完整数据,对于这种操作只能认为架构的脑袋被电梯夹了,在没有什么借口可以解释。好多人都是这样,对新技术有点皮毛了解就想”大显身手”,结果就闹得很是尴尬。 大家知道Redis是非关系型数据库,特点就是”快”,那我们直接把想要的数据全部存入Redis中,不做任何关联。 这个问题解决之后又发现了一个新问题,Redis在多线程高并发下出现数据错乱,也就是A的数据给了B,B的数据给到了C…. 分析:程序是多线程并且高并发情况,而Redis是单线程,也就是程序在Redis驱动返回结果时发生了张冠李戴的现象. 解决方法:Redis操作方法添加线程锁(lock),让其他线程排队.","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"Redis简介","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis简介/","text":"Redis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 RedisRedis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 Redis目前提供五种数据类型： string(字符串), list（链表）, Hash（哈希）, set（集合）, zset(sorted set) （有序集合） Redis开发维护很活跃，虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。 Redis可以做消息队列？ 首先，redis设计用来做缓存的，但是由于它自身的某种特性使得它可以用来做消息队列，它有几个阻塞式的API可以使用，正是这些阻塞式的API让其有能力做消息队列；另外，做消息队列的其他特性例如FIFO（先入先出）也很容易实现，只需要一个list对象从头取数据，从尾部塞数据即可；redis能做消息队列还得益于其list对象blpop brpop接口以及Pub/Sub（发布/订阅）的某些接口，它们都是阻塞版的，所以可以用来做消息队列。 对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明： 入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"Redis相关问题一","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis相关问题一/","text":"Redis 相关问题一 Redis为什么那么快? 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"相关问题四","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis相关问题四/","text":"Redis 相关问题四 Redis跳跃表问题。 Redis单进程单线程的Redis如何能够高并发？ 如何使用Redis实现分布式锁？ Redis分布式锁操作的原子性，Redis内部如何实现？","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"Redis相关问题三","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis相关问题三/","text":"Redis 相关问题三 Redis高并发会出现哪些问题？ 1) 如果redis宕机了，或者链接不上，怎么办？ 解决方法： ①配置主从复制，配置哨兵模式（相当于古代门派的长老级别可以选择掌门人的权利），一旦发现主机宕机，让下一个从机当做主机。 ②如果最坏的情况，只能关闭Redis连接，去往数据库连接。但由于数据量大，这样SQL数据库也会宕掉的。 2) 如果redis缓存在高峰期到期失效，在这个时刻请求会向雪崩一样，直接访问数据库如何处理？ 设置条件查询判断，判断redis缓存里是否有数据，如果没有，则去往数据库连接。当然要加分布式锁，利用redis的单线程+多路IO复用技术，原子性原理，让其它的线程请求等待，假若第一个线程进去获取到分布式锁在查询数据的途中宕掉了，不能让其它线程一直等待，设置等待一定时间判断是否取回数据，如果没有，递归调用自己的方法让第二个线程继续拿分布式锁查询数据库。当第二个锁从数据库拿到数据时，把数据值设置到redis数据库缓存中，设置失效时间，避免占内存，方便使用提高效率。 3) 如果用户不停地查询一条不存在的数据，缓存没有，数据库也没有，那么会出现什么? 如果数据不存在，缓存中没有，数据库也没有，当然如果不设置判断，会一直调用数据库，使数据库效率降低，访问量大时甚至会宕机。 解决方案：从数据库查询，如果数据库没有，则返回值为Null，判断数据库返回的值，如果为Null，则自定义把标识的字段存到Redis中，用key,value的方法，jedis.setex(key,”empty”)，设置失效时间跟具体情况而定，然后调用String json=jedis.get(key),判断是否获取的值”empty”.equal(json),如果相等，则抛出自定义异常，给用户提示，或者直接return null。这样用户再次查询的时候由于先从reids缓存中查询，redis会有对应的Key获取之前设置的value值，这样就不会再次调用数据库，影响效率等问题。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"Redis和Memcached比较","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis和Memcached比较/","text":"Redis和Memcache都是将数据存放在内存中，都是内存数据库。本文介绍两者的区别。 Redis和Memcached比较 Memcached是多线程，而Redis使用单线程. Memcached使用预分配的内存池的方式，Redis使用现场申请内存的方式来存储数据，并且可以配置虚拟内存。 Redis可以实现持久化，主从复制，实现故障恢复。 Memcached只是简单的key与value,但是Redis支持数据类型比较多。 Redis的存储分为内存存储、磁盘存储 .从这一点，也说明了Redis与Memcached是有区别的。Redis 与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改 操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis有两种存储方式(默认:snapshot) snapshot 实现方法是定时将内存的快照(snapshot)持久化到硬盘，这种方法缺点是持久化之后如果出现crash则会丢失一段数据。因此在完美主义者的推动下作者增加了aof方式。 aof 即append only mode，在写入内存数据的同时将操作命令保存到日志文件，在一个并发更改上万的系统中，命令日志是一个非常庞大的数据，管理维护成本非常高，恢复重建时间会非常长，这样导致失去aof高可用性本意。另外更重要的是Redis是一个内存数据结构模型，所有的优势都是建立在对内存复杂数据结构高效的原子操作","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"}]},{"title":"Linux目录结构简易理解","date":"2017-06-01T08:18:02.000Z","path":"2017/06/01/Linux目录结构简易理解/","text":"Linux目录结构/bin：bin是Binary的缩写, 这个目录存放着最经常使用的命令。 /boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。 /etc：这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all/root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。 sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp：这个目录是用来存放一些临时文件的。 /usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 note在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux处理目录的常用命令","date":"2017-06-01T08:18:02.000Z","path":"2017/06/01/Linux 处理目录的常用命令/","text":"Linux处理目录的常用命令常见的处理目录的命令： ls: 列出目录 cd：切换目录 pwd：显示目前的目录 mkdir：创建一个新的目录 rmdir：删除一个空的目录 cp: 复制文件或目录 rm: 移除文件或目录 mv: 移动文件与目录，或修改文件与目录的名称 使用 man [命令] 来查看各个命令的使用文档，如 ：man cp。 ls (列出目录)在Linux系统当中， ls 命令可能是最常被运行的。12345语法：[root@www ~]# ls [-aAdfFhilnrRSt] 目录名称[root@www ~]# ls [--color=&#123;never,auto,always&#125;] 目录名称[root@www ~]# ls [--full-time] 目录名称 参数： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -d ：仅列出目录本身，而不是列出目录内的文件数据(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用)将家目录下的所有文件列出来(含属性与隐藏档) mkdir（创建目录）语法1mkdir [-mp] 目录名称 参数： -m ：配置文件的权限，不需要看默认权限 (umask) 的脸色 -p ：将所需要的目录(包含上一级目录)递归创建起来 实例：在/tmp下尝试创建数新目录：1234567[root@www ~]# cd /tmp[root@www tmp]# mkdir test [root@www tmp]# mkdir test1/test2/test3/test4mkdir: cannot create directory `test1/test2/test3/test4': No such file or directory [root@www tmp]# mkdir -p test1/test2/test3/test4加了这个 -p 的选项，可以自行帮你创建多层目录！ 实例：创建权限为 rwx–x–x 的目录。12345[root@www tmp]# mkdir -m 711 test2[root@www tmp]# ls -ldrwxr-xr-x 3 root root 4096 Jul 18 12:50 testdrwxr-xr-x 3 root root 4096 Jul 18 12:53 test1drwx--x--x 2 root root 4096 Jul 18 12:54 test2 rmdir（删除目录）1rmdir [-p] 目录名称 参数： -p ：连同上一级『空的』目录也一起删除删除 runoob 目录1[root@www tmp]# rmdir runoob/ 过要注意的是，这个 rmdir 仅能删除空的目录，你可以使用 rm 命令来删除非空目录。 cp（拷贝文件和目录）语法:12[root@www ~]# cp [-adfilprsu] 来源档(source) 目标档(destination)[root@www ~]# cp [options] source1 source2 source3 .... directory 参数： -a：相当於 -pdr 的意思，至於 pdr 请参考下列说明；(常用) -d：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身； -f：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次； -i：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行(常用) -l：进行硬式连结(hard link)的连结档创建，而非复制文件本身； -p：连同文件的属性一起复制过去，而非使用默认属性(备份常用)； -r：递归持续复制，用於目录的复制行为；(常用) -s：复制成为符号连结档 (symbolic link)，亦即『捷径』文件； -u：若 destination 比 source 旧才升级 destination ！ 用 root 身份，将 root 目录下的 .bashrc 复制到 /tmp 下，并命名为 bashrc123[root@www ~]# cp ~/.bashrc /tmp/bashrc[root@www ~]# cp -i ~/.bashrc /tmp/bashrccp: overwrite `/tmp/bashrc'? n &lt;==n不覆盖，y为覆盖 rm (移除文件或目录)语法：1rm [-fir] 文件或目录 参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息； -i ：互动模式，在删除前会询问使用者是否动作 -r ：递归删除啊,最常用在目录的删除了 12[root@www tmp]# rm -i bashrcrm: remove regular file `bashrc'? y 如果加上 -i 的选项就会主动询问喔，避免你删除到错误的档名 mv (移动文件与目录，或修改名称)语法：12[root@www ~]# mv [-fiu] source destination[root@www ~]# mv [options] source1 source2 source3 .... directory 参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖； -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！ -u ：若目标文件已经存在，且 source 比较新，才会升级 (update) 复制一文件，创建一目录，将文件移动到目录中1234[root@www ~]# cd /tmp[root@www tmp]# cp ~/.bashrc bashrc[root@www tmp]# mkdir mvtest[root@www tmp]# mv bashrc mvtest 将某个文件移动到某个目录去，就是这样做！ 将刚刚的目录名称更名为 mvtest21[root@www tmp]# mv mvtest mvtest2","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux更改文件属性","date":"2017-06-01T08:18:02.000Z","path":"2017/06/01/Linux 更改文件属性/","text":"Linux 更改文件属性1、chgrp：改属组 change group 语法：1chgrp [-R] 属组名 文件名 参数选项 [-R]: 递归更改文件属组，就是在更改某个目录文件的属组时，如果加上-R的参数，那么该目录下的所有文件的属组都会更改。 2、chown：改属主，也可以同时更改属组 change owner 语法：12chown [–R] 属主名 文件名chown [-R] 属主名：属组名 文件名 进入 /root 目录（~）将install.log的拥有者改为bin这个账号：1234[root@www ~] cd ~[root@www ~]# chown bin install.log[root@www ~]# ls -l-rw-r--r-- 1 bin users 68495 Jun 25 08:53 install.log 3、chmod：更改文件9个属性 Linux文件的基本权限就有九个，分别是owner/group/others三种身份各有自己的read/write/execute权限。 例如：文件的权限字符为：『-rwxrwxrwx』， 可以使用数字来代表各个权限，各权限的分数对照表如下： r:4 w:2 x:1 每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的，例如当权限为： [-rwxrwx—] 结果： 123owner = rwx = 4+2+1 = 7group = rwx = 4+2+1 = 7others= --- = 0+0+0 = 0 设定权限变更为：770 语法1chmod [-R] xyz 文件或目录 参数 xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。eg: 770 Example: 将.bashrc这个文件所有的权限都设定启用12345[root@www ~]# ls -al .bashrc-rw-r--r-- 1 root root 395 Jul 4 11:45 .bashrc[root@www ~]# chmod 777 .bashrc[root@www ~]# ls -al .bashrc-rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 那如果要将权限变成 -rwxr-xr– 呢？那么权限的分数就成为 [4+2+1][4+0+1][4+0+0]=754。 符号类型改变文件权限另一种改权限的方式。针对user,group,others设置权限。 user group others 那么我们就可以使用 u, g, o 来代表三种身份的权限。 可选 ： u g o a (指所有，即全部的身份) 操作符： 加入 除去 = 设定权限： r w x 可以使用以下语法来设定:1chmod u=rwx,g=rwx,o=rwx 文件名 Example：将文件权限设置为 -rwxr-xr– ，123456# touch test // 创建文件# ls -al test // 查看默认权限-rw-r--r-- 1 root root 0 Nov 15 10:32 test# chmod u=rwx,g=rx,o=r test // 修改权限# ls -al test-rwxr-xr-- 1 root root 0 Nov 15 10:32 test 而如果是要将权限去掉而不改变其他已存在的权限呢？例如要拿掉全部人的可执行权限，则：123# chmod a-x test1# ls -al test1-rw-r--r-- 1 root root 0 Nov 15 10:32 test1","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux绝对路径 、 相对路径","date":"2017-06-01T08:18:02.000Z","path":"2017/06/01/Linux 绝对路径 、 相对路径/","text":"Linux绝对路径 、 相对路径Linux的目录结构为树状结构，最顶级的目录为根目录 /。 其他目录通过挂载可以将它们添加到树中，通过解除挂载可以移除它们。 绝对路径：路径的写法，由根目录 / 写起，例如： /usr/share/doc 这个目录。 相对路径：路径的写法，不是由 / 写起，例如由 /usr/share/doc 要切换到 /usr/share/man 底下时，可以写成： cd ../man 这就是相对路径。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"Linux 目录结构","date":"2017-06-01T08:18:02.000Z","path":"2017/06/01/Linux目录结构/","text":"Linux目录结构/bin：bin是Binary的缩写, 这个目录存放着最经常使用的命令。 /boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。 /etc：这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all/root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。 sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp：这个目录是用来存放一些临时文件的。 /usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 note在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"异常处理","date":"2017-04-10T14:30:00.000Z","path":"2017/04/10/异常处理./","text":"====&gt;StackOverFlowError(栈溢出) ====&gt;VirtualMachineError ====&gt;OutOfMemoryError (内存溢出) ====&gt; Error ====&gt;AWTError Throwable ====================================================== ====&gt;EOFException ====&gt;IOException ====&gt;FileNoFoundException ====&gt; Exception ====&gt;ClassNoFoundException ====&gt;NullPointerException ====&gt;RuntimeException ====&gt;IllegalArgumentException ====&gt;ArrayIndexOutOfBoundsException ====&gt;UnknownTypeException ====&gt;MissingResourceException ====&gt;ArrithmeticException(除数为0的异常 ArrithmeticException,就是运行时异常) 在 Java 中，所有的异常都有一个共同的祖先java.lang包中的 Throwable类。Throwable：有两个重要的子类：Exception（异常） 和 Error（错误）二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"mybatis相关问题","date":"2017-04-08T14:22:22.000Z","path":"2017/04/08/mybatis/","text":"mybatis相关问题mybatis应用1.mybatis优缺点，spring与mybatis集成 2.Config、Sql配置、Mapper配置、有几种注册Mapper的方式，优先级如何？ 3.mybatis的一级缓存、二级缓存、mybatis的二级缓存为何是鸡肋？ 4.通用mapper的实现，mybatis编写sql语句的三种方式。 mybatis源码分析1.@MapperScan的源码分析？mapperScan如何生效 2.mybatis如何扩展spring的扫描器的，mybatis扫描完之后如何利用FactoryBean？ 3.mybatis底层如何会把一个代理对象放在spring容器中？用到了spring的哪些知识？ 4.mybatis和spring的核心接口ImportBeanDefinitionRegistrar之间的关系。 5.mybatis如何实现一级缓存，spring集成后如何失效的？为何将其失效，有没有办法解决。 6.mybatis分析mybatis的执行流程、mybatis的sql什么时候缓存的？缓存在哪里？ 7.mybatis当中的方法名为何要与mapper当中的id一致？从源码分析？","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://wxzhongwang.github.io/tags/Mybatis/"}]},{"title":"关于 final 关键字的一些总结","date":"2017-04-05T07:30:00.000Z","path":"2017/04/05/final关键字的一些总结/","text":"#关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。(初始化之后不能修改) 1.对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。. 2.当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 3.使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义;第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"hashCode 与 equals","date":"2017-04-04T05:30:00.000Z","path":"2017/04/04/hashCode 与 equals/","text":"hashCode 与 equalshashCode（）介绍hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！ 意义：可以快速找到所需要的对象。 为什么要有 hashCode？我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 在散列表中才有用，在其它情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与equals（）的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"== 与 equals","date":"2017-04-04T01:30:00.000Z","path":"2017/04/04/== 与 equals/","text":"== 与 equals== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true(即，认为这两个对象相等)。 12345678910111213141516public class test1 &#123; public static void main(String[] args) &#123; String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) &#123; // true System.out.println(\"true\"); &#125; &#125; Notice: String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"判断字符串是否为空字符串","date":"2017-04-01T09:10:10.000Z","path":"2017/04/01/判断字符串是否为空字符串/","text":"判断字符串是否为空字符串12345678910111213141516171819public class StringUtil &#123; /** * 判断是否为空字符串最优代码 * @param str * @return 如果为空，则返回true */ public static boolean isEmpty(String str)&#123; return str == null || str.trim().length() == 0; &#125; /** * 判断字符串是否非空 * @param str * @return 如果不为空，则返回true */ public static boolean isNotEmpty(String str)&#123; return !isEmpty(str); &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"pom.xml","date":"2017-03-06T08:22:30.000Z","path":"2017/03/06/pom.xml/","text":"pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;!--父项目的坐标。--&gt; &lt;parent&gt; &lt;artifactId/&gt; &lt;groupId/&gt; &lt;version/&gt; &lt;!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。Maven首先在构建当前项目的地方寻找父项 目的pom，其次在文件系统的这个位置（relativePath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。--&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;!--声明项目描述符遵循哪一个POM模型版本。--&gt; &lt;groupId&gt;com.dick.application&lt;/groupId&gt;&lt;!--项目的全球唯一标识符--&gt; &lt;artifactId&gt;first-app&lt;/artifactId&gt;&lt;!-- 构件的标识符，它和group ID一起唯一标识一个构件。Maven为项目产生的构件包括：JARs，源 码，二进制发布和WARs等。--&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;!--项目当前版本, 格式为:主版本.次版本.增量版本-限定版本号--&gt; &lt;name&gt;application-name&lt;/name&gt; &lt;!--项目的名称, Maven产生的文档用--&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt;&lt;!--项目主页的URL, Maven产生的文档用--&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;prerequisites&gt; &lt;!--构建该项目或使用该插件所需要的Maven的最低版本--&gt; &lt;maven/&gt; &lt;/prerequisites&gt; &lt;!--项目的问题issue管理系统(Bugzilla, Jira, Scarab,或任何你喜欢的问题管理系统)的名称和URL，本例为 jira--&gt; &lt;issueManagement&gt; &lt;!--问题管理系统（例如jira）的名字，--&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!--该项目使用的问题管理系统的URL--&gt; &lt;url&gt;http://jira.baidu.com/banseon&lt;/url&gt; &lt;/issueManagement&gt; &lt;!--项目持续集成信息--&gt; &lt;ciManagement&gt; &lt;system/&gt;&lt;!--持续集成系统的名字，例如continuum--&gt; &lt;url/&gt;&lt;!--该项目使用的持续集成系统的URL（如果持续集成系统有web接口的话）。--&gt; &lt;notifiers&gt; &lt;!--构建完成,通知的开发者/用户的配置项。包括被通知者信息和通知条件（错误，失败，成功，警告）--&gt; &lt;notifier&gt; &lt;type/&gt; &lt;!--传送通知的途径--&gt; &lt;!--发生错误时是否通知--&gt; &lt;sendOnError/&gt; &lt;!--构建失败时是否通知--&gt; &lt;sendOnFailure/&gt; &lt;!--构建成功时是否通知--&gt; &lt;sendOnSuccess/&gt; &lt;!--发生警告时是否通知--&gt; &lt;sendOnWarning/&gt; &lt;!--不赞成使用。通知发送到哪里--&gt; &lt;address/&gt; &lt;!--扩展配置项--&gt; &lt;configuration/&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; &lt;!--项目创建年份，4位数字。当产生版权信息时需要使用这个值。--&gt; &lt;inceptionYear/&gt; &lt;!--项目相关邮件列表信息--&gt; &lt;mailingLists&gt; &lt;!--该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。--&gt; &lt;mailingList&gt; &lt;!--邮件的名称--&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!--发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建--&gt; &lt;post&gt;dick@126.com&lt;/post&gt; &lt;!--订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建--&gt; &lt;subscribe&gt;dick@126.com&lt;/subscribe&gt; &lt;!--取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建--&gt; &lt;unsubscribe&gt;dick@126.com&lt;/unsubscribe&gt; &lt;!--你可以浏览邮件信息的URL--&gt; &lt;archive&gt;http:/hi.baidu.com/dick/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; &lt;!--项目开发者列表--&gt; &lt;developers&gt; &lt;!--某个项目开发者的信息--&gt; &lt;developer&gt; &lt;!--SCM源代码管理，项目开发者的唯一标识符--&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!--项目开发者的全名--&gt; &lt;name&gt;dick&lt;/name&gt; &lt;!--项目开发者的email--&gt; &lt;email&gt;dick@126.com&lt;/email&gt; &lt;!--项目开发者的主页的URL--&gt; &lt;url/&gt; &lt;!--项目开发者在项目中扮演的角色，角色元素描述了各种角色--&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!--项目开发者所属组织--&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!--项目开发者所属组织的URL--&gt; &lt;organizationUrl&gt;http://hi.dick.com/dick&lt;/organizationUrl&gt; &lt;!--项目开发者属性，如即时消息如何处理等--&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!--项目开发者所在时区， -11到12范围内的整数。--&gt; &lt;timezone&gt;-5&lt;/timezone&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!--项目的其他贡献者列表--&gt; &lt;contributors&gt; &lt;!--项目的其他贡献者。参见developers/developer元素--&gt; &lt;contributor&gt; &lt;name/&gt; &lt;email/&gt; &lt;url/&gt; &lt;organization/&gt; &lt;organizationUrl/&gt; &lt;roles/&gt; &lt;timezone/&gt; &lt;properties/&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!--该元素描述了项目所有License列表。 应该只列出该项目的license列表，不要列出依赖项目的 license列表。如果列出多个license，用户可以选择它们中的一个而不是接受所有license。--&gt; &lt;licenses&gt; &lt;!--描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。--&gt; &lt;license&gt; &lt;name&gt;Apache 2&lt;/name&gt;&lt;!--license用于法律上的名称--&gt; &lt;url&gt;http://www.baidu.com/banseon/LICENSE-2.0.txt&lt;/url&gt;&lt;!--官方的license正文页面的URL--&gt; &lt;distribution&gt;repo&lt;/distribution&gt;&lt;!--项目分发的主要方式(repo)，可以从Maven库下载(manual)， 用户必须手动下载和安装依赖--&gt; &lt;!--关于license的补充信息--&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!--SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。--&gt; &lt;scm&gt; &lt;!--SCM的URL,该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。--&gt; &lt;connection&gt; scm:svn:http://svn.baidu.com/banseon/maven/banseon/banseon-maven2-trunk(dao-trunk) &lt;/connection&gt; &lt;!--给开发者使用的，类似connection元素。即该连接不仅仅只读--&gt; &lt;developerConnection&gt; scm:svn:http://svn.baidu.com/banseon/maven/banseon/dao-trunk &lt;/developerConnection&gt; &lt;tag/&gt;&lt;!--当前代码的标签，在开发阶段默认为HEAD--&gt; &lt;url&gt;http://svn.baidu.com/banseon&lt;/url&gt;&lt;!--指向项目的可浏览SCM库（例如ViewVC或者Fisheye）的URL。--&gt; &lt;/scm&gt; &lt;!--描述项目所属组织的各种属性。Maven产生的文档用--&gt; &lt;organization&gt; &lt;name&gt;demo&lt;/name&gt;&lt;!--组织的全名--&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt;&lt;!--组织主页的URL--&gt; &lt;/organization&gt; &lt;!--构建项目需要的信息--&gt; &lt;build&gt; &lt;!--该元素设置了项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。--&gt; &lt;sourceDirectory/&gt; &lt;!--该元素设置了项目脚本源码目录，该目录和源码目录不同：绝大多数情况下，该目录下的内容 会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。--&gt; &lt;scriptSourceDirectory/&gt; &lt;!--该元素设置了项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。--&gt; &lt;testSourceDirectory/&gt; &lt;!--被编译过的应用程序class文件存放的目录。--&gt; &lt;outputDirectory/&gt; &lt;!--被编译过的测试class文件存放的目录。--&gt; &lt;testOutputDirectory/&gt; &lt;!--使用来自该项目的一系列构建扩展--&gt; &lt;extensions&gt; &lt;!--描述使用到的构建扩展。--&gt; &lt;extension&gt; &lt;!--构建扩展的groupId--&gt; &lt;groupId/&gt; &lt;!--构建扩展的artifactId--&gt; &lt;artifactId/&gt; &lt;!--构建扩展的版本--&gt; &lt;version/&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!--当项目没有规定目标（Maven2 叫做阶段）时的默认值--&gt; &lt;defaultGoal/&gt; &lt;!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。--&gt; &lt;resources&gt; &lt;!--这个元素描述了项目相关或测试相关的所有资源路径--&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录（例如$&#123;project.build.outputDirectory&#125;）。 举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven /messages。 然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。--&gt; &lt;targetPath/&gt; &lt;!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。--&gt; &lt;filtering/&gt; &lt;!--描述存放资源的目录，该路径相对POM路径--&gt; &lt;directory/&gt; &lt;!--包含的模式列表，例如**/*.xml.--&gt; &lt;includes/&gt; &lt;!--排除的模式列表，例如**/*.xml--&gt; &lt;excludes/&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。--&gt; &lt;testResources&gt; &lt;!--这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明--&gt; &lt;testResource&gt; &lt;targetPath/&gt; &lt;filtering/&gt; &lt;directory/&gt; &lt;includes/&gt; &lt;excludes/&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!--构建产生的所有文件存放的目录--&gt; &lt;directory/&gt; &lt;!--产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。--&gt; &lt;finalName/&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表--&gt; &lt;filters/&gt; &lt;!--子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。 给定插件的任何本地配置都会覆盖这里的配置--&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述插件所需要的信息。--&gt; &lt;plugin&gt; &lt;groupId/&gt; &lt;artifactId/&gt; &lt;version/&gt; &lt;!--是否从该插件下载Maven扩展（例如打包和类型处理器），由于性能原因，只有在真需要下载时，该元素才被设置成enabled。--&gt; &lt;extensions/&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。--&gt; &lt;executions&gt; &lt;!--execution元素包含了插件执行需要的信息--&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标--&gt; &lt;id/&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段--&gt; &lt;phase/&gt; &lt;!--配置的执行目标--&gt; &lt;goals/&gt; &lt;!--配置是否被传播到子POM--&gt; &lt;inherited/&gt; &lt;!--作为DOM对象的配置--&gt; &lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--项目引入插件所需要的额外依赖--&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目--&gt; &lt;inherited/&gt; &lt;!--作为DOM对象的配置--&gt; &lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!--使用的插件列表--&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素--&gt; &lt;plugin&gt; &lt;groupId/&gt; &lt;artifactId/&gt; &lt;version/&gt; &lt;extensions/&gt; &lt;executions&gt; &lt;execution&gt; &lt;id/&gt; &lt;phase/&gt; &lt;goals/&gt; &lt;inherited/&gt; &lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals/&gt; &lt;inherited/&gt; &lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--在列的项目构建profile，如果被激活，会修改构建处理--&gt; &lt;profiles&gt; &lt;!--根据环境参数或命令行参数激活某个构建处理--&gt; &lt;profile&gt; &lt;id/&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标志--&gt; &lt;activeByDefault/&gt; &lt;jdk/&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字--&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows')--&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本--&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;名称&#125;引用），其拥有对应的名称和值，Profile就会被激活。如果值 字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段--&gt; &lt;property&gt; &lt;!--激活profile的属性的名称--&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值--&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活 profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。--&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。--&gt; &lt;exists&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。--&gt; &lt;missing&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!--构建项目所需要的信息。参见build元素--&gt; &lt;build&gt; &lt;defaultGoal/&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath/&gt;&lt;filtering/&gt;&lt;directory/&gt;&lt;includes/&gt;&lt;excludes/&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath/&gt;&lt;filtering/&gt;&lt;directory/&gt;&lt;includes/&gt;&lt;excludes/&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory/&gt; &lt;finalName/&gt; &lt;filters/&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素--&gt; &lt;plugin&gt; &lt;groupId/&gt;&lt;artifactId/&gt;&lt;version/&gt;&lt;extensions/&gt; &lt;executions&gt; &lt;execution&gt; &lt;id/&gt;&lt;phase/&gt;&lt;goals/&gt;&lt;inherited/&gt;&lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals/&gt;&lt;inherited/&gt;&lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素--&gt; &lt;plugin&gt; &lt;groupId/&gt; &lt;artifactId/&gt; &lt;version/&gt; &lt;extensions/&gt; &lt;executions&gt; &lt;execution&gt; &lt;id/&gt; &lt;phase/&gt; &lt;goals/&gt; &lt;inherited/&gt; &lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals/&gt; &lt;inherited/&gt; &lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径--&gt; &lt;modules/&gt; &lt;!--发现依赖和扩展的远程仓库列表。--&gt; &lt;repositories&gt; &lt;!--参见repositories/repository元素--&gt; &lt;repository&gt; &lt;releases&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;id/&gt; &lt;name/&gt; &lt;url/&gt; &lt;layout/&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表--&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素--&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;id/&gt; &lt;name/&gt; &lt;url/&gt; &lt;layout/&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。--&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--该元素包括使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting元素--&gt; &lt;reporting&gt; ...... &lt;/reporting&gt; &lt;!--参见dependencyManagement元素--&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--参见distributionManagement元素--&gt; &lt;distributionManagement&gt; ...... &lt;/distributionManagement&gt; &lt;!--参见properties元素--&gt; &lt;properties/&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径--&gt; &lt;modules/&gt; &lt;!--发现依赖和扩展的远程仓库列表。--&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息--&gt; &lt;repository&gt; &lt;releases&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式--&gt; &lt;url&gt;http://192.168.1.169:9999/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然 而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。--&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表--&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素--&gt; &lt;pluginRepository&gt; ...... &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。例如，如果你想要构建两个单独的构件成 JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器，你就可以使用分类器来生成两个单独的JAR构件。--&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile ：默认范围，用于编译 - provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime: 在执行时需要使用 - test: 用于test任务时使用 - system: 需要外在提供相应的元素。通过systemPath来取得 - systemPath: 仅用于范围为system。提供相应的路径 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用--&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!--仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。--&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!--当计算传递依赖时， 从依赖构件列表里，列出被排除的依赖构件集。即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!--可选依赖，如果你在项目B中把C依赖声明为可选，你就需要在依赖于B的项目（例如项目A）中显式的引用对C的依赖。可选依赖阻断依赖的传递性。--&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--该元素描述使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。--&gt; &lt;reporting&gt; &lt;!--true，则，网站不包括默认的报表。这包括“项目信息”菜单中的报表。--&gt; &lt;excludeDefaults/&gt; &lt;!--所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。--&gt; &lt;outputDirectory/&gt; &lt;!--使用的报表插件和他们的配置。--&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述报表插件需要的信息--&gt; &lt;plugin&gt; &lt;!--报表插件在仓库里的group ID--&gt; &lt;groupId/&gt; &lt;!--报表插件在仓库里的artifact ID--&gt; &lt;artifactId/&gt; &lt;!--被使用的报表插件的版本（或版本范围）--&gt; &lt;version/&gt; &lt;!--任何配置是否被传播到子项目--&gt; &lt;inherited/&gt; &lt;!--报表插件的配置--&gt; &lt;configuration/&gt; &lt;!--一组报表的多重规范，每个规范可能有不同的配置。一个规范（报表集）对应一个执行目标 。例如，有1，2，3，4，5，6，7，8，9个报表。1，2，5构成A报表集，对应一个执行目标。2，5，8构成B报表集，对应另一个执行目标--&gt; &lt;reportSets&gt; &lt;!--表示报表的一个集合，以及产生该集合的配置--&gt; &lt;reportSet&gt; &lt;!--报表集合的唯一标识符，POM继承时用到--&gt; &lt;id/&gt; &lt;!--产生报表集合时，被使用的报表的配置--&gt; &lt;configuration/&gt; &lt;!--配置是否被继承到子POMs--&gt; &lt;inherited/&gt; &lt;!--这个集合里使用到哪些报表--&gt; &lt;reports/&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。这部分的依赖信息不会被立即解析,而是当子项目声明一个依赖（必须描述group ID和 artifact ID信息），如果group ID和artifact ID以外的一些信息没有描述，则通过group ID和artifact ID 匹配到这里的依赖，并使用这里的依赖信息。--&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。--&gt; &lt;distributionManagement&gt; &lt;!--部署项目产生的构件到远程仓库需要的信息--&gt; &lt;repository&gt; &lt;!--是分配给快照一个唯一的版本号（由时间戳和构建流水号）？还是每次都使用相同的版本号？参见repositories/repository元素--&gt; &lt;uniqueVersion/&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;banseon maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout/&gt; &lt;/repository&gt; &lt;!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionManagement/repository元素--&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion/&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;Banseon-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.baidu.com/banseon:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout/&gt; &lt;/snapshotRepository&gt; &lt;!--部署项目的网站需要的信息--&gt; &lt;site&gt; &lt;!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置--&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!--部署位置的名称--&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!--部署位置的URL，按protocol://hostname/path形式--&gt; &lt;url&gt;scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web&lt;/url&gt; &lt;/site&gt; &lt;!--项目下载页面的URL。如果没有该元素，用户应该参考主页。使用该元素的原因是：帮助定位那些不在仓库里的构件（由于license限制）。--&gt; &lt;downloadUrl/&gt; &lt;!--如果构件有了新的group ID和artifact ID（构件移到了新的位置），这里列出构件的重定位信息。--&gt; &lt;relocation&gt; &lt;!--构件新的group ID--&gt; &lt;groupId/&gt; &lt;!--构件新的artifact ID--&gt; &lt;artifactId/&gt; &lt;!--构件新的版本号--&gt; &lt;version/&gt; &lt;!--显示给用户的，关于移动的额外信息，例如原因。--&gt; &lt;message/&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有：none（默认），converted（仓库管理员从 Maven 1 POM转换过来），partner（直接从伙伴Maven 2仓库同步过来），deployed（从Maven 2实例部 署），verified（被核实时正确的和最终的）。--&gt; &lt;status/&gt; &lt;/distributionManagement&gt; &lt;!--以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件（见settings.xml配置文件里activation元素的说明）。格式是&lt;name&gt;value&lt;/name&gt;。--&gt; &lt;properties/&gt; &lt;/project&gt;","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wxzhongwang.github.io/tags/Maven/"}]},{"title":"对maven中setting.xml配置文件进行解释","date":"2017-03-06T08:22:30.000Z","path":"2017/03/06/MAVEN/","text":"settings.xml对maven中setting.xml配置文件进行解释 1.声明规范123&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; 2.localRepository12&lt;!-- 本地仓库的路径。默认值为 --&gt;&lt;localRepository&gt;/opt/repository&lt;/localRepository&gt; 3.interactiveMode1234 &lt;!--Maven是否需要和用户交互以获得输入。 如果Maven需要和用户交互以获得输入，则设置成true，反之则应为false。 默认为true。--&gt;&lt;interactiveMode&gt;true&lt;/interactiveMode&gt; 4.usePluginRegistry123&lt;!--Maven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让Maven使用文件来管理插件版本，则设为true。默认为false。--&gt;&lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; 5.offline12345&lt;!--表示Maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 --&gt;&lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; 6.pluginGroups12345678&lt;!--当插件的组织Id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表 。该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo --&gt; &lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; 7.proxies12345678910111213141516171819202122&lt;!--用来配置不同的代理，多代理profiles 可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;proxies&gt; &lt;!--代理元素包含配置代理时需要的信息--&gt; &lt;proxy&gt; &lt;!--代理的唯一定义符，用来区分不同的代理元素。--&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!--该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 --&gt; &lt;active&gt;true&lt;/active&gt; &lt;!--代理的协议。 协议://主机名:端口，分隔成离散的元素以方便配置。--&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;!--代理的主机名。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!--代理的端口。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!--代理的用户名，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!--代理的密码，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!--不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。--&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; 8.servers1234567891011121314151617181920&lt;!--配置服务端的一些设置。一些设置如安全证书不应该和pom.xml一起分发。这种类型的信息应该存在于构建服务器上的settings.xml文件中。--&gt; &lt;servers&gt; &lt;!--服务器元素包含配置服务器时需要的信息 --&gt; &lt;server&gt; &lt;!--这是server的id（注意不是用户登陆的id），该id与distributionManagement中repository元素的id相匹配。--&gt; &lt;id&gt;server001&lt;/id&gt; &lt;!--鉴权用户名。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;!--鉴权密码 。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。密码加密功能已被添加到2.1.0 +。详情请访问密码加密页面--&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;!--鉴权时使用的私钥位置。和前两个元素类似，私钥位置和私钥密码指定了一个私钥的路径（默认是$&#123;user.home&#125;/.ssh/id_dsa）以及如果需要的话，一个密语。将来passphrase和password元素可能会被提取到外部，但目前它们必须在settings.xml文件以纯文本的形式声明。 --&gt; &lt;privateKey&gt;$&#123;usr.home&#125;/.ssh/id_dsa&lt;/privateKey&gt; &lt;!--鉴权时使用的私钥密码。--&gt; &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; &lt;!--文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用权限（permission）。这两个元素合法的值是一个三位数字，其对应了unix文件系统的权限，如664，或者775。 --&gt; &lt;filePermissions&gt;664&lt;/filePermissions&gt; &lt;!--目录被创建时的权限。 --&gt; &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; &lt;/server&gt; &lt;/servers&gt; 9.mirrors123456789101112131415&lt;!--为仓库列表配置的下载镜像列表。高级设置请参阅镜像设置页面 --&gt;&lt;mirrors&gt; &lt;!--给定仓库的下载镜像。 --&gt; &lt;mirror&gt; &lt;!--该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;!--镜像名称 --&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;!--该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;!--被镜像的服务器的id。 例如，如果我们要设置了一个Maven中央仓库（http://repo.maven.apache.org/maven2/）的镜像，就需要将该元素设置成central。这必须和中央仓库的id central完全一致。--&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 10.profile123456789101112&lt;!--根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。它包含了id，activation, repositories, pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings中的profile被激活，它的值会覆盖任何其它定义在POM中或者profile.xml中的带有相同id的profile。 --&gt;&lt;profiles&gt; &lt;!--根据环境参数来调整的构件的配置--&gt; &lt;profile&gt; &lt;!--该配置的唯一标识符。 --&gt; &lt;id&gt;test&lt;/id&gt; 11.activation1234567891011121314151617181920212223242526272829303132&lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。如POM中的profile一样，profile的力量来自于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test）。--&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标识--&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。--&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。--&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本--&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;name&#125;引用），其拥有对应的name = 值，Profile就会被激活。如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段--&gt; &lt;property&gt; &lt;!--激活profile的属性的名称--&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。--&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。--&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; 12.Repositories远程仓库列表，它是Maven用来填充构建系统本地仓库所使用的一组远程项目。123456789101112131415161718192021222324252627282930313233343536373839404142434445 &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。 这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做 -ignore（忽略），fail（失败），或者warn（警告）。--&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素--&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表。仓库是两种主要构件的家。第一种构件被用作其它构件的依赖。这是中央仓库中存储的大部分构件类型。另外一种构件类型是插件。Maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址。--&gt;&lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见profiles/profile/repositories/repository元素的说明--&gt; &lt;pluginRepository&gt; &lt;id/&gt; &lt;name/&gt; &lt;url/&gt; &lt;layout/&gt; &lt;releases&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; 13.activeProfiles 配置解释：手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的profile都会被激活。如果没有匹配的profile，则什么都不会发生。例如，alwaysActiveProfile是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行1234&lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt;&lt;/activeProfiles&gt;","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wxzhongwang.github.io/tags/Maven/"}]},{"title":"NodeJs插件","date":"2017-03-01T14:22:22.000Z","path":"2017/03/01/NodeJs相关插件/","text":"lodash采用延迟计算，意味着我们的链式方法在显式或者隐式的value()调用之前是不会执行的，因此lodash可以进行shortcut（捷径） fusion（融合）这样的优化，通过合并链式大大降低迭代的次数，从而大大提升其执行性能。 cors实现跨域的功能 helmet是一个保护Node.JS应用的安全项目一些著名的对Web攻击有XSS跨站脚本， 脚本注入 clickjacking以及各种非安全的请求等对Node.js的Web应用构成各种威胁，使用Helmet能帮助你的应用避免这些攻击。 csurf跨站请求伪造 colors终端着色插件 express-validator校验插件npm install –save express-validator uuid GUID插件npm install –save node-uuid","tags":[{"name":"NodeJs","slug":"NodeJs","permalink":"https://wxzhongwang.github.io/tags/NodeJs/"}]},{"title":"NodeJs基础","date":"2017-03-01T14:22:22.000Z","path":"2017/03/01/NodeJs基础/","text":"NodeJs基础Node 核心Node核心： 非阻塞； 单线程； 事件驱动。 NodeJs回调回调函数：异步编程依托于回调来实现，但不能说使用了回调后程序就异步化了。Node 使用了大量的回调函数，Node所有API都支持回调函数。未使用回调函数可能会产生阻塞。 阻塞：1234var fs = require(\"fs\");var data = fs.readFileSync('input.txt');console.log(data.toString());console.log(\"程序执行结束!\"); 非阻塞：12345678var fs = require(\"fs\");fs.readFile('input.txt', function (err, data) &#123; if (err) return console.error(err); console.log(data.toString());&#125;);console.log(\"程序执行结束!\"); NodeJs事件事件驱动程序。当web server接收到请求，就把它关闭然后进行处理，然后去服务下一个web请求。当这个请求完成，它被放回处理队列，当到达队列开头，这个结果被返回给用户。这个模型非常高效可扩展性非常强，因为webserver一直接受请求而不等待任何读写操作。（这也被称之为非阻塞式IO或者事件驱动IO） 模块化 NodeJs 提供了两个命令，require 和 exportsexports是模块中公开的接口，require用于从外部获取接口，其实可以通过如此获取到所需模块的一个对象。 模块的加载顺序一）从文件模块缓存中加载： 尽管原生模块与文件模块的优先级不同，但是都会优先于从文件模块的缓存中加载已经存在的模块。 二）从原生模块加载： 原生模块的优先级仅次于文件模块缓存的优先级。require 方法在解析文件名之后，优先检查模块是否在原生模块列表中。以http模块为例，尽管在目录下存在一个 http/http.js/http.node/http.json 文件，require(“http”) 都不会从这些文件中加载，而是从原生模块中加载。原生模块也有一个缓存区，同样也是优先从缓存区加载。如果缓存区没有被加载过，则调用原生模块的加载方式进行加载和执行。 三）从文件加载： 当文件模块缓存中不存在，而且不是原生模块的时候，Node.js 会解析 require 方法传入的参数，并从文件系统中加载实际的文件，require方法接受以下几种参数的传递： http、fs、path等，原生模块。 ./mod或../mod，相对路径的文件模块。 /pathtomodule/mod，绝对路径的文件模块。 mod，非原生模块的文件模块。 NodeJs BufferJS本身只有字符串类型，没有二进制数据类型。但在处理像TCP流或文件流时，必须使用到二进制数据。因此在 Node.js中，定义了一个Buffer类，该类用来创建一个专门存放二进制数据的缓存区。 NodeJs StreamStream 是一个抽象接口，Node 中有很多对象实现了这个接口。例如，对http 服务器发起请求的request 对象就是一个 Stream，还有stdout（标准输出）。 Node.js，Stream 有四种流类型： Readable - 可读操作。 Writable - 可写操作。 Duplex - 可读可写操作. Transform - 操作被写入数据，然后读出结果。 所有的 Stream 对象都是 EventEmitter 的实例。常用的事件有： data - 当有数据可读时触发。 end - 没有更多的数据可读时触发。 error - 在接收和写入过程中发生错误时触发。 finish - 所有数据已被写入到底层系统时触发。 NodeJs OS模块：（基本的系统操作函数） os.tmpdir() 返回操作系统的默认临时文件夹。 os.endianness() 返回 CPU 的字节序，可能的是 “BE” 或 “LE”。 os.hostname() 返回操作系统的主机名。 os.type() 返回操作系统名 os.platform() 返回操作系统名 os.arch() 返回操作系统 CPU 架构，可能的值有 “x64”、”arm” 和 “ia32”。 os.release() 返回操作系统的发行版本。 os.uptime() 返回操作系统运行的时间，以秒为单位。 os.loadavg() 返回一个包含 1、5、15 分钟平均负载的数组。 os.totalmem() 返回系统内存总量，单位为字节。 os.freemem() 返回操作系统空闲内存量，单位是字节。 os.cpus() 返回一个对象数组，包含所安装的每个 CPU/内核的信息：型号、速度（单位 MHz）、时间（一个包含 user、nice、sys、idle 和 irq 所使用 CPU/内核毫秒数的对象）。 os.networkInterfaces() 获得网络接口列表。 NodeJs Path 模块：（提供了处理和转换文件路的工具） path.normalize(p) 规范化路径，注意’..’ 和 ‘.’。 path.join([path1][, path2][, …]) 用于连接路径。该方法的主要用途在于，会正确使用当前系统的路径分隔符，Unix系统是”/“，Windows系统是”\\”。 path.resolve([from …], to) 将 to 参数解析为绝对路径。 path.isAbsolute(path) 判断参数 path 是否是绝对路径。 path.relative(from, to) 用于将相对路径转为绝对路径。 path.dirname(p) 返回路径中代表文件夹的部分，同 Unix 的dirname 命令类似。 path.basename(p[, ext]) 返回路径中的最后一部分。同 Unix 命令 bashname 类似。 path.extname(p) 返回路径中文件的后缀名，即路径中最后一个’.’之后的部分。如果一个路径中并不包含’.’或该路径只包含一个’.’ 且这个’.’为路径的第一个字符，则此命令返回空字符串。 path.parse(pathString) 返回路径字符串的对象。 path.format(pathObject) 从对象中返回路径字符串，和 path.parse 相反。 其他NodeJs Net 模块：（提供了用于底层的网络通信的小工具，包含了创建服务器/客户端的方法） NodeJs DNS 模块：（用于解析域名） NodeJs Express框架：请求和响应：app.get(‘/‘, function (req, res) { // –}) Express 应用使用回调函数的参数： request和response对象来处理请求和响应的数据。 Request 对象：request对象表示 HTTP 请求，包含了请求查询字符串，参数，内容，HTTP 头部等属性。常见属性有： (1)req.app：当callback为外部文件时，用req.app访问express的实例 (2)req.baseUrl：获取路由当前安装的URL路径 (3)req.body / req.cookies：获得「请求主体」/ Cookies (4)req.fresh / req.stale：判断请求是否还「新鲜」 (5)req.hostname / req.ip：获取主机名和IP地址 (6)req.originalUrl：获取原始请求URL (7)req.params：获取路由的parameters (8)req.path：获取请求路径 (9)req.protocol：获取协议类型 (10)req.query：获取URL的查询参数串 (11)req.route：获取当前匹配的路由 (12)req.subdomains：获取子域名 (13)req.accepts()：检查可接受的请求的文档类型 (14)req.acceptsCharsets / req.acceptsEncodings / req.acceptsLanguages：返回指定字符集的第一个可接受字符编码 (15)req.get()：获取指定的HTTP请求头 (16)req.is()：判断请求头Content-Type的MIME类型 Reponse 对象： (1)res.app：同req.app一样 (2)res.append()：追加指定HTTP头 (3)res.set(): 在res.append()后将重置之前设置的头 (4)res.cookie(name，value [，option])：设置Cookie opition: domain / expires / httpOnly / maxAge / path / secure / signed (5)res.clearCookie()：清除Cookie (6)res.download()：传送指定路径的文件 (7)res.get()：返回指定的HTTP头 (8)res.json()：传送JSON响应 (9)res.jsonp()：传送JSONP响应 (10)res.location()：只设置响应的Location HTTP头，不设置状态码或者close response (11)res.redirect()：设置响应的Location HTTP头，并且设置状态码302 (12)res.send()：传送HTTP响应 (13)res.sendFile(path [，options] [，fn])： 传送指定路径的文件 -会自动根据文件extension设定Content-Type (14)res.set()：设置HTTP头，传入object可以一次设置多个头 (15)res.status()：设置HTTP状态码 (16)res.type()：设置Content-Type的MIME类型","tags":[{"name":"NodeJs","slug":"NodeJs","permalink":"https://wxzhongwang.github.io/tags/NodeJs/"}]},{"title":"动态属性ExpandoObject","date":"2017-02-12T08:20:10.000Z","path":"2017/02/12/动态属性ExpandoObject/","text":"#动态属性ExpandoObject 动态对象 ExpandoObject中添加动态属性？ 12345678dynamic obj = new ExpandoObject();IDictionary&lt;string, object&gt; temp = new Dictionary&lt;string, object&gt;();temp.Add(Key, Value);foreach (KeyValuePair&lt;string, object&gt; item in temp)&#123; ((IDictionary&lt;string, object&gt;)obj).Add(item.Key, item.Value);&#125; 因为不难发现ExpandObject继承自以下接口：IDictionary&lt;string, object&gt;, ICollection&lt;KeyValuePair&lt;string, object&gt;&gt;, IEnumerable&lt;KeyValuePair&lt;string, object&gt;&gt;, IEnumerable","tags":[{"name":"NET","slug":"NET","permalink":"https://wxzhongwang.github.io/tags/NET/"}]},{"title":"单例模式","date":"2017-02-08T04:18:45.000Z","path":"2017/02/08/Java单例模式/","text":"单例模式什么是单例模式因程序需要，有时我们只需要某个类同时保留一个对象，不希望有更多对象，此时，我们则应考虑单例模式的设计。 特点 单例模式只能有一个实例。 单例类必须创建自己的唯一实例。 单例类必须向其他对象提供这一实例。 单例模式VS静态类在知道了什么是单例模式后，我想你一定会想到静态类，“既然只使用一个对象，为何不干脆使用静态类？”，这里我会将单例模式和静态类进行一个比较。 单例可以继承和被继承，方法可以被override，而静态方法不可以。 静态方法中产生的对象会在执行后被释放，进而被GC清理，不会一直存在于内存中。 静态类会在第一次运行时初始化，单例模式可以有其他的选择，即可以延迟加载。 基于2， 3条，由于单例对象往往存在于DAO层（例如sessionFactory），如果反复的初始化和释放，则会占用很多资源，而使用单例模式将其常驻于内存可以更加节约资源。 静态方法有更高的访问效率。 单例模式很容易被测试。 几个关于静态类的误解： 误解一：静态方法常驻内存而实例方法不是。 实际上，特殊编写的实例方法可以常驻内存，而静态方法需要不断初始化和释放。 误解二：静态方法在堆(heap)上，实例方法在栈(stack)上。 实际上，都是加载到特殊的不可写的代码内存区域中。 静态类和单例模式情景的选择： 情景一：不需要维持任何状态，仅仅用于全局访问，此时更适合使用静态类。 情景二：需要维持一些特定的状态，此时更适合使用单例模式。 单例模式的实现1. 懒汉模式（线程不安全）123456789101112public class SingletonDemo &#123; private static SingletonDemo instance; private SingletonDemo()&#123; &#125; public static SingletonDemo getInstance()&#123; if(instance==null)&#123; instance=new SingletonDemo(); &#125; return instance; &#125;&#125; 如上，通过提供一个静态的对象instance，利用private权限的构造方法和getInstance()方法来给予访问者一个单例。 缺点是，没有考虑到线程安全，可能存在多个访问者同时访问，并同时构造了多个对象的问题。 之所以叫做懒汉模式，主要是因为此种方法可以非常明显的lazy loading。 针对懒汉模式线程不安全的问题，我们自然想到了，在getInstance()方法前加锁，于是就有了第二种实现。 2. 懒汉模式二 （线程安全）123456789101112public class SingletonDemo &#123; private static SingletonDemo instance; private SingletonDemo()&#123; &#125; public static synchronized SingletonDemo getInstance()&#123; if(instance == null)&#123; instance = new SingletonDemo(); &#125; return instance; &#125;&#125; 然而并发其实是一种特殊情况，大多时候这个锁占用的额外资源都浪费了，这种打补丁方式写出来的结构效率很低。 3. 饿汉模式123456789public class SingletonDemo &#123; private static SingletonDemo instance = new SingletonDemo(); private SingletonDemo()&#123; &#125; public static SingletonDemo getInstance()&#123; return instance; &#125;&#125; 直接在运行这个类的时候进行一次loading，之后直接访问。显然，这种方法没有起到lazy loading的效果，考虑到前面提到的和静态类的对比，这种方法只比静态类多了一个内存常驻而已。 4. 静态类内部加载1234567891011public class SingletonDemo &#123; private static class SingletonHolder&#123; private static SingletonDemo instance = new SingletonDemo(); &#125; private SingletonDemo()&#123; System.out.println(\"Singleton has loaded\"); &#125; public static SingletonDemo getInstance()&#123; return SingletonHolder.instance; &#125;&#125; 使用内部类的好处是，静态内部类不会在单例加载时就加载，而是在调用getInstance()方法时才进行加载，达到了类似懒汉模式的效果，而这种方法又是线程安全的。 5. 枚举方法123456789101112enum SingletonDemo&#123; INSTANCE; public void otherMethods()&#123; System.out.println(\"Something\"); &#125;&#125;public class Hello &#123; public static void main(String[] args)&#123; SingletonDemo.INSTANCE.otherMethods(); &#125;&#125; 提倡的方式，在我看来简直是来自神的写法。解决了以下三个问题： (1)自由序列化。 (2)保证只有一个实例。 (3)线程安全。 这种充满美感的代码真的已经终结了其他一切实现方法了。 6. 双重校验锁法12345678910111213141516public class SingletonDemo &#123; private static SingletonDemo instance; private SingletonDemo()&#123; System.out.println(\"Singleton has loaded\"); &#125; public static SingletonDemo getInstance()&#123; if(instance == null)&#123; synchronized (SingletonDemo.class)&#123; if(instance == null)&#123; instance = new SingletonDemo(); &#125; &#125; &#125; return instance; &#125;&#125; 接下来我解释一下在并发时，双重校验锁法会有怎样的情景： STEP 1. 线程A访问getInstance()方法，因为单例还没有实例化，所以进入了锁定块。 STEP 2. 线程B访问getInstance()方法，因为单例还没有实例化，得以访问接下来代码块，而接下来代码块已经被线程1锁定。 STEP 3. 线程A进入下一判断，因为单例还没有实例化，所以进行单例实例化，成功实例化后退出代码块，解除锁定。 STEP 4. 线程B进入接下来代码块，锁定线程，进入下一判断，因为已经实例化，退出代码块，解除锁定。 STEP 5. 线程A初始化并获取到了单例实例并返回，线程B获取了在线程A中初始化的单例。 理论上双重校验锁法是线程安全的，并且，这种方法实现了lazyloading。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://wxzhongwang.github.io/tags/设计模式/"}]},{"title":"编码规范","date":"2017-01-24T02:10:00.000Z","path":"2017/01/24/C# 编码规范/","text":"编码规范C# 编码风格指南一旦进入代码开发阶段，你必须安排好代码审查计划以确保每个人都遵守同样的规则。建议按以下3种方式进行代码审查： 互相审查 - 由其他团队成员进行代码审查以确保遵守了代码规范且达到要求。这种方式可以配合单元测试。项目中每个代码文件必须要通过这个程序。 架构审查 - 架构人员需对项目核心模块进行审查以确保符合设计，没有出现大的甚至有可能影响整个项目运转的纰漏。 团队审查 - 每周随机选择一个或多个文件进行一次团队审查。审查会议开始前30分钟，将文件打印并分发到每个成员手里，会议开始后用投影仪将文件内容展示出来。代码的每一块都要进行审查，让所有成员提出改进建议。（别忘了要感谢提供素材的开发人员，并确保他不会觉得受到了“群嘲”！） 代码文件组织C# 源代码文件每个源代码文件应该只包含一个类定义，也即类定义只出现在它自己的文件中。源代码文件名需与类声明里的类名保持一致。譬如一个名为User的类的源文件名应该是User.cs。 版权声明1234567891011//-------------------------------------------------------------------------------// Copyright (c) dick. All Right Reserved. // This source is subject to the Microsoft Permissive License. // Please see the License.txt file for more information. // All other rights reserved. // // THIS CODE AND INFORMATION ARE PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY // KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE // IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A // PARTICULAR PURPOSE. //------------------------------------------------------------------------------- 目录设计每层命名空间都要建立目录（以MyProject.UI.Admin为例，请使用MyProject/UI/Admin这样的目录结构，而不是只建立一层名为MyProject.UI.Admin的目录）。 代码排列C#源代码文件整体顺序应为：123Using 声明Namespace声明Class 和 interface 声明 C#类内部分的顺序应为：12345成员变量属性构造函数方法` 以上每个部分都需要放到 #region里。 命名空间和USING声明 Using 和命名空间声明都要左边界齐平。 命名空间的每个组件名首字母要大写 如果组件名是个缩写，只让第一个字母大写，如 System.Data.Sql。 如果这个缩写只有2个字母，那可以2个字母都大写，如 System.IO。 注意: 移除不需要的或重复的命名空间，使用短的命名空间来代替。譬如：1234567891011121314// Preferredusing System.Data.SqlClient;public void ValidationCall()&#123; … SqlConnection conn = new SqlConnection(connstr);…&#125;// Avoidpublic void ValidationCall()&#123; … System.Data.SqlClient.SqlConnection conn = new SqlConnection(connstr);…&#125; 创建和修订记录文件创建和修订记录需要按以下格式填写到using代码段后面，namespace之前。12345/** 作者: 创建日期:* 修订者： 修订日期： 修订内容：* */ XML 文档Visual Studio提供了一种文档类型，在开发环境中可以用来检测并导出到结构化的XML中，以用来创建与源代码分离的代码级别的文档。XML 文档用于描述类、方法和属性。它应当在所有可用的情况下尽可能使用。 缩进空行适当的空行能增强代码可读性。它们能帮助区分逻辑不相干的代码片段。双行空行要放在：源代码中逻辑不相关的代码片段之间不同的类和接口定义（如果有时候不得不放在同个文件，尽量避免这种情况）单行空行要放在： 方法与方法之间 属性与属性之间 方法体内的局部变量和它的第一次使用之间 方法体内不同逻辑片段之间以增强可读性 换行如果一个表达式在一行内放不下，则根据以下常用原则来进行换行： 操作符号之后换行 逗号之后换行 择较高等的换行，其次才是较低等的换行 （譬如优先对括号外的部分换行）换行后需要缩进：1234567// 调用方法SomeMethod1(expression1, expression2, expression3, expression4, expression5);// 声明方法void SomeMethod1(long Expression1, long Expression2, long Expression3, long Expression4, long Expression5)&#123; //...&#125; 注意: 换行后第二行的缩进与第一个参数齐平以下是一个数学表达式换行的例子。第一种换行方式是较好的，因为是在括号以外的地方进行的换行，也即是在较高等进行的分行。 12345// Preferredvar1 = var2 * (var3 + var4 – var5) + var4 * var6;// Avoidvar1 = var2 * (var3 + var4 - var5) + var4 * var6; 对if表达式的分行需要使用缩进：1234if ((condition1 &amp;&amp; condition2) || (condition3 &amp;&amp; condition4) &amp;&amp; condition5 || !condition6)&#123; ...&#125; 三元表达式请用这两种格式：1234Alpha = aLongBooleanExpression ? beta : gamma;Alpha = aLongBooleanExpression ? beta : gamma; 代码间距空格间距在代码中间的逗号或分号后应该有单独的一个空格，此外一个关键字与后跟的括号之间也要有个空格，譬如：12345678910// CorrectTestMethod(a, b, c);while (condition)&#123; //... &#125;// AvoidTestMethod(a, b, c);TestMethod(a, b, c); 注意: 在方法名和它的前括号之间不需要有空格。这样便于区分是关键字还是方法名。操作符两边要加上空格（++或逻辑非这样的一元运算符除外），譬如：1234567a = b; // Avoid a=b;// Avoid for(int i=0; i&lt;10; ++i) or for(int i=0;i&lt;10;++i)for (int i = 0; i &lt; 10; ++i)&#123; ...&#125; TAB间距该使用几个空格作为代码缩进距离从来没有达成一致过。有人喜欢2个空格，有人喜欢4个而有人喜欢8个甚至更多。所以最好使用Tab缩进。它的好处有：可以自定义缩进距离它只需一个字符，所以只要按一次键而不是2,4,8… 如果你要增加缩进（或减少）一片代码，可以选中它们然后按Tab来增加或者Shift+Tab来减少缩进。几乎所有文本编辑器都支持这个操作。在这里Tab指的就是标准的缩进字符。注意： 不要使用空格来缩进代码-用Tab！在VS中把Tab配置成4个空格的距离。 一般注释通用注释前缀TODO : 表示以后别忘了在这里还需要进一步处理BUG: [bugid]：表示这里有个已知bug，解释一下bug情况，如果可以则给出bug id。KLUDGE: 表示这里的代码有点糟糕，解释一下有时间的话将如何改进。TRICKY: 表示以下代码比较奇技淫巧，如果没有仔细思考请不要改动。WARNING: 表示当心某事COMPILER: 表示临时注释掉某些影响编译通过的代码。这些问题最终会被解决的。ATTRIBUTE: value：嵌在注释中的属性的一般形式。你可以自定义属性，它们最终会被VS提取。 单行注释12//Console.WriteLine(\"哈哈\");//Console.ReadKey(); 多行注释1234/* * 注释内容 * 注释内容 */ 文档注释1234/// &lt;summary&gt;/// 文档注释/// 文档注释/// &lt;/summary&gt; 变量声明及命名规范变量：Camel：变量名首单词的首字母小写，其余每个单词首字母单词大写,多用于给变量或字段或方法参数命名。 123var str = \"123\";var highSchoolStudent = \"123\";int num = 5; 常量：1public static const string BaseIpAddress = \"\"; 方法名：Pascal：每个单词的首字母都要大些其余小写，多用于类或方法。尽量用一般名词或动名词。1234public string GetHighSchoolStudent()&#123; &#125;","tags":[{"name":"NET","slug":"NET","permalink":"https://wxzhongwang.github.io/tags/NET/"}]},{"title":"Nginx无法访问的问题","date":"2017-01-24T02:10:00.000Z","path":"2017/01/24/Centos7安装nginx后仍然无法访问/","text":"Centos7安装nginx后，无法访问的问题安装步骤 1curl localhost 发现是通的 （Welcome to nginx） 1firewall-cmd --zone=public --add-port=80/tcp --permanent 重启防火墙：12systemctl stop firewalld.servicesystemctl start firewalld.service","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wxzhongwang.github.io/tags/Nginx/"}]}]